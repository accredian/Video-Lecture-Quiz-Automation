 How is that I that you No, it's not prithik. Who said? Yeah, you have to know the logistic regression. So, could be a typo or something. I don't know, but today is not the last day of the stone. I can talk to the team if that was communicated. We just start three weeks of class. It will take two more weeks. At least one and a half more weeks because tomorrow there is no class. Yeah, some typo. Okay, I'll wait for one more minute for people to join and then we'll get started. So, today we are going to do two things. I might stretch this class. I might stretch this class by maybe 15, 20 minutes. So, that may not anyone has any concern at that point, but there are two things that I planned to do today. So, one is there are two important concepts before we move into the hands on. So, quickly touching upon four relation. I remember some of you also had this question around, okay, what is correlation? What does that mean? How it's different from regression? Then I'll also talk about how it's used in linear regression. That's one piece and then the second piece that I'm going to talk about is, how do you deal with categorical variables? If in your data you have a variable which has categories or text, right? How do you deal with that? Was that something which has come up in our hands on? And then after that we would move towards the linear regression hands on that we are going to do together. So, those are the three things that are on my radar for today. If required, I might extend this class by 15 minutes because I don't want to stop the hands on. And leave it for the next week. So, we are with me, but yeah, with that let's get started. So, last week we did discuss about and we did complete linear regression, right? In the entirety. So, we did talk about how do you evaluate the model performance, whether it's good or not. We did talk about R square. We did talk about the other pieces of linear regression in terms of its application. And all the model lifecycle, how do you evaluate the model and everything around it, right? So, anyone has any doubt on that? Is everyone clear? Are there any doubts, any confusion, anything before I start? If there are any, let me know. I'm happy to clarify. If not, then let's get started. Then anyone has any doubt on what we have studied now, anything you want me to clarify? There are no doubts and start with the new topic as discussed. Okay. Okay, looks like everyone has sorted. So, let's get started. So, as I said, so far, I'm going to talk about what is correlation, how it's different from regression and where it's used in regression, right? Because our various used in the entire modeling cycle, that's what we will talk about. Because you might say, okay, why are we even studying correlation? And we don't even use it in the modeling part, right? So, it's something which is heavily used in linear regression because linear regression is starting basic technique. So, we end up using that to some extent in linear regression, we'll study why. Second thing that I'm going to talk about is how do you handle the categorical variables in your model? Because in your data, there can be ample amount of categorical variables, which we'll talk about. And then the third thing that we'll do is we will start with the hands-on. So, the first two topics are quick topics. They are not something which are going to take a lot of time, but let's do for this test with them. So, quickly starting with correlation. What is a correlation? Can anyone tell me what is a correlation? Any cases, any anything that you are aware of? Anyone? Any cases? Okay, what is the question? Okay, very good, Radha. Very good with the very good Roman word about others, anyone else? Very good, sopnel. Okay, so let me quickly share my screen that I already talked about it. So, all of you mentioned it perfectly. All of you mentioned it correctly. So, correlation is nothing but it defines an association between the two variables. It tells you how two variables are related to each other. So, that's what correlation is. For example, let's say based on DNA and blood group, you can match a child associated to a parent. So, if you need to define association between two variables, how two variables are related to each other, the go to thing is correlation. So, let's talk about more examples of correlation. So, let's say we are talking about a house right where we have area and where we have let's a number of bedrooms. Right, I want to understand how area and number of bedrooms are related to each other. So, can someone tell me like, what do you guess? Will they be directly related or indirectly related? Any guesses? There are two relationships, right, that we know. One is your directly proportional. The second is indirectly proportional. All of us have studied this, right? So, area and bedroom, whether they would be directly related or indirectly related. Okay, right, with the saying, we have more bedroom than 10 to have. Very good. Prati Kradar also saying directly related. Prishans also saying directly related. Right, so very good. So, ideally the expectation is that your area as your area goes up, right, as your area increases. Your bedrooms are also expected to increase. Right, on the 100 square feet area, you will have two bedrooms on 200. You might have three on 300. You might have four and so on. So, in the hypothetical scenario, as your area increases, your bedroom increases. Right. So, that's why they are called directly proportional. So, you can even think of it or visualize it on a plot where you say that this is what is called directly proportional. Where you have area, your area increasing your number of bedrooms are increasing. So, it's upwards sloping curve upwards sloping relationship that you see. Right. Your area increases your number of bedrooms increases. They are directly proportional. Everyone makes sense. Okay. So, that's something like price of the house in distance from market. Right. So, when they be directly related or indirectly related. So, first of all, what does indirectly proportional means? Indirectly proportion means if one is increases the other reduces. Right. So, direct means both are moving in the same direction. Indirect means if one reduced, if one is increasing the other is decreasing. So, what is indirect price and market distance? Does they seem to be direct or indirect? So, one guys, Radha is saying indirect. What about others? Okay. When we say indirect, it simply means that what we are saying is that our house price is indirectly proportional to. Distance from market. So, what we are saying as we move far away from market. Right. What we are saying is as we move far away from market, the price is expected to go down because you are going far away from amenities. Right. You will have to use a vehicle to even go to the market. Which way is just and go down. But what I am trying to say is that your as the distance from market increases as your amenities moves farther. Right. Your price is expected to go down. So, this is called inversely proportional or indirectly proportional or indirectly related. Right. So, it is a downward sloping curve. Everyone with me till now. Any question on this? Any question? Anyone needs any clarity? So, this particular piece or the one that we are talking about tells us this particular piece tells us how the two variables are related to each other. Right. They could be directly related or they could be indirectly related directly related means they are both of them are moving in the same direction. Indirect relation means if they are moving in the opposite direction if one increases the other reduces like you can see in case of price and market distance. If your market distance increases your price reduces right. So, it's indirectly proportional. Similarly, in case of area and bed you could see as bed increases your area increases right. They are directly proportional. They are moving in the same direction. So, when we talk about correlation, when we talk about correlation, correlation is nothing but has two components. One, it defines the direction of relationship between two variables. Second, it also defines or it also tells you the strength of relationship between two variables. So, it does tell you how two variables are related to each other whether they are directly related or indirectly related. But at the same time, it also tells you the strength. How strongly are they related? So, let's say if I am talking about area and bedroom. So, if I am talking about a correlation of area and bedroom right. So, you will see two components. You will see it as plus some value. This is what you will see. And essentially, before we go into that, so correlation ranges from minus 1 to 1 to plus 1. So, correlation ranges from minus 1 to plus 1. Higher the value, closer is the value to 1. Higher is the strength of relationship. So, closer to 1 higher strength. This is what it means. Closer to 1 higher strength plus and minus defines the direction. So, if it's plus, that means directly related. If it's minus means indirectly related. For example, correlation between area and bedroom would be plus because they are moving in the same direction. Similarly, correlation between your price and market distance would be minus. Does this make sense to everyone? So, there are two components in the value of correlation. When you are calculating correlation between two variables, you will essentially get two components. One is the direction of the relationship which is defined by a plus and minus that you can see out here. So, if it's plus, if it's positive, that means both of them are moving in the same direction. They are directly related to each other. If it's minus, that means they are indirectly related to each other. Then, the next value, you will have these strength of relationship between those variables. So, let's say they are positively related. Both are moving in the same direction. So, how strongly are they related? What is these strength? So, if this value is, let's say, 0.9. That means they are very strongly related. They are not that strongly related. Similarly, in this, you will have a value. If they are minus 0.9, they are very strongly related. If it's minus 0.2, they are indirectly proportional. They are inversely related, but the relationship is not that strong. So, essentially, correlation has two components. One is the direction of correlation, right? Direction of relation of the variable. Second is the strength of relationship between the two variables. And the range is minus 1 to 1, because there are two directions that we are talking about. Does this make sense to everyone? What is correlation? What does it do? How it's defined? We won't go into the derivation of it, because it's a very complex. Mathematical derivations of no point going there. You should at a high level understand how to interpret the correlation values that we are talking about right now. So, any question or doubt on this? Anyone who is not clear? All good with correlation? If we then next I will talk about where it's used. Like, where do you use it in an Indian regression? Okay. So, now let's talk about where it's used in linear regression, if there are no questions. So, in linear regression, yes, Prishand. So, strength, so, core, exactly 0 to 1 is the strength plus minus is the direction. Closer is the value to 1 higher the strength. Positive means same direction, negative means opposite direction. So, in linear regression, let's take the house price example. Then we were predicting the house price. We were saying house price can be predicted. This is the area. This is the number of bedroom. This is the let's say number of bathroom. Let's say this is the distance from market. Right? There can be multiple variables basis which you can predict the price. So, all of these become all on the right become your independent variables. And on the left is your dependent variable. Now, let's say if, if, area and let's say number of bedroom has a correlation of maybe plus 0.95. Let's say plus 0.9. If this is the core relation, then that means they both are almost similar. Area and bed is almost similar. Their strength of relationship is 90%. So, they are exactly the same. They are, they are mostly similar. Right? So, then comes the question. Can you simply use one out of them in the model? What do you need both? Right? So, if I only keep area and I remove number of bed from the model. Right? Because both of them are essentially the same. They are very highly related to each other. Then I don't need to capture both. Right? One will do the justice. Then I don't need to capture both. One will do the justice for me. Right? Because they are very highly correlated to each other. Think about I have two people from the same team who are working on the same project. And I want to do a presentation of that project. So, I can do away with one person. Right? I might not need both. So, how will it help? It will help save capacity of one of them and they can still continue working while other one presents. Right? So, same is the case here. When you are creating the model. In the first case, you are starting with four independent variables. Right? Now, you realize two of them are very similar. They have a very high relationship. Very strong relationship, which means they are very similar. So, you would say, okay, let me drop one out of them and you end up with three. Finally, so how will it help? This will help you run your model quickly in an industry scenario. When you have millions of data and all of that. This will help you save a lot on infra. Right? So, you are right, Prashant. So, this is what we are going. We are saying that if you if in your independent variables, right? Because we say we have loads of data. We run a model on top of that. We look at R square, existed R square and so on. So, if in your independent variables. There are variables which are highly correlated with each other. Then it's worth removing one of them, right? Because they are redundant. If I'm keeping both area and bed in this case, it's redundant for me. Right? Because both of them one out of them will solve the purpose. Both of them are very strongly related. So, if in any case you find correlation between between two or three different variables. If in any case you find correlation between between between two variables. If in any case correlation between two variables is greater than 90 percent point nine, right? I'm talking about strength. Forget about the direction. I don't care about the direction. All I care about is the strength. If the strength of correlation between two variables is greater than point nine or 90 percent. Recommendation is to keep only one. That is the recommendation. Does this make sense guys? Where what is correlation? Where does it fit in? How it's used? We'll use it during hands on. This terminal logic that we are we just talked about is also called multi-coloniality. In case you come across this term somewhere. So, this is what multi-coloniality means. If there are two independent variables which are very highly correlated to each other. We drop one out of them to remove redundancy in the model which is called multi-coloniality. Does this make sense to everyone? And the only thing that we care about is the strength of correlation. We don't care about the direction. We only care about the strength of correlation. If the strength is more than 90 percent, if the strength is more than 0.9 then that's where we say that we need to drop one out of the two. Any questions guys? Does this make sense to everyone? Yes or no? Anyone is not clear. Please let me know. I'll be very happy to clarify but it's an important thing. We will do it during hands on today. So, this is irrespective of the direction. So, this is irrespective of the direction. So, whether it's plus 0.9 or whether it's minus 0.9 in both the cases, I would ask you to drop one out of the two. Does that make sense? Reshant? Direction when the only ten you, whether they are positively correlated or negatively correlated or inversely correlated. Whether they are directly correlated or indirectly correlated. That's what direction will tell you. But the idea is whether it's plus 1 or plus or minus whenever your strength, the magnitude is greater than 0.9, you should keep on the one out of the two. That's what the idea is. Does this make sense, please? Okay. This is how we will leverage correlation and this is how we will try and update the variables that we have. Now, I'll quickly talk about the next thing that we had discussed in terms of how do you manage categorical variables in the data. Let's again talk about house price. So, when we talk about house price, house price can be influenced by area. Let's say the number of bedroom and then let's see once I have the location. When we look at the data. You Let's say you have data like this. When this data, this variable location, it has text values or this has categorical values. It has text inputs. Now, when we talk about computer or a machine, machine cannot understand text. Machine can only understand numbers. Machine cannot understand text. Machine cannot understand text. So, what do we do about it? Because our model won't be able to work on these variables. And these variables at the same time, they are important. Because they are adding information. The way we studied R square and adjusted R square, we also talked about it. There could be certain variables. You need to keep on adding more information and see how your model performance is increasing. So, you can't even drop these variables because it's an input. It's an information that goes into your model. Right. So, there is no option to remove this particular variable. We want to use it in our model. So, how to use it? Anyone has any thoughts? Any idea? Okay. Prashant is saying map it to a category. Let's say you say one of the things that Prashant just talked about. Let me put it here. Same map it to a category. Map category to the numbers. Which means, let's say I, for example, I put north equal to 1. South equal to 2. West equal to 3. East equal to 4. Let's say something like this. What else? What else can I do? Only guess is, come on guys. No guesses. Okay. So, let's talk about what Prashant just mentioned. If we are mapping the categories to the numbers. So, my data will, let's say, let me call it location 1. My data will look something like this. 1, 3. Right. So, what could be an issue with this? What could be an issue with this? Can anyone guess? What could be an issue with this? So, the biggest issue with this is, I have rank ordered these locations. Consciously or unconsciously, I have rank ordered these locations for the machine. For machine, for machine, when we talk about numbers, machine understands 4 is higher than 3, 3 is higher than 2 and 2 is higher than 1. Right. So, anything with 4 or machine, that's the highest number out of all the numbers that are available right now. Right. So, if I'm mapping category to numbers, I have rank ordered these for the machine. Unconsciously, I have given them the weights where I'm saying location east has the highest weight because it's number is 4 and that weight is higher than west because that is only 3. That's higher than south because that is only 2 and so on. Right. So, inherently by giving by mapping category to the numbers, I have created a rank ordering of this which machine will understand as a weight. Machine will give higher weight to E versus let's say w or let's say n or s, which is not right. Right. So, that's why mapping categories to the numbers is not the best option. So, if this is not the best option, then what? Let's talk about that. So, what we usually so pressure and physics clear why mapping category to the number is not the best option. Everyone is it clear why mapping category to the number is not the best option. Let me know if there are any questions. Okay. So, now let's see what should be ideally do. Right. What do we ideally do. So, there is a technique. There is a technique which is called dummy variable creation. Or we also call this one hot encoding. So, this is also called one hot encoding through which we convert our categorical or the text data into numerical data. So, how does this work? Let's see. So, one of the things and let's take a simple example. Let's not take one with the four categories. For now, let's say we are for now. Let's say we are dealing with maybe. Let's say we are predicting income. Right. We are predicting income basis gender. So, you have income. And then let's say you have gender. Right. So, income you have 100 to 150 to 50 something like this. Then you have gender male female female something like this. Right. So, essentially in your gender variable, you have two categories male and female. Right. So, let's see what does dummy variable creation or one hot encoding means. So, what it says is, depending on the number of categories that you have, let's first create those many columns. So, I will create one column for male gender underscore M. And another column for gender underscore F. Wherever for gender underscore M, wherever it's M, I will put one L, I will put zero. Similarly, wherever gender is F, I will put one L, I will put zero. So, this is how you converted your categorical variable into a numerical. Does this make sense? So, this entire variable that you see where it's written M F M F can be represented by these numerical representations. Does this make sense? Yes or no? So, what we have done is, wherever gender was M in the first one, we simply replace it with one. Wherever it was now, we replace it with zero. So, in this column, wherever we have one, right, your machine would know that that particular row corresponds to male. Does this make sense? Does this make sense? Any questions, guys? Yaya will come to that also. But rather, we will come to that also. Wait, first understand this. First understand this, then we will move to location. Let's start small. Any question on this? So, Swapnel, here it's zero and one, right? Zero means it's not there. One means it's there. So, that's what we are representing. So, this particular row that you see it corresponds to a male. So, one is there. That means this row exists, right? This is for male. Second row is for a female. So, here it's zero. Or, no, or machine would know that this is not this is something else. This is not the male, right? Because it's zero, then this is something else at a different category. So, it's not about give me, I'm not giving a weight here. This is more like a binary representation. Right? One belongs to one category, zero belongs to other category. Yes, you can also call it Boolean representation. Does this make sense? Now, let me ask you a question. To represent gender. Do I need both the columns or one is enough? Do I need both these columns? Gender equal to M and gender equal to M for one of them is enough. Do I need both the columns or one is enough? Anyone who feel we need both. So, I have three people, four people who said one is enough. But anyone who feels we need both. So, ideally one column is enough, right? And let's see how. So, if I hide this for a second. And if I only have gender equal to M. So, you would say wherever it's one, it's male, wherever it's zero, it's female. So, you don't need a separate column for female. You would say wherever it's one, it's a male, wherever it's zero, it's a female. So, you don't need a separate column. So, one column will do the job for you. Is that correct? Understood everyone. And the reason why we want to remove or keep on the one column again is to remove redundancy and make it less of a load on the infra. So, if these two columns having both male and female are not adding any value, right? So, better keep one. It will help run your model faster. But let's load on the machine, let's load on the system. So, ideally if I have to generalize it. So, if you have n categories, you only need n minus 1, damees. So, the variable that we have created, they are called dummy variable. So, if you have n categories, you only need n minus 1, damees in your data. So, in gender, you had two categories. So, we only need one dummy variable to represent it in a numerical format. Now, let's move to location and let's start a fresh, right? So, in location, you will have one column for north. I mean, then you will have one column for south. You will have one column for west. You will have one column for east. Right? So, you will have these four columns. So, wherever it's north, you will put one and see what put zero. Wherever it's south, you will put one and see what put zero. Wherever it's west, you will put one and see what put zero. Wherever it's east, you will put one and see what put zero. Right? So, the same thing that was happening with gender, right? I think one of you had this question, what will happen to them? Radha, you had this question, right? So, the same thing happens here. Again, here you have four categories. So, you only need three dummies. You don't need four dummies and see how. So, now in this case, if I randomly hide one of them, let's say I hide east, right? So, you can see from the data that wherever all three of them are zero, right? It's zero, zero, zero. That means it's something else. That means it's east. So, even with having these three columns there, I can represent all four categories. One, triple zero means north, zero, one, zero means south, triple zero means east, and zero, zero, one means west, right? We can still represent this entire column with B3 columns, right? Three dummies. I don't need four dummies for this to remove the redundancies. Does this make sense, guys? So, the simple rule of thumb is, if you have any categories, you only create n minus one dummies. This methodology is called dummy variable creation, and we also call it one hot encoding. These are the two names. This is the favorite question of an interviewer. You'll get this question in an interview. This is helpful. There is a perspective of the technique that you're working on because this is a data free processing thing. So, whenever you are using linear regression or as complicated as neural network, you will have to use this because in realistic scenario and industry, you will have some data which is textual data, right, which has categories in it. You will have to convert it into machine readable format, which is numerical. There is no other option. This is the only way to accommodate them. Yes, they will increase the load, but again, they will add value also, right? We have predictions. They will make your model more accurate, better depending on how relevant is the variable. Right? Make sense? Does this make sense? Everyone is it clear? This is a very important concept. You will use it almost in every machine learning exercise that you will do. Okay. If it's clear, then I think we are good with both the topics. I'll stop here for a break. After the break, we'll start with the hands on. So rather we do care about compute and efficiency and all of that, but not at the post of model performance, right? We'll only remove the pieces which are done. And for example, we are already saying that if you have any categories, create only n minus 1, right? You can also create all n, but they are not adding any value. So we say n categories n minus 1, so we are trying to remove that redundancy. We are trying to remove those pieces that could impact the compute, but not at the cost of model performance. So location, let's say, has a bigger role to play in in your house price. And if you are not including location in your model, so then that's not going to like your model will fall back on performance, right? You might not get the most accurate performance. Does this make sense? Does that make sense? Radha. Okay, perfect. Great. So well, stop here for a quick break. I'll see you guys after 10 minutes and we'll start with the hands on. So we'll start with the hands on offer of the linear regression. Thank you so much everyone. Even 10 minutes. You You You You You You You You You You You You You You You You You You You You You You You You You You You You You You You You Everyone back from the break. Now we will start with the most interesting part. The hands on. So I'll share my screen. I would also ask all of you toyttrady with the earth for lab notebooks. where you could start with the fresh notebook. I won't be sharing any files and all of that. So start with the fresh notebook. Let me know once ready. Let me know once everyone is ready. Meet up fresh internet, knowledge popular passage from the whole house to detail the suspects cases. when you discover thesefreiboards, similarly, in which their you And everyone ready? I'll not be not people's annoying. What about others? Okay, perfect. Anyone who is still doing? I can wait for one more minute. Okay, that's fine. So we will keep this case study relatively simple and we'll try to apply all that we have studied and now. So let me explain the problem statement quickly. So in this problem statement, we have the data for fish. So we have eight of the fish length, different lengths of the fish height and width and species. So what we need to do is we need to create a model that helps us predict weight of the fish with respect to the different information that we have. That's what we need to do. That's fine, Rada. Does this make sense guys? It's problem statement clear. So you are a data scientist. You need to create a model that helps you predict weight of the fish. There are certain information that is given to you to do that prediction. I took you that model and that's what you need to do. Is problem statement clear to everyone? Anyone who is not clear? Okay. Sorry. So let's move ahead. What do we do as the first thing? First, we need to import the libraries, right? The first thing that we need to do is we need to import some critical libraries. So I'll do one more thing. So can any one of you send me a question in Q&A? So there are a few codes that I will paste in that Q&A question. So zoom does not allow pasting codes from the chat box, right? So if I paste it in the chat box, you won't be able to pick the one methodology that I follow is that 1% sent me a question, which I think someone already did. So I'll just paste the codes in that question and you will pick it up from there. So these libraries as they are pretty common. There is nothing new in terms of typing it them out. So I'll just paste them in the question. Peshant has sent the question to have pasted these impressions question. So you can go to all of you can go to Q&A tab in Q&A. You will go to answered tab and there you will find a question from Peshant. In that I have responded with this code. So you can pick it up from there and you can run this. But let me quickly talk about it. So we are importing on the basic setup libraries. So we are importing pandas. Pandas is used to import the data to write to to play with the tabular data. Then I'm like some of these libraries would be new to you because all three of them. I have associated with the model. So let me explain them a bit. So there is a library called scikit learn. Which is also called referred as SK learn here. So the scikit learn library has all the models. This scikit learn library has all all the all the functions of the components associated with machine learning or the model building. So you could see that from scikit learn. Model underscore selection. We have imported this train test plate. Now as a part of model creation, you remember right before it's need to divide the data into train and test. We need to divide the data across train and test. So this function train test plate helps you divide your data across train and test. Similarly, it also has the model model libraries or the model components. So you could see from scikit learn. There is something called linear model which has this. Function of linear regression. To run linear regression, right. So here we are saying from SK learn import linear regression. Similarly, there is something called sk learn. mittrix where they have all the model evaluation metrics. Component stored so mean absolute error means squared error R square all of them are stored here. So you can see all three phases of model creation starting from dividing data into train and test importing the model. And then also the evaluation metrics all of this is housed in scikit learn. So this is a very important library when it comes to machine learning or model creation. Does this make sense guys? Does this make sense guys? Anyone who is not clear? Okay great. So everyone imported these libraries? No, SK learn is the only library version which has all the modern stuff. No, SK learn is you will extend real world scenario whatever we are going to use is used in real world scenario only. Okay, so let's quickly talk about the data if there are no questions or libraries. So I think that's a good question. Mother we how do we know which libraries to import? So there are like for every use case there are a basic set of libraries that are required to be imported right. For example, let's say you need pandas to import the data to play with the data to pre process the data. So pandas you would anyways import right if you want to run a model. So you need to import through scikit learn you would need you so to run a model you need to split the data into train and test you need to evaluate it for its performance. So you definitely need scikit learn both both the metrics piece and the test train split that the same time because you need to train the model. So from scikit learn you will also have to import the model file right the technique that you want to run in this case we are importing linear regression. So there is a basic set of libraries that we import irrespective of the use cases right and these are the libraries that are shared with you. In some cases you might also import a seaborn or matplot where you would want to do visualizations. Yeah yeah this is fine. That's okay. Marvi does that answer your question. So let's look at the data a bit so in our data we have 159 records we have seven columns. So features are nothing but the columns so we have seven columns where we have three lengths right vertical length and centimeter diagonal length and centimeter cross length and centimeter we have height of the fish we have width of the fish. We have species name of the fish which species does that belong to and then we have weight of the fish so this is what we need to predict. So this is our dependent variable right or this is our why all of these are x or independent variable. Does this make sense everyone aligned. So we have 159 records and we have these seven columns where we have your mate of the fish is the dependent variable it's the y and then all other are the x is independent variables. So everyone clear any questions anything which is not clear. Okay so what is the first thing that we will do now what is the first thing that we need to do now to get started with the model building exercise. Now what is the first thing that we will do correlation is not the first thing Christian. You got this problem statement right so what is the first thing that you will do to create this model the problem statement is you need to create a predictive model that helps you predict weight of the fish so how will you get started what is the first thing that you will do. So by the way that we already did but what is even step before that what is the first thing that we need to do to get started. The first step is to import the data right you won't be able to do anything without importing the data. As step one you need to import the data so how do you import the data. How do you import the data. Which library is there to import the data. Which library do we use very good and what function do we use in pandas to import the data. Very good. So there is a function called read underscore CSV in pandas that we use to import the data so this is something which now I want you to type it out. So I will share the file name what is there in the bracket because it does not make sense for you to type it out but you need to type out this function and story or data and something called data. I will use same question that Prashant had shared earlier and I will paste this file name there. I have pasted the file name there so the step one or first thing that we need to do is we need to import the data right so you need to use pandas. Read CSV function to import the data so please type it out. What PD pandas not read underscores CSV and then take it forward. Guys I want you to practice with me so type it out. I am not sharing any codes until unless they are repeatedly used or they are very huge to type and not that. Let me know once done. So Prashant you run it at your end. What is the error? Prashant you have typed out read underscores Excel. Read underscores Excel is not a function. You need to import CSV it is a CSV file. Here it is on the screen that you need to type it out and file path have already shared. Does that make sense Prashant? Are you using Kulab only? Madhavi that error should not come. Just restart and then run. Is everyone able to import the data? Take your time guys. No rush. So the board is there in the bracket. You have shared with all of you. So you need to write pandas.read underscores CSV, put a bracket, put these, put this there and then run. Let me know once everyone is done. So you have imported the data. What is the next thing that you will do after importing the data? Can anyone tell me? We need to check whether the data has been imported correctly or not. Sure as we need. Take your time. So before, so let me put this step. Step 1, you import the data. As a next step, you need to validate the import. Whether the data has been imported correctly or not. So what all will you look to validate the imported data? Very good, Rada. So you are saying you will look at a sample. What else will you do? Okay, so you will look at a sample. Then through that you will get to know whether it has all variables or not. So the next thing that I will look at is the data. Whether it has all the variables or not. Whether they are correct or not. So very good. So these are the three things that you would want to validate. So let's first start from the second one that Raman said. Looking at the dimensions. So how do I check the dimensions? I know there should be 159 records and there should be seven features. So how do I validate what function will I use to validate that it's 159 comma seven the one that we needed. What function do we use to validate it? But there is a function called step, right? Data.Shaped through which you can get the number of rows and number of columns in the data. Right? There is a function called data.Shaped through which you can get the number of rows and number of columns in the data. So please give it a short. Help me whether you are getting 159 comma seven or not. So this is the function that you will run, right? Data.Shaped. So you should get 159 comma seven which means there are 159 rows and seven columns. So this will this will elevate Raman's concern that whether all columns and rows are there or not. Right? Very good. I want other also to type it out. Guys, the more you type it out, the more you will practice and you will get more confidence. In my coding classes, I always push people to code it along with me to type it out. It's easier for me to share this code, right? It's very easy for me to share this file and then all of us just run it together. That's the easiest thing, but that's not the best thing because that won't give you lot of confidence. You won't be able to practice and anyways people don't go back and revise at home. So my objective is like make them practice as much as possible there. So please type it out. Type out data.Shaped and see if you are getting 159 comma seven or not. All of you should get this. So this will help you elevate the dimension concern. It Ashwini are you able to follow? Ashwini, did you were you able to import the data? Jyoti, did you use this function data.Shaped? You are not able to import the data. Why Jyoti? What is there any error or something? I can help you resolve it. Okay, let me share this entire. What is coming? Jyoti, what is data? You should also be able to debug it. And I'm sharing this entire code and press on question. Right. So now you can pick it up from there. Run and see if you are still getting the error. I have shared the entire. I have shared the entire code. This one in press. Hans question. So go to q and a go to answer tab and pick it up from there. Ashwini and Jyoti. I have shared the complete file name and I have shared the complete code in press. Hans question. So you can pick this up run and then let me know if you still get the error. Right. So, and if you are if you still face any error, then maybe later on I'll ask you to share your screen and we will resolve it together. Right. So we can do that also. So now through this we can see there are 159 rows and seven columns which are imported. So we are good from dimension perspective. Right. Now we want to look at the sample of the data and see how the data is looking like is everything aligned or not and so on. So how do I look at the sample? How do I pull a data sample? What functions are available to pull a data sample? Very good. Radha. What about others? Very good. So you can use data.head or data.tl to it's working now. Perfect. So you both can do very quickly data. Shape and then you can do data.head to print a data sample. Right. It will give you top five rows. Similarly, there is something called data.tale which you would have studied. Right. So you can print the last five rows. But idea is to look at the sample. When I do data.head I get these top five rows which has species. Right. Which species the fish belonging to. What is the weight? The three lens that we have height and width. So through this we can look at the sample and we can see that all the columns are pretty much aligned. There are no delay, delination errors and all of that. All of them are pretty much aligned. So we are good looking at the samples also. Everyone did data.head data.shaped please take a minute and do it guys. Okay. Let me know once done. So now we need to look at the third thing which is the data type. So what the data type means is. So ideally if you look at these columns. Right. So you're all these numerical columns should be tagged as data type equal to numerical which is integer or let's say. Right. The decimals and all of that and your species should be. So let's see whether they are tagged correctly or not. If not then in some cases you need to change the data type also. So how do you print the data type? Do you remember? Have you guys studied? How do you print the data type? Do you remember what function do we use to print the data type? Very good. Richanne. So there is a function called data.info. You run this where you get the data type also. You get loads of information which includes data type also which you will talk about. But first run data.info. First run data.info. So this will also take mark your third piece. So once you run this you can see you have a data type column here and we can validate that species is an object. So it's correct because it's a categorical variable. All others are the Americans so you can see their data type as float which is correct. So even our data types that have been imported are perfectly alright. Everyone with me for data type. Very good. Another piece of information that you can see in this particular output is your non null count. How many values are missing? How many values are non null? So in this case you can see all 159 are non null across all columns. 159 are non null. That means there are no missing values in the data. In the total data we have 159. All those 159 are non null. Right. Which means which means there are no missing values in the data. Does this make sense guys? So this data.info is giving you two information about your data. One is you can validate the data type. Second you can also look if there are any missing values. So in this case your data types are correct. There are no missing values with me. Everyone able to follow. Very good. So let's move ahead now. Let's see what is the next thing that we do. So any cases on what is the next thing that we will do now? Or model building? We will look at the correlation. Right. We will look at the correlation and we will see if we need to drop any variable. So describe it all as mostly for those cases where let's say you want to do the ED and all of that. So for these classes I will focus more on model building. I won't go into ED. We will focus more on model pieces. So the next thing that we are going to do is we are going to look at the correlation between the variables or the information that are available. Right. And we will see if we need to drop any particular variable. That's what we will do. Right. So now the correlation can only be calculated between the numerical variables. It cannot be calculated between the numerical and a categorical variable. So the first thing that I'm doing here is I am filtering out only numerical variables in our data. So you could see numerical underscore data. I am doing data dot select and there's four types and only including float 64 and integer 64. The ones that are numerical in nature, that's what I'm doing. I'm not including anything else. Right. And then on this new data that I have created numerical underscore data. I'm simply doing numerical underscore data dot core. P whatever. And it will give me this correlation matrix. Right. Let's first try to understand this correlation matrix. Then we can go back to the code. So in this correlation matrix, you will you can see the columns and the rows, right. It's a n cross n matrix with all those columns out there. So this particular cell means what is the correlation between weight and weight. So it's 100 percent right because it's the same column. If you calculate the correlation between weight and weight, it's 100 percent. So in the diagonal when it's when you are calculating it for the same column, every where you will see it to be 100 percent. Right. So you can ignore the diagonal. You can ignore the diagonal. Right. But another way to interpret is let's say if I want to look at the correlation between weight and length. Right. So 91 percent is between weight and length. 72 percent between weight and height and 88 percent between weight and width. Right. But our weight weight is our dependent variable. So we are not going to do anything related to the dependent variable right now. The idea of looking at that correlation was filter out any independent variables that are highly correlated with each other. So I'm removing wherever we are we have the correlation present with the weight or the dependent variable. I'm just removing or hiding that. Right. And let me also hide the diagonals. So that we can focus on the code data. Right. So now if we look at these data points. Now if we look at these data points right. So you can see if I look at the correlation of length one with length two. It's very high. It's 0.99. Length one with length three is very high. It's 0.992. But with height and weight it's not that high. It's less than 90 percent. Right. So wherever it's low, I'm marking it in green. Right. Wherever it's less than 90 percent, I'm marking it in green. Right. So what you can see is what you can see is these lengths. Right. Length one, length two and length three. They are very highly correlated with each other. So if I'm talking about length one, length two, length three, they are very highly correlated with each other. So length one correlation with length two is 0.99. Length one with length three is 0.99. It's similarly. Right. You could see these are nothing but the same combinations out there. So if I have to summarize it, length one, length two and length three are very highly correlated with each other. So I can keep only one out of them. You can keep anyone. You can keep L1, L2, or L3 depending on whatever you like. There is no straight answer to that. It because all three of them are highly correlated. So only one should do just this. So what we can do is we can keep only L1. We can drop L2 and L3 from our data to solve for model complexity. Is this clear to everyone? Does this make sense? Yes or no? Does this make sense? Yes or no? Is it clear everyone? Sure. So before the break, what we studied was if there are independent variables that are highly correlated with each other, we keep only one out of those. So we took example of house price prediction, right? Basis the area in the number of bread. Right? And we said if the correlation between area and number of bread is less than really high. If it's greater than 0.9, we can only keep one. We don't need both. So we can just do house price as a prediction of area. We don't need number of bread because essentially they are one or the same thing. They are they both are the same thing. That's what the data is telling. So same is the thing we are doing here. If you look at the correlation between length 1 and length 2, it's 0.9999. L1 and length 3 is 0.9992. So they are highly correlated with each other. They are one or the same thing. So I don't need to keep both. I can keep only one out of the three. Does this make sense? And then 2 and 3 also have a very high correlation. It's 0.994. Yeah. So they are showing the direction. Prishant is positive only all. Direction is positive. Does this make sense now, Raman? And everyone. So through this data, through this output, we identified that we only need to keep one length out of these three. Keeping all three is redundant. Yeah, but positive negative does not matter Prishant. We are only looking at the magnitude. What we are interested in these trends. I don't care about the direction. Even if we would have negative here in front of it, we would still have done the same thing. We don't care about the direction. We care about how strongly they are related to each other. And if they are very strongly related to each other, which means they are one or the same thing, then we simply say that, okay, we will keep one. Remove the other. Does that make sense? Okay. Sure, I think we should get that data as we move forward. It's not there right now for sure. Okay, so let's go back to the code. Right. So first, we need to select the numerical data types, which is a standard one. And then you simply do the numerical data. So the first one I'll share with you. This is a common code that you can use anywhere to select the numerical data points. So this I am pasting in Prishant's question. After this, you need to type it out. The first run this, then you need to type out. You can simply do numerical data. Or you don't even need this. And you will get this output. So you simply need to type out this statement. So I am first including the numerical data points and then taking the correlation. Does this make sense? Everyone please do let me know what it's going to be. Yeah, I did the first line I did second, I didn't. I did. It's there. Scroll down and see. Click on show all that's there. I can paste again. Is it able to see it now? Is there is any? Click on show all and scroll down. Click on show all you will get it. Let me know once done. Very good. Is there any are able to find it now? Very good. Everyone done? Okay, the next thing that we need to do now is we also need to do the one hot encoding. Because there is one column that is categorical in nature. So if we look at the data. If we look at the data, the species column, species column is categorical in nature. So I need to convert it into an numerical through one hot encoding. So while we were looking at the one hot encoding on the white board, the illustration that we did. So it looked it looked simple, right? But then all of you had that question. Oh, how will we code it out? So essentially it looks simple, but how do we code it out? So coding when it comes to machine learning is not at all difficult because it's maximum one line or the two lines that will solve things for you. So to one hot encode the data, there is a function within pandas called get dummies. So within pandas, there is a function called get dummies. GT underscore dummy, the you double MIS in this as an input. You simply pass your data. So we have named our data as data right. So we'll pass our data. We will specify the column that we need to one hot encode. So we put the column name and then we do drop first equal to true. So what this drop first does is it ensures that for end categories, we are creating only n minus one dummies. So this ensures that for end categories, we are creating only n minus one dummies. If you want all end dummies, you don't use this drop first equal to true. But ideally it's recommended to use it. So let's just run this command. I want everyone to type it out and run it. Create a new data data and that's for OHE. Very well stored this. And then we can look at the data sample and see the new columns that are created. But first type it out and run this. And if there are any questions on this, then let me know. Ashwini are you able to follow Jyoti are you able to follow? What about others? Anyone any single person in the class who is not able to follow? You need to tell me. I'll make sure that I am able to solve your doubts. Rehwati are you able to follow her on? This particular code is reney to perform one hot encoding. So let me remove the annotation. So this code you need to type out to perform one hot encoding. We need to convert our categorical column which is species into numerical right? So please take everyone take from in that send type it out. So this dummy is doing nothing but what we discussed in the class before the break Jyoti. We were saying let's say if you have gender male and female right? Two categories male and female. So it will convert that gender column into two columns gender equal to M gender equal to F. And then you will put one everywhere it's male under the male column L0. Similarly in female column where it's female you put one L0 right? And then we discussed that you need to drop one column because one column is enough for two categories. And minus one columns are enough. So that's what this drop first is doing. We are dropping one column to keep only in minus one categories. Does does that make sense Jyoti or is there anything else that you want me to repeat and explain? I'm happy to do that. He great. How many people have done the typing? How many are still doing? There is no rush. Take your time. There is no rush. As I said earlier I'll extend this class by 15-20 minutes. Right? I don't want to leave this hands on in between because all of you will forget it. So I want to complete the model part and then leave it. We are almost there. We are done with the data preprocessing. Yeah, but I would recommend you to write. So Raman the reason you are getting the error is because true T should be capital. You have name but it's small T. So some of these things I want you to debug on your own as well. Guys, when you will do it in the out there in the industry, no one is going to solve your errors. So just look at where you are getting an error. What could be the potential reason and then try to solve it. Right? Instead of I would say spend a minute on it and try to solve. That is that is good thing as best practice. So now I am here to help you solve the errors. But tomorrow I won't be there. Right? So you will have to sell salt. So before directly pasting it here, just give it a thought and see. Like what could be the error by this is coming and then try to solve it. If it's not solvable, then anyway, I'm here to help. Right? So those two are false rather machine will interpret as one in zero only. So now let's do a quick print of this data. So simply do data underscore OHD OHD.head. Let's do a quick print of this data and see what happened. So now I'm just data species column is nowhere. You cannot find species column anywhere. Species have been replaced with these columns. So species and it's for perky, perch, pike, roach, smell, etc. Right? So there are six columns that have been created for species now. Right? And under these you can see false true false true, right? So this falls for the machine as zero true is one. So you don't have to convert false and true into one and zero. So false and true are automatically interpreted by machine as one and zero. These are Boolean terms. So you don't have to do anything about it. But what you can see is your data has been converted exactly in the format. The way we have studied about it, what one not encoding or the dummy variable does. Does this make sense? Everyone able to follow yes or no? Yes or no? Anyone has any question on this? Any doubt? Yeah, that is missing because we will create we we have dropped one category right? Mother we we are saying for n categories we only want n minus 1. That means that's why one category is missing. Does that make sense? Yes or no? Anyone any doubt? Yes or no? Yes or no? Yes or no? Yes or no? Yes or no? Yes or no? Anyone any questions? Okay. Yara dha please tell me. Yes Rara please tell me I can answer every question. No because it does not make sense right? So all false means this belongs to a category other than these. So all false means that this particular row that you see here it belongs to a category other than these. So that automatically gets taken care of. You don't need a column to represent it. So the idea is to segregate which row belong to what species right? So where all of these are false you can easily say that these rows belong to that species. Does this make sense? Rara does that make sense? You can still add the column if you want but just that it's mathematically you are trying to reduce the computation and remove the redundancy. Okay let's move ahead now we are done with the processing part. So let's move ahead and we will we will go ahead and create our model. So the first thing that we need to create our model is we need to segregate X and Y from the data. So the first thing that we need to do is we need to segregate X and Y even before splitting the data into training and test. We need to segregate X and Y because X and Y cannot stay together in one data else they will mix up right? So we first need to separate X and Y which we are doing here. So see how we are doing it here. So I am creating something called X you can put it as capital X or whatever you want to name and then something called Y right? In Y I am simply passing weight column because this is a dependent variable right? So from data underscore O H E I am passing only weight column to Y right? This was our dependent variable. Whenever in X from the entire data we are dropping weight. So from our entire data to get the X we are simply dropping our Y and then I am also dropping those other two columns length to and length three which were collinear with length one. So this will become my X this will become my Y and then I will pass it on to test train split. So we will come to this don't going to this for now. Let's hide this for now. Let's take it step by step right? So you first all of you type out these two codes. We are in the first one in X we are dropping Y and the variables which were highly collinear. So I am dropping so think of it as simple as that right? I have this block of data in this block of data I have let's say let's say this is my Y and let's say this is my X right? In this block of data let's say this is my Y and this is my X. If I need to segregate these two right? So from this block if I chop off from this block if I chop off Y. So I would be left with X. I would be left with only this right? So that's what we are doing in the first case from our overall data we are removing this Y. We are dropping Y and I am also removing those highly collinear columns and then we are left with this AX. Why we are simply storing weight in a new data set that becomes our Y. Does this make sense? Does this make sense everyone? Please type it out. Ashwin need is this make sense? Radha does this make sense? Madhavi. Everyone. So the first thing that we need to do to create the model is we need to segregate our X and Y. So from our data set we know the weight is the Y right? Weight is the dependent variable. Everything else is independent variable. So from our data if I remove weight right? So I will get the set of independent variables. So that's what I am doing in step one. So I am just removing dependent variable from my overall data and I am retaining independent variables. Let's do a length three I am removing because we saw initially that they are highly correlated with each other. So we might not need all three right? To segregate Y I am simply storing Y in another bucket. That's what I am doing. These are the two things that I am doing. Does this make sense Ashwin? Okay everyone clear, pressure on this is clear. Let me know once all of you have typed it out. Anyone who is still doing? Anyone who is still doing? Okay take one minute and please complete it Ashwin. Thank you. Okay. Are we good to go? So next what we are going to do is now that we have segregated X and Y. Now we will split the data into train and test. So we will have X train X test, Y train Y test. Something that we studied. So you have the entire data used, split it into two parts. Train and test for you to train you. Students training data to create the model and test data to evaluate the model. So the function that we have to do to split the data into train and test is called train test. Flit train underscore test underscore split something that we had imported from circuit learn library. Right. We had imported this from circuit learn library, train underscore test and standard sports split. So in this function we simply pass our x whatever is the independent variable. We simply pass our y whatever is the dependent variable basis these x and ys that we have created here right you need to pass them here and then we specify something called test standard sports size. So if you remember we had discussed that right that you have one big data you try to create two parts to it. One is train another is test and then we discuss train we keep mostly 80% test we keep 20% right. We are doing the same thing here. So we are putting test size equal to 0.2 which means test data is 20% train is 80% that's what we are doing right. Now the best part of this particular function train test split is that you don't have to create four different buckets like x train x test y train y test you don't have to run this code or this like piece of line four times you simply put x train comma x test comma y train comma y test you run this function and it will automatically put the data into these four buckets will automatically divide and allocate the data to these four buckets. So it's as straight forward as that. So does anyone have any question on this particular line of code the train test split you will use it every time you do machine learning. So anyone has any question on this the train test split if not in the interest of time I'll share this line of code and Roshan's question and maybe we will type it out when we do logistic. But is there any question on this code what it is doing and all of that? And after this you can simply print the shape of x train y train x test y test to see whether it seems 80 20 or not. So here I have printed the shape of x train and x test so you can see train is 127 test is 32. So this is 80% of the data this is 20% of the data so you can see all of this. So first split the data into train test right using test train train test split function do 80 20 split and after that you print the shape of train and test to see whether it's truly 80 20 or not. So we are just making the data ready to run our model now after this point we will simply run our create our model. Does this make sense? Yes. Is a shmene is it clear Jyoti are you able to follow? Is it clear guys what we are trying to do here? Anyone who is not clear? I am happy to clarify because after this would be the last step where we will create the model. Okay if there are no questions I will move ahead very good. Yeah because you are printing x train and y train so in y train you have only one column right. Varun you print x test also so you will get this. Okay. Okay so now we will create our model so model creation is just one line of code right as I was telling you guys while we were discussing it and like I told you it's very easy from coding perspective so let's see so this is the last piece of code that I'm going to discuss today and we are going to create our model. So the linear regression that we had imported in the first step right I'm storing it in something called model so I'm doing model equal to linear regression you can give it any name right I'm storing it in something called model. Now the first step that is there for model creation is to train the model right we need to train the model how do we train the model so to train the model we discuss that we will use training data to train the model so there is a command called model dot fit so dot fit is the command that trains the model so if I have to put it here dot fit is the command that trains the model you just need to input your training data so in this you can see we are doing model dot fit and we are passing our training data x train and y train model we have to find as linear regression right so we are simply doing linear regression dot fit fit linear regression on this data x train and y train so this creates my model so this particular piece of code will create my model so now I have a model in place now using this model that we have created we will we will predict on test data because we want to evaluate right we want to evaluate the model to evaluate the model if you remember we study that on test data we compare actual versus predicted right we have started this on test test data we compare actual and predicted so you can see in this step I am storing this in virenders core thread where using this model I am predicting on test data so you can see pre dot predict x test right so using x test that we had kept aside for evaluation and I am just making my predictions on that particular data and storing it in y thread after this we need to compare actual and predicted right so after this I am calculating the r square where I am passing the I am passing the component that we had imported right from scikit learn dot matrix r 200 square into this I am giving two things my y test and y thread so my y test is the actual my y test is the actual right y thread is the predicted so here we are doing nothing but we are comparing actual to predicted to get a r square which will help us understand the model performance right does this make sense to everyone so to simplify as step one we simply trained the model using training data leveraging dot fritt command in second step we predicted on test data using the model using train model and then we are simply evaluating using r square by comparing actual which is your y test to predicted the one that we did in step two does this make sense guys this is what is happening here does this make sense this is easy right nothing complicated at least that at least that complicated what we studied behind the scenes Jyuthi what do you mean so let me share this piece of code with all of you in Prishan's question you run your model and tell me what is the r square that you are getting I am getting r square as 94 percent what is the r square that you are getting do type it out in practice when you go back which is okay so your r square can be different in my r square because we have them random split right my test data could be different from your test data all of us are doing random split so my r square can be different than your r square so you don't have to worry about that the outcome should be you should have a r square printed on your screen don't go into the value because value can be different my r square can be different than yours right we have them random split so at this step we have them random split of the data your test data can be different than my test data and because it's a very small data right the results can fluctuate so it's okay even if you are getting 85 percent 90 percent 95 or whatever I don't have to worry about it but the good thing is all of you are getting an r square to assess the model performance so with that many congratulations on successfully creating your first machine learning model which is linear regression right I will stop here to answer if there are any questions but do tell me if there are any inputs on the hands on that we did in terms of how we practice in terms of the pace etc so that I can update it so that I can update it right in when we study logistic regression so if you have any feedback or anything on how we did hands on right on the pace or anything do tell me right now I'll be like you can definitely fill the feedback hole but I would also like to hear right now in the class so that I can make those changes when we do the hands on for logistic regression right but but do give your feedback in the feedback hole as well it has been shared it's very important for me to understand what is working what is not working why you guys fill the feedback hole I'm happy to answer if there are any questions or doubts what is what is the doubt Jyoti where are you stuck can you share your screen Jyoti I can help you solve so can you share your screen Jyoti is it possible for you to share your screen let me make you the panelist and anyone else who is facing any issues guys I'm here to resolve your doubts right so just share your screen and I'll help you solve those doubts Jyoti I have promoted you to the panelists thank you so much Ashwini and Prashan do share your feedback in the feedback hole it's very important do take out a minute to fill the feedback hole please so Jyoti can share your screen thank you so much Radha do share your feedback in the feedback hole people who don't have any doubts or anything can feel free to drop off Jyoti initiator screen just just a second let me also switch on the audio I'm not able to hear if you are speaking just a second