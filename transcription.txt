 back to the session. A quick check and I request all participants to please confirm that you're able to see the screen and also if the audio is clear. Then we have a quick guess on the chat. All right, good. All well. Had a good break last Sunday. Had some time to relax rest. Probably sleep a little late also, I believe. Everybody, last Sunday. I'm sure good. So feeling fresh, right? I hope everyone's got good energy because we have lots for this weekend. Way too many things actually. I'll talk about what those are in some sometime from now. But I hope everybody is 100% here in the session, right? Because we're going to do some good stuff this weekend actually. And I'll tell you what exactly. But we're waiting for people to join in because I wanted to set the agenda for today's session and more importantly, also tomorrow's session. So we have tomorrow also a follow-up session as well. And I'll explain what we're going to actually do. I'll also take some feedback from all of you in terms of like what we want to actually cover, what we want to do, etc. Yes, definitely. We'll bring in deep seek, everything, right? Whatever you guys want to discuss, all of that we will actually get into it. Yes, this week, Heyman, it's going to be 8 to 11. We're going to do it an extra hour. Two things. I'll explain that. That's where I'll come back to what the agenda is for these two days. And I want to focus on certain important elements, right? I think few of you had asked for a few important concepts to be covered. And I'll come to that because I know the entire class is not here. So I don't want to get the agenda items for tomorrow today itself, right? So I'll come to that probably a little later towards maybe towards the break, right? When we take a break at around 930, we'll quickly check with all our participants. What is it that they want to cover? What are the additional things that they are interested in, etc. All of those things we'll talk about. So while we cover the important things, things like image for your low model, I'll share those with you separately. Because that's not very difficult. It's just a code change. That's all right. So we'll utilize our session for more important things because that's like going to be about two lines of code and then the entire batch doing that would not be very helpful. So that's where we will focus on very important things. I think they did right. It was shared, I believe, Harish. An email was shared. Vinay, can you confirm? I think there was an email sent out to all the batch mates. Like today's going to be a longer session. Vinay, are you there? Yeah. All right. I'm not sure. But don't worry about it. I think the recordings will definitely be there. So no need to worry about it. We will cover all the concepts also and make sure that you have enough time as well to go through it. See, use today's session as well as tomorrow also. We'll have another one day where we are not going to focus only on generative AI for computer vision only. We are going to generally talk about generative AI. That's where I'll take a feedback midway during the session to understand what are the additional things you want to get covered. Bear in mind, we'll try to cover as much as we can. But you have to remember that AI is generative AI is especially dependent on hardware. So when we don't have access to high amount of hardware, there might be cases which we might not be able to cover. But definitely, we'll try to see as much as we can. That is a separate module. Yes, Hemant. We'll have a separate thing altogether for deployment. So that's an AI apps. So those things which are covered in deployment, MLOps, etc. All those processes, AI ops, etc. Everything is separate. That's a different term altogether after this. Anything related to genie AI, we'll talk about that like what all we want to additionally cover in the session. Sounds good, everyone. Yes, shall we get into the session very quickly then? Because we have a lot to cover today. And more importantly, we want to be able to keep time for additional things as well. So very quick discussion about genie AI. Now we are talking about genie AI not with respect to text models. We are talking about genie AI with respect to computer vision models. So bear in mind that this particular focus here is not about generating a chatbot or making it work like a chat GPT or a GPT of any of those sorts. Rather, it's about how do I generate images or the output is basically an image that can help you to create. I wouldn't say fake, but an image that is a new content altogether. How does the model actually visualize the inputs that you provide and create a new image or a new content or something totally new which is not really created by human hands basically. So that's where we last week talked about an example of pictures over here of people. And interestingly, these are photographs which are of people who don't actually exist in the world. These are fake images that have actually been generated by the algorithms themselves. So till now what we have learned is most of the algorithms are actually built on the concept of discriminative AI. So we've come across scenarios where they've been able to discriminate between, let's say for example, a cat and a dog. We have a CNN model for example that can do a discrimination between a cat and a dog, a patient with a heart disease versus no heart disease or being able to classify a tumor which is malignant versus non malignant. All these are the kind of use cases that we've generally come across. But what we're generating model what happens is that you know the models actually move to another level where the idea is not about just discriminating between cats and dogs. It's also about being able to generate an image of a cat and a dog. But the point here being that the image of the cat dog or whatever object is generated has is not actually reality that that particular object doesn't exist in reality. It is an interpretation that the model does based on the training that has been provided to it. So that's where if you notice training is basically all the data that you provide as images. The model is storing all of this information within the weights and biases. And when required what it is able to do is generate an image or generate an output which resembles the data that it has been provided in terms of training. How does all the math work that is exactly what we are going to talk about today. Right I will explain to you those elements in terms of you know the elements in terms of how exactly the math is or how exactly the entire layers are etc all of those things we will talk about today in our session a little more in detail. So that you have a better understanding of the process itself. So that's where the entire idea of generative models especially for video text sorry for video images etc came into play. Now text is something we are not really focusing on because we already saw the text element part of it where in our NLP two sessions we saw how exactly you can use large language models to generate text right to and text generation could be anything like text completion could be a use case or you could use it as a chatbot you could use it as a Q&A you could use it as a sentiment analyzer a classifier whatever you want to do it that's the idea behind the generative model that you have come across in the case of text that we have seen. Now here however we are looking at a different set of problems altogether right. So here the idea is as I said not to generate text but the idea is to be able to generate image or a video in this particular scenario. So how does the math work that's something we are going to deep dive today into and then what we are going to actually do is once we are done with the math I'll show you two examples of an image generation use case one is using what we call as a generative it was real networks and I'll explain that what it is and the other is using what we call as a transfer learning model built on something called stable diffusion right. So we're going to look at both of them one the the model that is transfer learning is called as dally and the model that is we are going to build ourselves is called as again a generative it was real network right and we're going to see the working of both these models and how exactly you know they're able to do a good job or bad job etc all of that we will come to it now. So that was briefly what we talked about I know we talked about the latent space and latent variable as well but I will go through this concept once again because this is a very very important concept doesn't matter whether it is NLP related generative AI or whether it is computer vision related gen AI both cases these are this is a very very important concept so I'll I'll revise this concept once again for everybody's interest but with that I'll quickly pause on the recap and move on to this concept but any questions before we move on to latent space and latent variable we're all good so shall we quickly jump into the concept of latent space latent variable all right very good okay so the good example to remember about a latent space of how humans actually create a latent space and how it actually works is to visualize the concept of let's say an artist for example right as I said in the previous session imagine a person imagine yourself okay as an artist who sees a beautiful scenery right of his and rivers and whatever basically now what you do is you capture that particular scenery in your mind right now that's not a photograph right so you're not having none of us have photographic memory right maybe one in a million might have that kind of a capability but in reality none of us have a photographic memory a photographic memory is simply put a control C control V of what you see basically that is called as photographic memory and that none of us can capability have because our brains are not that advanced to get copy paste of an image that we see so what we do is the image that we capture we store it in our latent space now our brain has only finite amount of memory right our brain cannot actually store entire images or entire you know inputs that you provide it cannot store all of it as it is so what it does is it converts it it transforms it into something called as a latent or a hidden memory now how the brain does that is what is an optimization our brain is capable of doing it's a it's a marvel right it's that's why we we always say the brain is commended for its ability to transform information into something and retain that information now what you do is imagine you captured that image and at a later point in time I ask you to draw what you saw what you're able to do is you're able to retrieve that information from your brain right no longer are you able to see that scenery in front of you so you can't visualize that scenery anymore you're probably back to your house in your apartment and now what you're doing is you're recreating what you actually saw you are recreating from a compressed input right the input that was provided it's compressed it is made into a latent value and that compression from that you're regenerating your image now is it going to be as good as the original the answer is no it is not and it is not expected to be also right it's like for example you don't want you wouldn't have appreciated Leonardo Vinci if for example the Mona Lisa was supposed to be a photograph of the real Mona Lisa right we wouldn't appreciate it we wouldn't really feel very great about it and that is exactly what the beauty of any generative computer vision model is also that you don't want it to be a copy paste of the original you want it to bring in variation and that variation is what the beauty of the entire entire model is or the the cable that's why we appreciate artists right we appreciate people who can beautifully draw people's faces etc all of those things it's because their ability to actually take an image and store it in a compressed space the same way the model does it but using what we call as a mathematical process and the easiest way of understanding this is the concept of PCA that we have seen earlier right so you feed in let's say hundreds of variables you can convert all those hundreds of variables into probably two or three principal components right that is one of the easiest way by which a model is actually transforming using mathematical concepts to transform a large amount of input into an encoded representation so do you see this this is a very important encoding each observed event in a compressed representation that is what is done by your model how does the model do that is where the concept of the model using what we call as the weight along with the activation function comes into play so the moment you see a tan H activation function generating an output between minus one to positive one what it is actually doing is that minus one to positive one is a value that the model knows about how that particular input is to be represented so if there is a representation of a particular pixel into a value minus 0.26 you and I will not be able to understand you and I will not be able to interpret what that actually stands for but the model will be why because the model knows what it has encoded it into it has converted it into its latent space that is what you can actually see so you give that input image or input text or input video or input anything this is where I wanted to understand that this is not just necessary images it is completely related to everything else in the world so you give text you give PDFs you give files you give folders you give whatever you want in this world as an input the model should be able to store it in the latent space and in its latent space it can actually visualize the entire it's kind of like creating a visualization of the entire inputs that you provided one of the easiest examples I've told you about this also embeddings right you pass in a large amount of text right and then what it does it creates a cosine similarity now that cosine similarity is about 10,000 tokens or 20,000 tokens you cannot visualize a matrix of 10,000 by 10,000 or 20,000 by 20,000 so what does it do it brings it down in terms of dimensionality transforms it into a latent space using a dimensionality reduction technique like T-SNE or PCA or whatever and hence we are able to reflect on that in a two dimensional space right haven't we all seen that that is what all the neural networks are actually doing they are transforming all the inputs you provide into a latent space and then when you require it they are able to actually regenerate from the latent space the input provided to create a new image or a new output same way like how it is using the same tokens that are embedded to regenerate the text in the case of a chatbot basically is that clear to everyone are we able to follow yes with me all right so that's where it you know one of the most important in fact if you have some time I would actually recommend all of you to read about this concept called as exploratory factor analysis it's there in SK learn a lot of them actually I think I gave an example in the previous session of how do you actually find out let's say for example a hidden factor so here for example you want to evaluate the technical ability of a student then what you look at is basically a certain key elements right let's say for example I look for I can actually observe this I can actually see this number of lines of code written I can actually measure I can go and see in each and every one of your scores like how many codes have you written I can do that I can see student A has written 20 lines of code 30 lines of code Kaggle scores like for example student A has got a Kaggle score of 20 or 30 or rank 2 or rank 4 or whatever it is like why is hacker rank or GPA etc these are observed a combination of these observations actually tell me if this is really high all the four are really high then maybe he's very good in technical ability so this is what is called as a hidden variable you have transformed all of this into one single index value or one singular you know representation that consumes all the four information I'll give you another beautiful example right you you have something called as I'm I'm sure many of you have heard of index index or indices one of the best examples of index or indices if you are a person who follows stock markets very regularly is a sensex for example or a nifty for example isn't it now when you look at a sensex or a nifty the sensex actually tracks or the nifty actually tracks I'm not sure entirely but I think it tracks about the 50 blue chip companies now all the blue chip companies are observed you can actually go and see you know how the share prices of these companies are increasing decreasing etc but what does the index do the index is transforming it is actually encoding all the information from those 50 blue chip companies into one thing called as a sensex and that sensex gives you an idea of whether it is increasing or decreasing a late variable is kind of like that right it's a compression of all the inputs that you actually provide right so that's where they are the same concept that it actually refers to in terms of converting it into a lower dimensional space and compressing it basically right it is able to create a compression in this case right any questions are we all clear till now so one of the first things what they actually did or explored is something called as a variational autoencoder okay so they said okay now that we have a fair of understanding of latent representation what we will do is we will bring in a certain number of layers and what we will do is those layers their job is to not actually provide you know transformation of inputs to do a classification or to do a regression they said we are not intending over here to create a model that at the end of the day has to classify whether it's a cat or a dog or whether this is a handwritten digit two or whether it's a handwritten digit three etc that's not the intent what they did was very simply used a set of layers right a dense layer or whatever layers whatever layers they got they use those layers but the only difference is instead of using the final layer right you have a dense layer at the end which is for classification what they did was they just let the model be as it is very simple so they actually created something called as an encoder and a decoder layer but let's not worry about the decoder layer for now all they did was they took the input image handwritten digits which we have already seen but the intent was that the encoder should not actually go through the entire input image and try to classify it remember the classification happens at the last layer the last dense layer that you come across that is where the classification happens all they did was remove that layer they said no we don't need that dense layer so what would end up happening the last layer before the dense layer it actually encodes the entire input into what shape into a shape that you specify so it could be a dense layer with 64 neurons I don't know 70 neurons or 128 neurons whatever layer you have right so all it does is you have a set of dense layers that take it and compress it into a shape that you specify which is 128 256 512 whatever right so the image could be let's say 28 by 28 which is a total of 784 pixels but it will compress it down to let's say 128 vector basically right that is what it will actually do the model stops that that's it nothing else it does basically but then what they do is they bring in another set of layers called as a decoder and that decoder takes the output now remember the output from the encoder is a compressed information right it's 128 or 256 or whatever it is that compressed information it actually takes it and passes it through its own dense layers but the problem here is that it doesn't actually look to reduce the dimensions it actually tries to increase the dimensions this is where it's interesting right so what it will try to do here it comes it does what we call as a down sampling process right it will do a a down sample very simple it is just taking this 28 by 28 and through let's say a convolution or whatever layers it uses it converts it into a single dimension right so this is basically let's say you have 784 input dimensions it converts it into a very similar very simple representation of let's say 256 and then what it does is it takes this vector and passes through a decoder which is what you're seeing here now what the decoder does is it does the reverse process of an encoder which is it starts to up sample now why how it does up sample that mathematics I'll explain later there is something called as an up sampling layer just like a convolutional 2d is a down sample there is a set of layers called as up sample which will actually re run the entire compressed output and then regenerate this output here right that is what is called as an auto encoder it's a very simple process that it actually follows but in the process what it does is it compresses the original input basically are we able to follow it so here you can see so this is what it actually does it's called as a bottleneck so all it does is it takes the input features passes it through a set of layers and those layers could be anything it could be convolutional or dense layers and what it will do is only thing is in the final layer it will not have a dense for classification it will not have a sigmoid or a softmax activation function at all the last layer is eliminated the last the second last layer the last before the last layer what it will do is it will just store the inputs as a compressed representation and array or a you can call it as a vector or whatever you want to call it it will store that in a shape that you want it to be and then what it will do is take that same information and pass it through another set of layers called as decoder layers and those decoder layers will have what we call as a up sampling not a down sampling up sampling so as you can see here here the input becomes small but here the smaller input becomes large and that is what we call as a reconstructed output right and that reconstructed is the input that you provided except that now it is actually regenerated or it is a lower dimensionality or a lower you can say pixel values with which it has been created so it will retain all the original characteristics as you can see here but the only difference is that it will convert it or it will transform it into a shape that is you can see here right you can visualize this this is you know the original image versus the reconstructed image the reconstructed image has a lower quality in fact if you see some of the quality of the images it will be like this they will be very very low quality it's it's just visualization it's like a poor artist right who tried to recreate something but got nowhere basically that's what it actually does in this case so autoencoders was the first model that was actually created which was generative AI which actually was way back not even you know now do it nowadays we talk about genie but these genie models were there 10 15 years back itself they were already proposed really long back right and people were already working on autoencoders but the only problem was that these autoencoders recreated images in practical which were not at all resembling the original image we would have hoped for something like this or something like this but it actually created something like this where not close at all it it has no relevant relevance to the original image itself so that's why autoencoders did not work really well in fact they kind of spoiled the image itself in that case are we all clear with the concept of autoencoders any questions on that before we move on to the next concept very quickly with me till now everybody ha Blu given me one month Just a minute. Auto encoders we need to define it Russian. So we need to create functional models for them. Where we need to create two models. One is an encoder and the other is a decoder. And from that encoder decoder we actually created. In fact, if you remember a sequence to sequence models, everyone remembers that right? There we also have the latent space. Everyone remembers what's the latent space there in our sequence to sequence. Let's see if everyone remembers an LP2 session. What is it that we have in terms of latent variable in our sequence to sequence model? It's not group of words. There are two specific elements that are there. HTCT everybody remembers that concept. Cell state and long term memory. Yes. That's actually a latent variable isn't it? Isn't wasn't the sequence to sequence model? The encoder actually representing it into a lower dimension. And then that lower dimension information it passed it onto a decoder that is to convert it. Everyone remembers in the case of transformers, what do you have? Which converts it into a latent space? Transformers? Anybody recalls? See people have forgotten their transformers session. Attention. Do you remember the concept of attention? That encoder passes on to the decoder? Yes. People have forgotten, right? Everything is forgotten. So there also is the same concept, isn't it? You've got give a long term memory. You've got to get the same memory. You've got to get the same memory. You've got to get the same memory. You've got to get the same memory. The same concept isn't it? You've got give a large sequence of input. What does it do? The attention is actually encoding it. It is converting it into a score, which you and I probably will not be able to interpret it. But the model is able to understand the, the its compressed representation. It is able to understand and store it such that when you decode it, you ask a question. It can give a long, longer answer when you give a, let's say for example, an input it is able to translate, etc. All of that comes in why? Because the model has the capability of taking in the entire input and generating an output. Yes or no? Everybody remembers that? I think people have forgotten some of our earlier sessions. But anyways, so what they did was they brought in something called as a variation not to encode us because the auto encode us concept itself was not really very successful. You can specify that how much dimensions you want to. That is something you can specify it. See like it's the second last layer before the dense layer for classification. You can keep it as 512 128 256, but definitely not equal into the original input shape. Right. So if you have let's say 28 by 28 pixels, which is 784, you will not create a dense representation or you will not create a compressed representation of 784. It will probably be 128 or 256 or 512 at the max, depending on what kind of image you are dealing with. So if it is let's say 2000 by 2000 pixel image, that probably leads to about 4 lakh or something odd, you know pixels, then you will not represent it in a 4 lakh representation. You will probably represent it in a 105 to or 512 or something like that. Right. In a lower representation is what you actually do. So basically you have model dot add. You keep adding dense layer 1, dense layer 2, dense layer 3, and then when you are about to do your dense layer 4, which is your classification layer, you stop at that dense layer for 3 itself. You don't add the dense layer 4 for classification. Primarily you don't want that sigmoid layer or softmax layer, which is supposed to do the classification. That is excluded from this entire process basically. Yeah. So what they did was they brought in something called as a variational auto encoders. Right. So variation auto encoders was a little bringing in variation as the word itself says. It's an auto encoder, but adding purposely variation. Now, how does that variation get added? It is called as noise and I'll explain to you what noise is at a later point. I think I mentioned it in our last session also what noises. But they add intentionally a little bit of noise. Any noise will bring in variation. Okay, noise is primarily as I said, I think for those who missed that part of the session. Noise is basically this right noise in TV. Old TVs right if you see this is what we call as noise in the old TV days right. So this is an example of a noise noise is just random pixel values. N P dot random dot random number exact exactly that nowadays. We don't have this so instead of noise what we have is we've started getting blue screens. Right so because it's like not very great to see this kind of a screen. So what they do is purposely what happens here is they start to add noise to the entire thing. So if you notice over here, it's the same encoder decoder method over here. So it takes the input compresses the representation. But what it does is from the sample it will also add as you can see here variance sigma and sigma two. So apart from representing it in a noise. Sorry representing it in a compressed latent space. That same information from the compressed latent space gets added with some amount of variance right a variance. What is that variance nothing but from the input that you get. I'm sure you'll be able to calculate the mean and standard deviation. Now to the mean average values just add a little bit of the standard deviation and you're sorted. That's about it. So that additional variance that you're actually bringing in. You can see here right the only part being the latent space area where you're actually changing and adding the variance in this particular case. So by adding the variance, you can actually bring in some sort of a distinguishing factor or a variation in this particular case like for example. Smile it has a particular distribution, let's say for example, right in this particular image. This file has a distribution which is towards the lower side in the case of let's say Mona Lisa picture also there is a smile, but. The smile is probably a little different representation likewise here. It's a different representation likewise here. There's a different representation now all the images have different distribution values and what it'll actually try to do is. Combine all of this right a bit of all of these miles and everything all the variations. It tries to add in some amount of standard deviation to the distribution and then what it'll do is create a new image altogether. So simply put what is exactly the variational auto encoder is it's an encoder with additional voice being added to the. Representation or the latent values additional voice noise is added for it to be able to actually regenerate the entire image that has been provided in this case. Right so that is what we call as a variational auto encoder so it can actually regenerate an images, but primarily what it is also able to do is. Add some amount of noise or voice noise in this case or in what voice add some amount of noise and bring in some sort of a variation in the image and that's why you will see that it can actually have a person smiling in a different way right so his original image might be like he's probably not smiling too wide now what happens is when you pass it through. These auto encoders they are able to now create a smile that looks a little bit better maybe his teeth is much more visible or something like that you know it combines basically brings in let's say Mona Lisa smile to a girl or something like that it can do those kind of things. The value add is a small minor changes it should not make a big change also right run genie so if it makes a big change in the image it will look off entirely over here so in this particular case you can see somewhat similar it's very fine if you look at it like in terms of the combination of all these smiles that are actually brought together and small element changes that it has actually bought in. It's again not a very great model but see you have to look at it from an evolutionally perspective right so from an encoders which were creating total rubbish right from auto encoders which were bringing in total rubbish outputs which you saw earlier like the handwritten digit had no resemblance to the input image this started to create some sort of resemblance but very minor variations that it actually bought in very high variations if you actually bring in then what will happen is it will completely break down the entire image in this case. You can't really choose that that is where there is a bit of a challenge with these models you can't choose how much you can bring in variations it's a random variation that the model by itself actually brings in the engine. So sometimes it might not be even visible to us like we might say like you just said right it looks like it's the same image there's no change actually so that's what it will be. 0.23 is not the coordinates it's the transformed values here Amruthu it's a it's what we call as the late int values right so it's a transformation so that's why we don't know what the 0.23 actually stands for or minus 0.18 stands for or 0.71 stands for it is a models transformation that it has converted it into these values but these values it can always take these values and regenerate the image that's the idea behind you. So they are not coordinates they are the pixel values itself that you're looking at right are we clear everyone yes no. So this is where the traditional auto encoders would generally create an output that doesn't actually make sense or there is no there's no nothing great about the original ones but of course variation auto encoders kind of give you with some amount of standard deviation so it becomes a little better basically. Ransini we spend considerable amount of time on latent value in the starting of the session so I would request to go through the recording once again my apologies but we have limited time which we need to reuse for lot of other concepts right. I hope that's alright i'm sure I'll ask the team to upload so first I think 10 15 minutes we spent only discussing latent values not sure if you were there for that part but I would request you to probably go through that. Yeah. Okay so is this clear to everybody just adding a little bit of standard deviation adds to some amount of variations to the latent value so the latent values what you have represented or encoded it into adding amount of standard deviation makes it a little variation from the input that you have. To get the final input for the decoder do we need to run encoder every time or we can get one output and modify values a bit okay very good question so generally what we tend to do Russian in that cases we run the decoder we run the encoder only once right so let's say we have 100 images we run the encoder once and take all of the images and represented into a lower dimension and then recreate it from the decoder so it's like first you run the encoder. And then you basically run the decoder yes you freeze the encoder and then you read on the decoder basically that's how it works. Is my voice low or is it fine i'm actually keeping the normal noise voice only but not sure okay. So initially what they did was they used it a lot for anomaly detection data denoising basically so what is data denoising is. You know it's a very interesting concept so earlier you had images like this okay corrupted photographs for example right so i'm sure many of you would have seen this right these are like corrupted images can you see the. It's like it used to happen in digital cameras also it's like corrupted two or three images have been corrupted here also it's like kind of corrupted images that are there so what denoising does is it removes that right corruption is nothing but. Remember that you know the pixels are completely changing from here to here and that is kind of inconsistent right so it can actually remove those kind of noise from there make it a bit better basically so. This was originally used for removing noise from the images and all of those things and very little image to image translations also so which means they used to convert images into translated images etc right. Kind of like a blurred image yes they used to actually do that as well. So they did that but it's not very successful unfortunately they were the original method that they actually tried to apply for. Yes we'll come to that Russian I'll come to that point so we're just looking at few of the original methods of you know generative AI that was there so of course the entire method of auto encoders variation auto encoders not very successful but why they are talked about very frequently is because they were the first steps right step number one for the entire world of. So it's like for example today we rarely use LSTM models or we rarely use an RNN but you have to remember that those LSTM and RNNs are what we see today as transformers right large language models so we appreciate the. The past or the evolution from which it has come from. Yes encoder is used only once which means it will actually encode all of the information into representation and then yes the so normal encoder or the normal auto encoder does not use new and sigma as such even only the variation auto encoder uses new and sigma added to the to the content to the entire latent representation that you have right it adds a little bit of mean and standard deviation so the. So that it creates a variation in this particular case and that is done by the decoder. The sorry yeah good point so the encoder itself adds this new and sigma to the compressed representation not the decoder the decoder just takes that input and then it recreates it so to your question here you can see when it is actually compressing it over here right at that point itself it adds this mean and standard deviation here. As you can see right hope that answers. So to your question the decoder doesn't add it's the encoder that adds while it generates the latent representation right are we clear everyone yes correct it has to every. Time and image is generated it is different and that is the intent behind it right it should be different otherwise it is just a copy paste of the original image and we want it to be a little different and that difference could be. Good bad we don't know but then that's where you need to train it enough amount of times to actually get a value added outcome basically. So it's like a batch basically right so what it will do him and this you give all the images in one short right let's say 100 images it will compress it into a lesser dimension right 100 by let's say 528 is what it will represent it into so that 100 by 528 you feed it into the decoder so it's like a it's not real time so it's not like. The encoder generates output and then immediately you feed it to the decoder you compress it store it as an array and then take that array and then pass it as an input into the decoder basically so it's not working in parallel you are waiting for the encoder to finish working and then you ask the decoder to take the input so it's like a kind of like a factory shop floor basically that's how it works. I'm not sure if that was a question but I just let me know if that is the case if that question got answered. You can do that but then the problem is Russian that the auto encoders don't have that kind of a flexibility right they were earlier models where you can't actually introduce noise or you can't bring in changes in sigma randomly like that so they were not they were not built for that so that randomness that came in was completely through just the encoder models itself so their idea was that they were built for just for the same thing. So that's bringing in some amount of variation that's the idea behind it for same prompt these images are generating so there is no prompt over here at all which is there is no prompting concept at all you pass an input as an image encoded and then decode it that's all there is no prompting concept at all in this particular case so they don't have they don't understand text they only understand images as an input. So there is no so human there is no pre training concept here at all see pre training is when you have a wide variable attached to your model right so you have to remember that the idea here when I'm training this model is not actually to take an image and you know give this as an X and this as an Y so when it learns let's say for example this relationship that relationship is what the weights and biases are actually. So that's the story that is called as a training where the model is learning all of these elements and what you're doing is next time you're actually passing the X to the W and B they're actually generating the Y which is what pre training is actually doing here there is no pre training all you're doing is there is no Y at all in the first place you're taking the X and just representing it into a lower dimension this is what the encoding process is all about and then what you're doing is you take the. So this is what is the representation and use something called as an up sampling which is what I will explain later what the up sampling concept is it just uses the up sampling process where if this was let's say convolutional 2d this is a reverse convolutional 2d process which will regenerate the entire image which is again if it is 28 by 28 you represented it into let's say 5 by 5 it uses the 5 by 5 and regenerates again 28 by 28 but the only difference is this 5 by 5 which it has it has it has the 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0. So in the case of normal auto encoder it doesn't add any mu or sigma in the case of variation auto encoder it adds a mu plus it adds a sigma or a standard deviation in this case. Exactly creating it into a latent space a compressed representation that is what it is actually doing. No, dally mid-journey are very different there I'll come to that a little later these are just very simple auto encoder models that's all nothing complicated they can't accept text all they can accept is images as an input compressed representation representation of the images and recreating the image from the compressed representation that's all they actually do. That clear I hope that answers now him. So don't confuse with dally for now so the prompting concepts I'll come to a little later as of now the models that we are talking about they cannot accept text as an input so that's where we'll come to that later and the concept is text to image generation so I'll come to that point a little later for now these are all models that can only accept text as an sorry image as an input compress the image and recreate the compressed image that's all they can actually do. Okay, shall we move on everyone a little quickly please can we do a little faster yeah because we would like to cover a lot more but remember I am also constrained by time I would like to use our time to focus on multiple other things as well. All right, so now comes the most important change that actually came into play so they got in something they said all these encoders auto encoders okay they can generate images which is great which is good. But then the problem is that the generated images don't really aren't really very great you also observed it right many of you said that okay it's not like you know I don't see much difference out of the generated output and that is exactly what people felt also like after all this encoder and decoder and all of these things that you've actually done what we are seeing is there is no big change in the image that is regenerated and that regenerated image has nothing of value for us to be able to do. So, make a conclusion and say that this is this is something new right there was no surprise element people didn't find it like oh amazing like how all of us were surprised when we saw chat GPT for the first time we all imagine that there was a human on the other side talking to us right in the same way that surprise element never came until the concept of generative adverse real networks so what they did was use the same encoder decoder concept but in a little different way and created what we call as today. So, we have a set of models which we call as deep fake in general normal language they called it as generative adversarial network. Now, before I get to the concept of generative adversarial network I'll explain a very simple concept because this is an important element which I want you all to focus on because we will be learning about or we will be doing a real time implementation of a deep fake right we will do a generative adversarial network in our example. So, let's understand this concept really really well right how it actually works what is exactly the process etc. Now, before I get to the generative adversarial network till now what we have seen is we have built models that have generative capabilities which means in text or in images they generate outcomes or outputs is generated and these outputs focus on only one objective right they focus on only one object. Anybody recalls what is the objective that they all try to focus on any model that you've built till now what is the objective that we are trying to fulfill. Come on this is the fundamental thing that we have all learned what every model is trying to minimize. So, you want to minimize the errors right minimize the loss or errors basically the difference between the actual and predicted everybody agrees with that. So, the objective is simple you want to be able to minimize the error. So, what they did was they said ok if all the models are only unidirectional which means they all focus on minimizing the errors then what will happen is the model is trained in a way that it will only look for trying to copy the original image yes or no think about it right if I feed an image of a cat and a dog right whether it is to recreate the image or whether it is to classify the image doesn't matter. I don't care but when it is encoding it or when it is converting it into a compressed representation the only thing that it focuses on when it is compressing is that it should be able to minimize the error at a later point in time right which means it should be able to recreate it as close to as the original or it should be able to predict it same as the the why has to be the same as the why had basically that is the objective that it actually tries to solve. Yes perfect model right it needs to be a perfect model where it does there is no difference from the original image you all agree with that isn't that the models objective all the time. Are you following me this is a very important thought process by the way so I want you to think through this process basically every model whatever sequential layers or functional layers that you have built till now the models thought process is very simple see just go back to your objective function right what are you actually doing. You are trying to minimize the difference between the actual and predicted yes or no everywhere we are trying to difference the reduce the error as much as we can now the moment you try to reduce the error what happens is the model has a unidirectional goal right it's only it is obsessed with trying to make sure that the input is the output is exactly equal into the actual output so they are always focusing on trying to make sure that they match to the why the why had has to match the model with the original image of the model. So the compression also is actually trying to match the original input image so in this process because they are always trying to match with the original input they end up creating or recreating outputs that actually don't have any added value so that is why to the question that run any asked I don't see any difference here the problem is because the encoder is trying to focus on only trying to minimize the error as much as it can and in the process of the process of the process is not going to be a good thing. And in the process of minimizing the error it is probably recreating images that you and I cannot actually differentiate we are saying this is the same image I passed what the model actually do in terms of the value so it becomes like photographs right photographic memory so what is it that you as an artist have brought in terms of variation basically so that is where they said we can't have models have the same process where you can't have a model do the same objective so they said okay we'll bring in two models okay one is called as a generator the other is called as a discriminator and I'll explain. So what is the difference in these models but both these models have a different objective function very very interesting okay so what they did was they said okay we'll bring in a concept of what we call as game theory now game theory is to give you a bit of background to what game theory is game theory is a is a subject within economics which focuses on strategies basically right so what kind of a strategy should be adopted by anything right so when we say that we are going to do this is a very important thing so when we say strategy a strategy could be in anything it could be in a organizational strategy it could be a chess game a strategy by a chess player or it could be a strategy in a cricket whatever it is but the point is that it teaches you how to basically make decisions such that your strategy wins so game theory typically looks at what we call as a zero sum game so a zero sum game is not a win win okay doesn't mean that let's say for example if you're playing chess a chess is basically a zero sum game you can draw which can be considered as a win win but generally in a chess you should have a zero sum which means one person loses and the other person wins that is how the idea is so here we wanted to build a model that follows the same strategy of zero sum game which means we will have two models with competing objectives one trying to minimize the loss and the other trying to maximize the loss and then what we will try to do is make one of them win and when one of them wins then we have achieved what we call as a recreated image which is actually fake and that is what is the objective of a generative adversarial network don't worry about all the concepts I'll explain to you how it works but fundamentally remember that it has two models no encoder decoder anymore it has one which is called as a generator model and there is another model which is called as a discriminator model and what you will do is the generator will just take some latent values which is nothing but the latent representations or it will just take random values right N p dot random function we have it will just take some random numbers which is noise and what it will do is it will take the noise through and it will generate a image out of that noise okay which could be let's say a shape of 28 by 28 right that is what I have actually generated as an output now how do I do that it is very simple there is something called as an up sample layer which I'll explain to you later that up sampling will recreate this 28 by 28 pixel now how does this 28 by 28 pixel actually look like this is exactly how the 28 by 28 pixel looks like right yeah irrespective so it will look like this only that TV noise I just showed you right now it will look like this basically static noise this is how it will actually generate a image okay initially then what will happen is it will give you on the other side if you notice here it will give you the real images as well so it will actually start to you can see real data samples that will also be generated and that could be let's say the handwritten digits basically right so you can see something like this data will be actually passed also now you tell me if I pass that noise and if I pass that handwritten digits to this discriminator will the discriminator be able to discriminate between the real images which is being passed which is the handwritten digits and the noise which I passed will it be able to differentiate between both of them it will be able to everybody agrees with that so what will happen is that the discriminator will easily understand that you have given handwritten digits here and you have given fake inputs over here and it easily discriminates it says this is the real handwritten digit and this is the fake now what it will do is this is where it gets interesting right so this is where if I don't know if you've seen the Amazon example right fuzzy I'll come to that point a little later it is something similar to that so what the discriminator do it will try to maximize the difference it will say that okay there is a big difference between the fake images and there is a big difference between the random noise that has been generated and it will tell you the difference also it will subtract it didn't already let's say the pixel values here and subtract the pixel values here and it will tell you the difference. Now initially the variation will be very high right. So if you see here as an example, if you subtract the pixel values of this versus subtract the pixel values of this, the values will be a big difference. There will be no relationship at all, they'll be totally different basically. So what will end up happening is that that output, that difference, the loss function is fed back to the generator again. So it is actually telling the generator that this is the difference between your between the real image that I've been fed with and this is the difference that I'm getting from your random noise or the you know the images that don't have any values, you're passing that. When I am able to see there is a difference between both the pixel values, the generator actually is fed with that loss function. Now what the generator does is it trains its weights and biases. It trains its weights and biases on the loss function that the discriminator provides. Okay and when it learns on that loss function, what it does is it updates its weights and biases such that it tries to match the or reduce the loss function of this discriminator. So while the discriminator is maximizing the loss function, the generator is trying to minimize the loss function. So this is where I told you remember they are adversarial, they're not working in the same direction. One is trying to increase the difference between the fake and real images and giving the feedback to the generator and the generator is trying to reduce the difference between the fake and real images and recreate images or videos that are representing or getting closer to the original image. Now through multiple trainings or through multiple let's say iterations what will happen is eventually the weights and biases get so perfect that based on the loss that it has been trained with, they recreate this noise gets converted into a real image such that the image between the fake the real image and the fake image cannot be differentiated by the discriminator. So the discriminator does not know the difference anymore and what ends up happening is it starts to actually treat this noise which is random values that are being generated by the generator as real it gets cheated and it considers it as real and that is exactly where the generator model has become so good that it has fooled the discriminator being able to create a fake image in this case. So that is why I gave you the example of of Furzee. I'm sure many of you would have seen that. So initially of course what happens I mean the artist basically recreates a fake currency but he gets caught. He gets caught in that case because of course it's like so naive that it looks so basic that immediately what happens is that people or wherever I think he's in a bar or somewhere where he's trying to buy fake, where he's trying to use the fake currency to buy booze and immediately they get caught catching basically but what does he do? He learns basically all the features isn't it? He goes back and he sees exactly how each and every you know curvature is how each and every feature is and then he recreates it to a point where after that when he recreates it so perfectly that people can't actually differentiate between the fake currency and the real currency right? That is exactly what a generator it was real does. It does initially of course generate noise and it is also fed with the real images. Initially the discriminator does a very good job. It can differentiate between the real and fake but over a period of time the loss the difference between the real and fake is fed to the generator and the generator actually learns from that loss keeps updating its weights and bias until a point where the same noise becomes a generated image and that generated image becomes so close to the real image that the discriminator cannot differentiate between the two and that is the point at which we have what we call as a deep fake or fake image which looks or resembles the original image but is no longer a real image it's just a fake generated image. Both inputs should be in same dimensions yes both inputs should be in the same dimensions. Because otherwise what will happen is if the dimensions are different if you get 28 by 28 to the real and let's say 14 by 14 to random then very easily it will differentiate. See you don't want to make it so simple for the discriminator also you want to make it a difficult job for discriminator as well. Yes so the discriminator fine per say if you see is actually a simple classifier discriminator is just a classifier all its job is to only discriminate and say whether this is a fake or real image. So this is a simple CNN but the generator is where you have something similar to an auto encoder in an auto encoder like the decoder layer you have where it will actually do an upsampling process from the noise and recreate an image where the image resembles the original image and you pass both of them to the discriminator and it will tell you whether it's a fake or real image until a point where it becomes it can't distinguish between both of them. One tries to maximize the loss the other tries to minimize the loss so this one will try to maximize the loss because when is a discriminator successful it is successful when it can keep discriminating between or if it can detect the fake and real if it can say that this is fake and if this is real it's a success right it's a classification. When is a generator successful when it is able to actually generate an output such that the discriminator cannot differentiate between a fake and a real that is the success of a generator basically. So that is where what you're doing here is you're trying to make sure both of them work in parallel with two competing objectives and those competing objectives initially of course favor the discriminator but as it goes through multiple epochs the generator starts to win and that is where we want the generator to win because then we create fake images that resemble the original images in this in this case basically. If you reduce the error too much is the model may become overfitted see errors over here is a different concept right mehika because see error from what we have learned is the difference between actual and predicted isn't it. Now here there is no y label at all all of these that you're seeing are actually unsupervised kind of methods where we don't have a y label so you'll not see that there'll be a y that'll be there except in the case of discriminator where the y will be real and fake. So the point here is that yes this tries to of course reduce the loss as much as it can it can become overfitted can be possible not a problem but as long as we see the generator there is no concept of overfitting at all because it doesn't have any y variable as such it is not being treated to any it doesn't have access to these images it doesn't know what these images looks like all it is doing is it is trying to recreate images as close as possible to the original images that is what it is actually trying to do. Exactly every cycle will have a new image with some difference basically. Very good point right so what Russian says what if noise is replaced with different images any random image of a person for this example. Now what will happen in that case is then see a very interesting point right so in that case what will happen is the discriminator let's say for example I put you know this is some actress right and let's say instead of the noise I put let's say for example the image of another actress over here. Now imagine the all that the generator will do is just try to create some sort of image again some sort of variation it will try to bring in but then the discriminator will not be able to discriminate between the real and the fake image basically. So what will end up happening is that there will be no difference between the fake and real image the discriminator will lose in the first shot itself and then there will be nothing to train the generator on basically right so the generator will never learn in that case so you want the generator to learn right you want it to figure out what is the real face basically right and if you feed the real face in the first shot itself then there is nothing to learn for it in that case so that's why we don't try to give a real image in the first place itself hope that answers Troshin yeah it will be different no doubt but not as different as random noise to real image right it will not be that much of a difference it will be somewhat similar basically so that is what you want to avoid you don't want that you want them to go through multiple iterations otherwise they will probably learn very less and you know it will not be enough for it to create in that case when do we stop this process we stop this process until we recreate an image that the discriminator thinks is real that is where you know the loss basically is zero but now you notice that you know this is not a real image this is a fake image basically that is where you stop generally we give epochs so we specify the number of epochs the more epochs you specify the better the generator will be at creating a good image basically generator is an encoder in this case it's actually an encoder and decoder both because what it is doing is it is taking the compressed information and passing it through a set of layers to kind of decode so we don't call it as an encoder decoder here him and primarily because of the fact that there is no decoder layer or encoder layers separately so that's why we pass them as just we just call them as a generator model per se it's a combination of both in a way that's how we have to look at it as are we all clear yes following shall we dive a little deeper into each of these models yes both are adversarial to each other with you so one is trying to see remember it's like you know cop and let's say a fake currency person right like a cop will obviously try to you know try to catch the person who generates the fake currency isn't it and the person who basically generates the fake currency has to cheat the cop right he has to make currencies that are so good that you know there is no way that the cop can discriminate between let's say fake currency and a real currency that's the intent so they have two different you know contradictory objectives that they are trying to actually fulfill but the point is that until this model all the models had you know same objective function because the objective function is the same the encoder also as the same objective function the decoder is also the same objective function they end up generating the same output and then there is nothing really worth that is there for us to you know look at it and say oh this is amazing kind of thing right so it was there was no surprise element or we call it as an entity right or I don't know I don't know the exact pronunciation but there is no surprise you feel that it's okay it's the same image what did you do anything great in this case but when you bring in a competing element you see that you are now making one model you know win eventually but that winning formula is what helps you to generate a value that is really really good basically yes kind of somewhat similar to that right like an empire basically where the batchman has to hit the ball out of the park and the bowler has to make sure that the batchman doesn't hit the ball out of the park kind of like that two competing players basically right are we clear everyone yes following me not following I see the energy is going a little low we'll get to the hands-on use case very quickly in some time but let me show you something over here right so you can see the generator model what it actually does is very simple just follow this process and you'll be clear it takes a fixed length random vector random noise z as an input what is the random noise NP dot random that's it okay it is nothing else so don't get confused that okay latent space means it does some encoding and all nothing we call it as a latent space because it's an input that we are providing so it is just random noise that we are passing it is just NP dot random that we are passing and what it does is it has a set of layers okay so there is something called as in I'll show it to you very quickly actually there is something called as an upsampling layer okay it is the exact opposite of it is the exact opposite of a convolutional 2d okay so you can see upsampling 2d it's the exact opposite right it is not the same as that so it will use and reverse process so can you see one example here I hope everyone can see this if you notice here you have an input shape over here right so you can see 2 comma 2 comma 1 comma 3 so this is basically an array you can see so typically think of it as 4 comma 3 right something like that you have so you can see here you have how many 12 values 1 2 3 4 5 6 7 8 9 10 11 12 now when this input x is passed through an upsampling 2d can you notice how many it becomes 1 2 3 4 5 6 7 8 8 into 3 24 so can you see from 12 input values it has regenerated 24 isn't it so in the same way if you had to imagine this this is nothing but a 2 comma 2 comma 3 right this is the representation so it has recreated some new values based on the input values how did it recreat it follows the reverse process of a convolutional 2d where instead of the filters actually generating random filter values the filters actually try to recreate random values in this case that's what they do in terms of the process over here so that's where it generates a recreates a new set of values in this case so in the generator I'll show you when we do the hands-on example as well the generator actually takes random noise np.random.random noise it takes that as an input passes it through in fact here it was just random noise if you look at it here this is you can see np.pro they have given but it's basically you can give np. here they've given actually range of numbers 0 1 2 3 4 5 6 7 8 9 10 11 and that it has generated is you can see new values 0 1 2 3 4 5 6 7 8 9 10 11 only differences instead of a range I give it as np.random that's all as simple as that nothing no change as such and what it actually does is it's not duplicated it's not it's recreated so duplicated would have been a you know it would have been a broadcasting right it's not broadcasting those arrays at all it's not doing a broadcasting in this case it looks like it is duplicated because it's a range of values that we are giving courage so it is creating a range again and again right so that's the pattern it's a range of 1 2 3 4 5 6 so it's that's called as a broadcasting basically so it's different concept altogether this is not doing that at all it's not duplicating it it is only creating new values in this case right I'll come to that a little later Mukti Kanta I'll just explain this concept first just understand the math behind this then you will you know be able to understand the other concepts as well so very simple is basically you can just put np.random right we can use this and np.random is equal to let's say some we can give some values basically right so let's say I'll call it as 28 by 28 pixels and I can generate let's say 100 samples like this okay and I'll store them as x which is what you see as latent space so what will it actually be every image right so it'll end up being 28 by 28 pixel images of 100 images you can convert it into let's say two dimension also so you can you can also represent it this way let's put it this way actually just give me one second so I will call it as x is equal to np.random and what will I give 784, 100 so what will it do it will generate 100 like this instances and it will give me some values for 784 now random means it will give some random values right it will give some 0 15 25 25 35 something something something it's not 35 sorry until 255 so 150 something like that it will generate values what it will do is it will take this entire x that you see here and pass it on to the generator let's not call it generator let's call it as an upsampling 2d now what the upsampling 2d will do is it will take these values and it will pass it through some weights and bias or it will do a dot product between a weights and bias and it will convert it into a 28 by 28 pixel which is reshaping the entire image now when it reshapes it what it will do is when these pixel values multiply with the weights and bias it will create some sort of encoding okay it will create some sort of an encoding what will that encoding be let's not worry too much about it right it's some sort of a transformation that it will convert all the pixel values into now this is my fake image I'll store them as my fake image now remember there are hundreds of fake images all of those fake images will be stored over here as something called as fake is this part clear to everybody yes random values convert them into 784 784 random values are generated pass them through an upsampling 2d where a weights and bias will get multiplied and you generate basically an image of the shape 28 by 28 with some random values and store them as fake in this particular case weights and biases are also random yes absolutely yes or no this part clear I think people really require a break anyways we'll break in some time but we'll have to just go through this concept so then only you will understand basically right now what I will do is remember the fake images that I stored okay everyone remembers that so let's assume that I have 28 by 28 pixels over here these are my fake images which I've got it from the previous previous process yes yes or no guys now to this I will have real images also okay so I will have let's say for example 28 by 28 pixels of real images so process one is done already okay so that job is over so now I have 28 by 28 pixels of real images okay so this I'm going to call it as real all I'm going to do is I'm going to build a CNN very simple now the CNN will be fed with this batch over here okay of fake images and this batch over here of real images right what do you think will be the accuracy let's say in the first you know the first thing let's say I'll call this as one and I'll call this as zero what will be the accuracy of the model remember this is the initial iterations right so let's say this is a handwritten digit two and this is how the two looks like okay so what will be the accuracy of the model very high accuracy everybody agrees with that can you not see the difference here right see this two isn't it different does this look like a two no right this looks like a two yes so will it not have high accuracy right it will be easy to discriminate what will be the loss between this and this do you think the loss will be less or high loss will be high in this particular case right so you'll have a very high loss let's say some value of 250 is what the loss is now what I will do is I will take this loss 250 and then I'll go back to my previous model basically right so let me go back to my previous model now so what I will do is I will go back to the previous model which is what I have here and what I will do is I will tell the generator this is my generator model that I'm looking at here isn't it I'll tell the generator that whatever weights and bias you have over here you need to update it okay you need to update it why because that guy in the discriminator easily discriminated this is the loss you have 250 is what the loss we got this is very high which means the guy is actually able to easily discriminate between your fake image and real image so what the generator does is it takes this error let's say 250 and it will differentiate between all the weights and bias you would have specified weights and bias right so it will do a delta e by delta w over here and it will update the weights so what it will do is because of this loss function it knows how much there is a difference between the image that it created and the original image that was fed by the real image that we have fed so that is what is denominated by here and now it has to minimize this loss so what it will do is it will increase or decrease the weight such that in the next iteration you know it generates an image let's say for example which is again a fake image again a fake image and in the second iteration what it will do is it will pass that image back it will recreate with the new weights and bias new image and it will create again a 28 by 28 pixel image okay so it will create a 28 by 28 again a 100 image stack it will actually create it call itself as fake images and then again I will feed the same real images 28 by 28 pixels and let's say for example I will again get real images which I'll call it as real I'll pass it through the CNN now let's say this is how the two looks like now the two it looks like this somewhat like this now you tell me is the is the loss going to be 250 anymore is the loss going to be 250 probably not right loss will become lesser it will still have a difference maybe the loss is 200 now okay it will become 200 in this case till this part is it clear to everybody yes are we following it now if the loss has reduced to 200 now let's go back to our problem again let's go back here right so let's go to the back to the model that we saw earlier so I go back here and now I say earlier the model was loss of somewhere about 250 is what you saw is the loss now what has happened is that the loss is no longer 250 but rather the loss has become 200 right so it has become 200 so what are we signaling to the generator what is the signal that you send to the generator when the loss has reduced what are you providing as a signal to the generator now think guys come on it's not so easy to get Jenny like that you have to understand you have to figure out also certain things so what you are actually communicating the signal you've passed from this loss function is you are telling that there is a improvement there is an improvement that has happened so you're saying that you are getting closer to the original image now right so what yes exactly so you have adjusted the weights and bias or the generator has changed its weights and bias in such a way that now you are getting closer to the real value you are getting closer to the real pixel values so what it will try to do is constantly try to reduce this error as much as it can so there will be a point at which the error is going to be absolutely no isn't it the error is going to be zero in this particular case that is exactly the point at which the fake image is the same as the real image as much as we can think of so it will be what what we call as the beat fake image that the generator has done are we able to follow the entire process yes yes or no so keep doing it multiple iterations until a point at which it becomes really really good at so it is not going to happen in one two three iterations it is going to at least require hundreds of iterations to be able to do that in this case right I think there was a question Hey month right are in accuracy and loss function inversely proportional how can high accuracy also mean high loss okay so here's where the difference between loss comes in right of course the ideas right what you're saying is correct I mean the loss basically has to reduce when the accuracy is high but when we say loss over here we are talking about the difference between the fake image and the real image isn't the loss going to be high aren't we going to have a high difference in that particular case simple example right just look at this if you subtract very simply right the pixel values here versus the pixel values here aren't you going to have a difference here Hey month the question is to you Hey month is not there No I'm just asking a simple question right like let's assume that for example you subtract the pixel values here versus the pixel values here don't think of it from a discrimination perspective if you simply look at the pixel values here versus the pixel values here are they going to be different or not okay I have to make it simpler now so let's say this is let's say the fake image okay I'm just going to take an example of a 2x2 pixel image basically right so let's say the fake image has a pixel value here of 25 35 45 and 65 let's say this is a real image simple subtraction if I do actually of the real and fake let's say 125 135 145 and 165 so this is the difference basically will there not be a difference that will exist now Hey month are you following or not we are waiting for you basically right so there is a difference what is the difference let's say 100 pixel plus 100 pixel plus 100 pixel plus 100 now again I'm just I'm I'm simplifying the concept so don't take it literally also so the difference is basically 400 over here okay now what will happen is the discriminators loss I know I know I know the the way you are comparing it basically is the loss in terms of actual and predicted basically right but here that is where the difference is right you're not trying to actually there is no actual and predicted that you're trying to do here it is in a way yes it's a CNN it is doing an actual versus predicted the only difference is that this is the actual this is the predicted but when it is discriminating it there is so much difference between them the losses between the fake being called a fake and a real being called a real that is what you are considering as a loss but what I'm considering over here as a loss is not the difference in probabilities I'm saying the difference in the pixel values itself that is what I'm calling as a loss over here so if that is reduced and being successful and the point is this is what I'm going to actually feed to the model such that when I feed this to the model what will happen is in the next iteration what it will do is it will go back and it will say this should be updated so let's say this will become from 25 it will become 55 this will become 65 this will become 75 and this will become 85 and then now you will know that earlier model I don't know what the values were I forgot that but let's say it was 125 135 145 and 165 you are getting closer now aren't you now if you subtract and see the loss will not be 400 probably the loss will have come down to something around let's say 250 right you are getting closer in this case right the fake is getting very close to the real image at one point in time what will happen is the fake will actually have the same pixel values or similar pixel values close to the real image that is when it has recreated a fake image so close to the real image in this particular case yes following it no are we able to follow it basically I hope it's now clear yeah so that is where you can see one loss function is increasing can you see this can you notice this so the discriminator loss function is continuously reducing actual versus predicted it is getting it this thing right it's reducing it and then you can see the generator loss function it is increasing right so the point is there has to be a opposite kind of a game that they are both playing right each other is trying to one is trying to minimize the loss the other is trying to maximize the loss and in the entire process they're able to create a model that you know is able to generate an image which eventually is cheating the discriminator the discriminator is not able to discriminate between the real and fake image and that is exactly what is success for us so this is the loss function don't worry about it for now I'll come back to it a little later when we do the example a little later basically it is still called as a loss only just that that's why I said right the loss concept is very different right him and like one concept of losses minimizing the loss between the actual and predicted which is deep which is the fake and the real the other is the loss between the the real image and the fake image itself the difference in pixel values itself that also is a loss so that also both both have to be considered in this case for it to be able to you know create a fake image that is resembles the original image basically I'll show you with the example I think you guys will understand that better that is when we will go into the optimization as well okay so this is something we'll come back to there are multiple gant models we will do one example of a gant model and there are multiple other ones as well things like for example conditional gant a conditional gant is nothing but you can bring in conditions also right you can bring in some sort of what do we mean by a condition is basically you can ask for let's say for example how do I put it so let's say you treat a certain number of you bring in a y variable over here right so let's say for example I bring in to the discriminator or to the generator some y or a label let's say male or female so I'm sure many of you would have seen this where you know you can convert a male image into a female image a fake deep fake where you know you can convert the male person looking like a female person right where you're now introducing labels perfectly purposely so that it can actually generate images where the variation is completely a gender change or you can create a person change like Donald Trump looking like let's say for example you know I'm sure this many of you would have seen right Donald Trump in Jong deep fake right I'm sure you can see this many of you would have seen this kind of images right I think they've removed all of that but there used to be there used to be this image where Donald Trump's face looks like Kim Jong Un's face so this Google has removed it because of deep fake purposes right so earlier there used to be an image I'll show you once let me just I think there was this PPT which I had seen somebody had presented it somewhere sometime long back yeah I'll show you here so this is where it purposely brings in some sort of labels actually here right so this I'm sure many of you would have seen right an image of Donald Trump looking like Kim Jong Un and Kim Jong Un resembling Donald Trump how you do this is you basically purposely bring in a Y label which is Donald Trump and you bring in let's say for example Kim Jong Un so you bring that also very complicated but don't worry about it for now I'll explain that a little later when we do the hands-on example how it actually is able to do that no for now only images is what it can capability to that's the only capability it has right now for it to work so this is how the models actually look like we'll do an example and I'll show you this is how the generator has if you noticed this is our simple CNN isn't it I'll just explain this concept then I'll let you guys take a break so you know if you notice here this is a simple CNN this we have all seen yes or no haven't we all seen this a CNN model that you have over here we have all come across this simple CNN model nothing very complex but look at the generator this is where it gets interesting so can you see the generator what it is doing is exactly reverse process of a CNN basically right it is not doing the same thing like a CNN so if you notice here what it is doing is you're just feeding the you know you're just feeding the noise as an input and what it is doing is it is using something called SNUPSAMPLING 2D the upsampling 2D is the exact opposite of a convolutional layer exactly the opposite if the CNN what happens is you have the filters are going from left to right to the bottom to top and they are actually you know doing a feature extraction the exact opposite process is actually followed so what the filter actually does is the filter this time also goes through the entire image but remember the image has random pixel values right it has noise so here it is noise right so this noise is what is there in terms of pixel value so let's actually visualize how this works so let's assume this is how it looks like so let's take an example so let's say I have a a visualization like this right now what will happen is that assume that the filter is going to be on top of this portion here okay so this is where the filter is so I'm going to call it as let's say like this this is the filter values okay so the filter is going to have some values right it will also have random values let's say 215 235 155 165 and here remember you have randomly generated some noise right so 25 35 45 and 65 so what will happen is they will have a dot product so this portion here will have a dot product with this portion here so when they do the dot product what they will do is they will recreate pixel values so through this process they will actually regenerate an image so when it keeps moving from here it will actually regenerate an image it will recreate an image so it will create additional pixel values it will not reduce the pixel values it will recreate so what will happen is eventually when you provide this it will keep adding more and more pixel values as it passes through multiple portions here and then eventually it will recreate an entire image like this so in the process while it actually goes through it it will be able to recreate additional pixels and that pixels will recreate an image it's a reverse convolutional process that it actually does so that is where the generator is able to recreate pixel values and create a generated image and then you have of course the original image pass them through the discriminator and it can classify between a discriminator and a generator in this case right shall we see an example of this yes do you guys want to see an example of this and then we'll come back to the generator models later okay so we'll do that let's do a quick 10 minute break I believe everyone is a bit tired getting a little break would be a good idea so let's do 15 minute break or let's do 13 minutes break right so let's come back at 950 right shall we do 950 okay let's come back at 950 thanks guys you you knowledge. you you you you you you you you you you you you you you you you you you you you you you you What happened to the others? I think 10 minute break we gave some of them are still not back. It looks like. OK, now, before we get into the hands-on use case, I believe some of you wanted some more things to be covered on generative AI. And I told I spoke to the academic team as well. And they said, you know, what else additionally we can do with respect to Genie. I thought I'll take feedback from you guys if there is anything specific you want to cover with respect to Genie, right? Not just computer vision, anything in general, right? What do you want to cover for Genie? Bear in mind, I'll try and see. We can cover as much as we can. But you have to also remember that not all use cases of Genie can be covered primarily because of the fact that hardware is required, right? I know many of you asked for deep seek, for example, right? We can do deep seek. I have no problem. But then the thing is it's not about running deep seek, right? It's also about deep seek being able to run on the hardware that we have as well. So pulling the model and running it is very simple. It says that some of these models are so huge that it's not so easy to run it on a local or on an environment. We primarily access through API is basically. So I can talk about that. Model deployment is something that's anyways there in our next session. So we don't need to worry about that 50 because we're going to cover deployment as a separate concept altogether, right? In the, you know, it's called as an AI apps. And we have a separate session where we will talk about how to create AI apps. And how those AI apps will be deployed onto cloud service, et cetera. Because right now since we are not doing cloud as such, like the different components on cloud, we can't really talk about deployment in details. We'll talk about that later. OK, so anything that you guys specifically want to cover, please let me know. It's something if you can put in the chat. I can work on it and then I can get back to you guys. RIG, vector DB is chunking LLM observability, given feedback to the model. OK, so we'll do that. Yes, OK, I'll bring in something related to RIGs and vector DBs and chunking as well. I'll, we'll do that tomorrow. So we'll go through that as well. I'll show you how to run it on local. And we can, we can talk about that. Maybe tomorrow I can get you some stuff on that. So we can go through it. Yes, for sure. So that I'll take it as one optimization of the models. So when you say optimization of the models, what do we refer to it? Mukti Kanta, if you could, if you could elaborate a bit more. So Ram, DB scan is not part of generative AI. So tomorrow session, primarily, we are focusing on Genai aspect. So that's where we'll try to focus on that. So DB scan is more on the ML part. So that'll kind of deviate us from this. So that's how that'll be a little difficult. I can share whatever I have in terms of information with you. Exposing a model as API, OK, sure. I'll walk you through API as well. I'll give you that. OK, we'll go through deep seek. I'll walk you through what exactly it is. Those things I can probably include in as well. See, very specific use cases would be a little difficult. Like in terms of cloud service optimization, specific to that will be a little difficult. Like specific use cases to pick up will be kind of difficult for me to do. But then I can definitely, we can walk through. We can create a kind of learning session where people can ask questions and I can answer on the fly and give you guys certain inputs as such basically. How to optimize if the model takes time. OK, so the GPU aspect, expect, etc. Again, that is something called as CUDA. That again is not something that I'm very comfortable with. I'll honestly say where I'm not comfortable with because I don't want to teach the wrong things. But I can definitely get you a lot of information on the CUDA toolkit. So what all is available, how you can do it, that I can show it to you. See, for you to learn CUDA, also the important thing is you need to have access to good amount of GPUs, right? So having access to Nvidia GPUs, etc. So if I show CUDA on Google collab, you're not going to be able to appreciate or won't be able to see much of a difference as such. So optimization in terms of hardware, it's not something is my, you know, it's not my cup of tea. So I won't really step into that that much, but I can give you all the details you want in that case. OK, agent AI also, OK, we'll talk about that as well. I'll bring that in. OK, there's a web app to streaming, OK, so apps is something you guys want to talk. So I see three things that we need to discuss. So one is basically RIGs and, you know, the entire concept of chunking and all of those. Then agent AI is something that all of you want to discuss. OK, we'll talk about that as well. And yeah, stream-lit apps or web apps, basically, how to create apps, that's something also we'll talk about. It's stream-lit is something we're going to talk right now. I'll explain that in some time from now. We'll do an use case. And yeah, we'll go through that as well. Now, all of these models are not on dark web. They're using only data available in the real web. Dark web data will be really dangerous to train models on. First of all, it'll be too much data to train on. Second is, if they get trained on it, then, you know, they might even have some sort of issues, basically. Right, so I'll focus on these three things. And then is there anything else we want to add? No, deployment is a separate thing, Amruta. We'll have it separately, basically. We'll go through that later. Got it. So the application part of it, I'm clear. So I'll include something on the app part of it as well. So how do we create apps or what kind of apps, etc. Front end, basically, that we'll talk about. So the DevOps role, I'll talk about later as well. Right, I mean, that we have in the deployment part. We'll we'll go through it separately, Russian, because we have a session specifically on Google Cloud with like DevOps and all of those things. See, mobile app is completely different. See, guys, you have to remember that we have to cover within the data science part of it. Right. So remember, this is a data science session. This is not a full stack development session. Okay. So I'm being flexible to, you know, bring in as many topics as I can, but don't ask me how iOS apps and how Android apps and SDKs are developed and all of that. Okay. We have to also be able to understand that the sessions are. We are trying to make it flexible for all of you to take as many things as you can. But then anything that goes off topic from our data science or AI part, I won't be able to cover it up completely because that's that's different. Right. I mean, you have to remember it, you know, people spend years of their life specializing on. Full stack development or app development and all of that and two hours or three hours of a session cannot cover the entire world of and, you know, software architecture basically. So those aspects, some of it, you know, we'll have to filter and choose basically. Sure, we'll do that. Yeah, Janay solution. Those aspects, yes, we will bring in all of that. Definitely, I'll bring it in. Again, hardware's I'm not very comfortable. I don't want to get there because that will kind of see something. I'm not very specialized in is a wrong information to bring in as well. Right. So that's why I don't want to. So cloud no need to worry about cloud. We have a separate session. We have a separate module altogether in the next case where we will talk about, you know, the cloud accesses and running on cloud, developing on cloud creating apps deployment. Everything that part is covered in the next module that we have. Right. So we'll be doing it on GCP. We have a session separately on that so that we will go through it in the next module. But this will be primarily focused on whatever we've covered with respect to Genai, anything specific you guys want to discuss. We'll do that tomorrow. So as of now, I've decided there are three things we will do for our session tomorrow. One is as I think somebody mentioned RIGs and chunking and all of those concepts. We'll go through that definitely. The second concept that I think somebody asked for is. Just let me take a look at it quickly. APIs, which is something that you guys wanted to understand. How do you get a model as an API, etc. I'll walk you through that through the session of RIGs and all of that. I will also explain to you the concept of deep seek. What is it? How is it different, etc. It is no different actually to be very honest and surprised that people found deep seek so great. Actually, in fact, I am not surprised at all about deep seek actually. But anyways, I'll talk about that. Yes, and Agenti. AI also, I'll explain to you what that is, how it works. But one very important thing you all have to remember, right? It is a three hour session. Okay. So in a three hour session, I cannot treat each everything in the software world as a whole. It's huge. See, people spend years on architecture, how to build and everything. So there will be some loose threads that you will have to pick up outside of the session. A three hour session will not be able to cover. So let me be very straight on that also. Of course, I will cover on all the entire concepts on RIGs and Agenti and everything. But within the Agenti world, there are multiple things, right? There are tools, there are APIs, there are everything. So we need to understand API concepts, etc. How does API work? Why do we need APIs in the first place? All of that, some of those things, a little bit of a background work will be helpful. So why I'm saying that is in the session, because it is three hours, you have to also cooperate with me, is the sense that many people immediately get upset that, Arul is not covering this, Arul is not covering that, Arul is not doing this. No, the reason why I'm saying that is primarily because many of them are, we want to learn everything, but there has to be a background to certain things that we come to as well. So from the scratch, I will not be able to teach APIs, because that's a little difficult concept or a different thing altogether. I can talk to you about a wrapper of an API, how it works, etc. Then there are, of course, concepts like tools and all of those things, which I will explain to you. But some of it, I would request you, if we can't go to the depth, I will give you the resources that you need, that you can actually use, and you can learn as well if you want to. Does that make sense for everybody? Is that okay with everyone? Yes, I hope you are all aligned on that. We have to understand the reality also. Some of these are so new, technologies are so new, things are so like, agent-like AI, for example. It's changing so fast. Just about a month back, people were talking about single agents. Today, we have multi-agents, and now you have multi-tiered agents, and then you have multi-tiered agents with multi-cloud and multi-model capability. Within weeks, things are changing. For us to be able to convert it into a full-stack process is quite difficult, because a lot of it I'm myself figuring. In fact, I still don't know how multi-cloud multi-agents work, actually, to be honest. I'm myself figuring out how I should make multi-cloud multi-agent work in multiple scenarios, etc. It's a tricky thing. That's where I can provide you the foundational knowledge. Since the world is changing so fast, some of it you might have to learn from outside. I'll give you the tools. You can definitely research through it, and you will be able to understand all sorts of it. I think the entire deep-seek thing is my personal opinion. It is bound to happen. Everybody knew it will come to it. And again, think about one thing. Deep-seek is not completely opened up by everybody. There are still some things that people are still figuring it out about it. It is a bit of a knee-jerk reaction that many people have given to it. The Nvidia shares falling and all of that. You have to bear in mind that Chinese are not really the most open people in the world. Even though deep-seek is free, it is cheaper than O1, O3, and all the open AI models. Even if open AI charges me three times more than what deep-seek charges, I can say it openly, honestly, in the session that I will pay and take open AI versus deep-seek any day. I will pay. And I could be the last person in the world ready to pay for it, but I will pay for it and I will take it. And that is because I don't trust Chinese. And there are reasons for it. So, hence I'll get into that probably a little later. This will be a very controversial topic. But anyway, the point is, I don't trust any model. That comes from China. I don't. And whether it is Quen or deep-seek or it is any model that tends to build or anything, I know there are going to be challenges. And people are just a bit too hyped in the cycle of AI that they are making it a big deal. But in reality, deep-seek is not anything that is very great. It is just a large language model. TrainDon probably the same data open AI has trained on. Another thing is that they have claimed that they have done it at 5 million. But that is purely because they have additional GPUs that they have imported without actually declaring to how many GPUs that they've actually got. So, many people are not aware that they have actually used the same amount of GPUs that open AI has done. It's just that they have not shared it openly in a public forum like how much of the GPUs they've used. They've just called it as I think some 100 or 15 something around 100 GPUs they've used, but they've not. They've used more than that. So, I'll probably walk you through that. Anyways, coming back to our session over here on the generative AI on deep-seek, sorry, deep-seek I'm saying, on deep-fake, we'll do a quick example of it. And we will see how it actually works. Is that something we all can do? I'll just share the notebook with all of you. Just give me one quick moment. Yeah, I have it here. Just give me a moment. Can you all see my screen? Everybody? Sorry, I should see my screen here. Okay. Okay, I'll just share the notebook as well. Everybody has the notebook with them. Just let me know. So, once you guys are ready, we can get started. So, let me explain to you what we are going to actually do here in this example that we're going to cover right now. We are going to actually build a deep-fake model. And we are using the data set, which is the fashion-emness. Now, I'm sure many of you are wondering why are we using this fashion-emness data? Why can't we use any other data that's available online? Right? You can. Okay. And you can do it also later. The reason I'm using fashion-emness is because remember, this data is not too big. Okay. In order to train a really good GAN, the first thing you need to know is that it requires a lot of GPU. Right? So, the deep-fake apps that you come across online or in different places, primarily are built on a large amount of data. They're built on massive amounts of GPUs and huge amounts of images. The code per se does not change. So, whatever you're going through, understand from the code perspective. I want you to understand from the process and code perspective. At any point in time, you can change the input data from what you see over here to any other data that you want. Okay. It doesn't matter. So, tomorrow if you have GPU access, like when we do the cloud sessions, etc. When we will do Google Cloud GCP sessions, you can always replace this input data with any other input data. You can even feed it with cats and dogs and data. And automatically it will start generating deep-fake cats and dogs images also if you want to. Right? But only for the session for you to understand the code and to be able to run you know successfully within this entire environment, I'm going to show you with examples related to fashion image. Okay. So, what are the exactly going to do? So, there are two things. One is we have the real data set. This is where you have the fashion image data set that will be used in this case. Right? So, we'll use the fashion image here. And what I will do is I will store them somewhere. Right? I'll store them as some sort of real images basically. Right? I'll call them as real images. And I will store them over here. Then what I will do is the second part I will generate noise. Okay. Using the np.random function. And that what I will do is I'll pass it through an up sample function, which you will see here. And that up sample function will recreate a image. Now the original image and the up sample image will both be fed into the discriminator. And the discriminator will actually do a classification, whether it's a real or a fake image. So, it has to provide a zero or a one basically. Now, what will happen is that this will go through multiple iterations, where first I will pass certain set of real images. Then I will recreate fake images. Then I'll append or concatenate both real and fake images. Pass them as an input. Then the model has to discriminate, etc. Calculate loss. And that loss, what I will do is I'll again feed it back to the generator. And then the generator will update again. Recreate. There will be multiple iterations. We will go through. So step number one, what we are going to actually do is we are going to first define the, we are first going to define the real images itself. So step number one is to give you the real data that you want to work with. Step number two, what we are going to actually do is specify the noise. We are going to actually specify how the noise has to be generated. After that step is done, we stop. We move to the next part, which is we will start creating the generator model. So we will specify the layers for the generator model, which will be a set of layers, which we will see a little later. And then we will set up the discriminator model. So there will be a certain set of layers. So here you will have basically upsample 2D. And here you will have a convolutional 2D. So that will be primarily the difference between the two models that you will come up with. Then what you will do is you will bring these models together because they have to work together. So that's where you will have to compile these models together. And then after we compile, we'll fit them. The only difference is earlier we have learned fit as x, y. Now that does not apply over here. There is no fit x, y. So there is an alternative approach to fit the data. So that is where I will explain to you how the fitting works. Once the fitting works, remember it will go through multiple iterations. It will go through an updating process. And then eventually it will learn enough for it to be able to generate an image that looks real or close to real basically. Is this process clear to everybody in terms of how we are going to process the fake and real images? Yes. Any questions? Any doubts? Any clarifications? All right. All right. Okay. So that's where we are going to create it. Again, there are some additional gans that have been given. But no need to worry about it. I think we are pretty much aware of this data set. It's a simple data set. It has all these 10 categories of handbag, shoes and everything. The idea is eventually the models should generate by itself a handbag, should generate by itself a shoes or it should be able to generate, let's say for example, t-shirts, shirts, etc. It should not actually be creating some sort of garbage. It should create some by itself over and above the training data. So maybe tomorrow, 60,000 training data should be augmented with 10,000 additional fake images and fake fake kind of t-shirts and shoes and bags and all of those things. That is our intent basically. That is what we wanted to develop in this particular case. No, this is not based on text prompt RAM. That is a separate session. So right now there is no textual prompt at all. That is called as diffusion models. We'll talk about that later. Once we are done with this example, I'll come to the concept of diffusion models. That's next part of our generative AI image problems, which is where this comes in, right? Diffusion models. So that's a separate thing. We'll come to it later. I haven't covered it. Right? I hope everyone's clear. Yes. Any questions? Okay. So first things first, what we will do is we will import all the libraries. Now here is where the most important library comes in. I think rest of it is all fine. If you look at it, this is the first important layer that we are actually getting in here. Can you see all of it? Up sampling 2D. Can everybody see it? So we are going to use the up sampling 2D. So compared to all the other layers, the only new layer that is coming in over here is up sampling 2D. I'll explain a little more in detail at a later point in time. So don't worry about it for now. We'll come to it a little later when we are actually getting to that particular layer. I will go into the depth of what that layer actually does. I'll talk about it a little more in detail. Now to load the fashion image data, we don't need to worry too much. TensorFlow has already got something called as TFDS. TFDS is nothing but TensorFlow data services or I forgot the full form of it. But it's basically TensorFlow has all the data sets in this particular case. So some of the standard data sets it already has. So that's where we are going to load the data, tfds.load into this particular variable called as DS. And the dataset name is fashionemnist and you split it as train. You just need the train dataset. You don't need all the dataset. So we just need the train dataset. So rather than we uploading the zip files and doing all of that manually, this file is already uploaded. So we don't need to manually upload all of this data. So give it a few minutes. It'll just load all the data. . It is already split into train intestines. Yes, sir each. But we will not require all of that. We just require I think only the train data. If you give test also, it'll give you train and test. Both will be given. But since we don't want to validate the model as a train or test, we are not doing a kind of supervised learning. So that's why we will not worry too much about that as that's. Okay, are we all clear? Yes. So you can see when you give type DS, it's called as a TensorFlow pre-fetched dataset. So what is a pre-fetched dataset is that this dataset is. Everyone remembers the image data generator function. I hope everyone remembers that. So what it actually does is it converts that into kind of, it doesn't store them as such. Guys, if I got an image data generator function, everybody. People have forgotten the function I think of image data generator. Image data set from directory basically. Image data set from directory. This is a function everyone remembers to load the data. No. So we give basically the cats and dogs image example. Everyone will recall. We had passed the directory where it actually stores it. So remember that if you use, let's say for example, a CV2 function to read an image, it will immediately convert it into an array. And that will take away our storage space over here. So we want to be able to create it as a pre-fetched. What is a pre-fetched? It will be passed into an array only when you need it. It will convert it as an array only when you read it. When it has to pass it as an input to the model, that is when it will do it. We are optimizing the memory. Otherwise, what will happen is the moment you store it as an array, the in-memory that you have will be taken away from you. So if you have one GB in memory, and this is already, let's say, somewhere about, let's say after be compressing all of it, if it is already 100 MB, gone, you've lost almost 10% of your memory already there. Now, the remaining 90% has to be used for processing also. So that doesn't make sense to use the in-memory of the model for storing all of this data. So what we are doing is we are storing it as a pre-fetched. A pre-fetched would be, it will be converted into an array only when it is required. Right? So you can see here, when you use this function next, only then it converts it as an array. So if you see here, ds as numpy iterator, comma next, only then it converts it as an array. Until then, it will just remain as a pre-fetched. As a, it's not really a variable. It is just not going to store it as a value as such. So it will be converting it only on the requirement. Intent is primarily to convert it into an array only when you need it. Don't need to convert it into an array immediately. Right? It doesn't make sense. Otherwise, you'll end up consuming your memory. Likewise, you can see this, it's a dictionary actually. So it has basically two things, image and label. So if you give it this data, you see here, it has got two parts, image and label. Of course, we are not interested much in the label. We are only interested in the image. And you can actually see, right? It can show you all the images as a set of pixel values as you can see here. These are the pixel values it has. Now, you can plot these images also. We've seen this in our previous session. I'm sure everybody remembers that example from our a CV one session, right? I believe everybody recalls seeing this. Yeah. So I'm just simple. We are taking the pixel values and we are plotting it into a 20 by 20 pixel sized image, 28 by 28 pixel image. So not much over here. Just that four random images have been plotted. A trouser, a handbag, a pullover and another trouser, etc. We have created these are the real images that we have. Okay, these are real images that we are working with in this case. Are we clear to this part? Any questions? Any doubts please ask before we move on to the next part. So in the pre-fetch that you do over here, if you want, maybe later on, you can actually just remove this DS and you can do a image data set from directory. Use this function and you can load all the cats and dogs images also if you want to later on. And those cat and dog images, you can pass them through the can. I'll show you how, but you can do it later so that you can avoid, you know, during the session, it will go slow basically. So first thing we are going to do is to scale the images. Okay, so we have pixel values as you can see here are 50, 165, 108. We want the pixel values to be in the range of 0 to 1. Okay, so that is where what we want to do is divide every pixel image by 255. And what it'll end up doing is it'll ensure that the pixel values are every single pixel value. So if you, for example, take here, 191 divided by 255. So if you do that, it'll, 191 divided by 255 will give you, if you do that, it'll give you a value of 0.74. Same way, if you do 255 divided by 255, it'll give you one. On the same way, if you give, let's say, for example, a pixel value 15 divided by 255, it'll store it as 0.05. So the point is you're just, you're just making sure all the pixel values in the images are normalized or you're bringing them in a range mapped, scaled, more than normalizing, you're rescaling them. You're not normalizing them because we're not subtracting it by mean and dividing by standard deviation. We are just scaling it between 0 to 1 in this case. Helps actually because otherwise what will happen is the gradients will be too large, right? The difference between the actual and the predicted values, the gradients will be so high that you will see like 400, 300 that we saw earlier, right? That will be the values. We don't want such high gradients also. Then you have a function called as ds.map. What is this map function is within the prefetching function, the image data set from directory, it has something called as map. And what it will do is this is a, this is called as a scaling function, right? We have created a user defined function called as scale images. Now if you want to apply it to the images, you can't simply take scale images as a function and pass it to the images. So just like how you know you create a function and then in pandas you have to use and apply or a map function, the same way you have to do it over here as well. So that's where what we do is ds.map on the entire data set that we have, we apply the scale images function, right? Now after scaling the images, what we will do is we'll use a cache function also. Now the transformation that has been done, it'll be cached, it'll be kept in the temporary memory. So that it doesn't have to recreate this entire process again and again and again because unwantedly we are going to use the compute for redoing this process again and again. So the moment you enable ds.cache, it will create a caching memory. Caching remember is a temporary memory, which means after the training is done, it will stop the entire, it will delete the entire memory that it has in its cache and it will restart the entire process, right? So that's how caching will enable that during the epoch, it doesn't have to keep recreating or recomputing the entire scaling process. Likewise, shuffling the order of data set, right? When you give the fake images and real images, you don't want to give them in a sequence. If you give them in a sequence, it's very easy. The model will learn the pattern of sequence. It'll know that the first 10,000 images are fake and the next 10,000 images are real. That pattern, it will recognize and it'll always be able to distinguish between fake and real. You want to shuffle the order, right? It's like for example, you know, somebody who's trying to detect fake currency from real currency, you don't give him first all the real currency and then the fake currency. You mix the fake and real currency and you want him to shift or be able to distinguish between both the groups. So that is where we need to use the shuffle function here. And then followed by that, what we will do is we'll convert it into a batch of 128 images. So 60,000 images divided by 128, right? So what will end up happening is you will end up having 469 images or 469 batches that you will actually create, right? So 469 batches, not all 60,000 are going to be fed into the model in one shot. And from that 469, sorry, from that 128 batches that are there, we will further have a prefetching of about prefetching basically is converting only about 64 from that batch of 128 into an array. So that is where we are not going to take all of them and converting it into an array. So you can see one example here, we've actually converted it and we've got a 28 by 28 pixel image that it has created. So all these steps are nothing but pre-processing the image before we pass them to the model in this case. Is this part clear to everybody? Yes, any questions? Because now is the most important part which is the building the network. So before we get there any questions or doubts or clarifications, all clear till now. Which part you want me to explain again Amruta, can you be a bit more specific? Pre-fetching is nothing but ensuring that it converts it into an array only when you pass the input into the model. Until then it will not convert it into an array because if you convert it into an array then the array will have to be stored in as an input here and that will consume the in-memory of the data. So you don't want that to happen. Otherwise, take away the memory of the data. So that's where when you use a pre-fetch it is still storing in its local form. It will only convert it into an array when you start to do the model.fit. That is when it will be converted into an array. Okay, everybody clear? Yes. The next part is important. So just before we move on to the next part, please if any questions, please ask me because the next part we want to make sure that we are all clear in terms of its working. Okay, I believe all clear I will take the silence as clear. So now comes the most important part, right, which is we need to create a model which is called as the generator. Okay, now remember, the generators job is to generate fake images. Okay, now in order to generate the fake images, it needs first and input. So what is going to be the input to this generator is it is going to actually get random noise. Okay, so what is a random noise basically? So think of it like this, right? So let's assume that I have a random noise of here we have given it as let's say how many is it one second just a second? 7 by 7 into 128. So let's call it as 7 into 7 into 128. Okay, so that's about 6 to 7 to. Okay, so what we are actually going to do is you see here 7 by 7 by 128 that in total comes to about 6,272. So let's assume that the first thing that we do is we use a np.random function. Now remember in the layers you don't need to specify the input, right? So you don't need to worry about it. So you don't need to specify inputs here. All you need to do is this random will actually generate a single vector. It will generate a single vector. Okay, and I'll show it to you actually you will see it is there in the next part here. I'll just show you. You can see here random values. We can actually pass that random value. So it's there in our data set if you see np.random.random. So what we are going to actually do is we are simply going to take a 6 to 7 to single dimensional input. Right, here we go. So this is going to be a 6 to 7 to single dimensional input one dimension one B. Okay, of values. What are the values that are going to be there over here? They are going to be random values. Right, completely random. So if you use np.random function, I'm sure you all know what it can actually generate. The first thing I do is I pass it on to a dense layer. So what will happen here is that remember there will be weight. So if it's a dense layer, you will end up having whatever number of you know, whatever the number you have, it will just give me one second actually. It's not 6 to 7 to my apologies. Just give me one second. It is actually 128 input. Sorry about that. Really sorry, not 6 to 7 to. So what it will do is it will take 128 input dimension is here my apologies. So what it will do is it will take a 128 single dimension like this. Right, so it will have values of 128 values. It will have and what it will do pass it through a dense layer. Now the dense layer remember has its weights and biases. Okay, so we all remember this concept from our earlier sessions. Right, so there will be some set of weights and biases that will multiply with the inputs and with the bias and they will generate an output. What is going to be the output shape over here? They are going to be of the shape 7 by 7 into 128. Is this part clear to everybody? 7 by 7 into 128. Okay, so they are first going to convert it into a 7 by 7 into 128 is what it will do basically. How is it converting 7 by 7 because remember the output shape? Right, in a dense layer, I can specify the output shape. Generally, we have given it as 256, 512 etc. The only difference is I am converting it into a 7 by 7 into 128 shape basically. Original image is not 7 to 7. Original image is 28 by 28 pixel resolution. Right now we are only talking about the fake images, the generator. Original images, I will come back to a little later which is what we are doing later. Is this part clear to everyone? Yes. Are we following it? We will come to upsampling in a moment. Just give me one second. Right. Now what we are actually going to do, if you notice very carefully here, that is where the first step comes in. Right. So first we create a user defined function called as build generator. Okay, so we are going to put the entire layers into this function called as build generator because when we compile it, we have a generator and we have a discriminator. So both these models have to come together. So that is why build generator will first create a user defined function where all the layers of the generator model are there. So the first layer is we are sending it as a sequential layer because remember within the generator, all the layers are sequences. So you can see here, first is you have 128 inputs which you are converting it into a 7 by 7 into 128. Now look very carefully. Right. So the input dimension is 128. So randomly generated 128 values that you have and after that what you are doing is you are converting it into 7 by 7 into 128. So you can see here it has you know 7 by 7 into 128 total of about 6 to 7 to that is what it actually has. So it passes it through it and it generates a single dimensional output. Right. 1D, one dimension is what it actually generates as an output and that it passes it through a leaky relu. Now what the leaky relu will do is unlike a relu. Now remember that when you are generating random values, right. The random values will definitely be of what shape it will be of any values negative to positive, any value can exist. Now what it will do is some amount of negative values it will allow up to 0.2, negative 0.2 or whatever that values it will allow it. But any high number of negative values it will not allow. Everything else that is very high in terms of the negative values, it will ensure that it will convert it into zeros but allow for a small amount of negative values close to 0 it will be those values it will allow for it. And followed by every positive value it will leave it as it is. So that's where we are bringing in leaky relu and not relu. Why do we want leaky relu is because we wanted to we wanted to allow for some amount of non-linearity, right. Some amount of variations to actually bring in in this case. So that is why we are bringing in a leaky relu in this particular case. How did we arrive at 7 by 7? Okay, the eventual is you want to get it to 28 by 28, right. So you are upsampling it. In your down sample what did you do? You took 28 by 28 and you reduced it down to 7 by 7, isn't it. Now you are doing the reverse process, right. Like we discussed here if you see it's the exact reverse process that we are doing, right. So the generator has to be exactly opposite to the discriminator. So which means the generator has to start small which means it is one fourth of the final image that is generated. So in this case the final image has to be 28 by 28 pixels because the real image is also 28 by 28 pixels, isn't it. So you start with 7 by 7 which is one fourth of the image and that is why you've taken it as 7 by 7 over here. So if it is 250 by 250 find out the one fourth of 250 basically, right, which will be about what will you come to about 50 by 50 something odd like that. You start with that and go on to 100 by 100 and then 150 by 150 and then you can go higher. Can we take 4 by 4 also you can take it but that will be too small right Amruthar then it will become too difficult rather this is one fourth of the final image that you are actually generating. So that's why it's okay to keep it at 7 by 7. Is this part clear to everybody so you can see here 6 to 7 to why is it 6 to 7 to 128 by 7 by 7 basically, right. That's where you have you know the output of this dense layer is it is going to convert it into a single dimension like this it's going to convert it into a 6 to 7 to then followed by that it will pass it through a leaky relu, right. Leaky relu, let's say for example, there will be values here, right. So let's assume that it will be minus 0.01 minus 0.5 okay, 2.5 and let's say 3.5. So when it actually passes it through a leaky relu, what it will do is it will take the same matrix and it will pass these images. So let's say for example what it will do let's take an example the same pixels that you see so minus 0.01 is very small it will allow for this negative pixel to go through but minus 0.5 is high so what it will do is it will convert it into 0. 2.5 and 3.5 it will leave it as it is because it's a relu, right. So positive values it will not disturb it. So that is what it does in terms of leaky relu over here till this part are we clear? Yes. Any questions on this? It's a random value that is being generated Amruthar. So we use the np.random function, right. So that is what is generating the random value here. Now this I will apply a reshape okay. So the reshape will convert it 7 by 7. So typically what it will do is it will take it's just a reshape function nothing changes as such the values remain as it is. So this is I have converted a year into 2 by 2 right. So it will look like this. Imagine it has to be 7 by 7 how it will look like. So 0.01, 0, 2.5 and 3.5 is what it converts it into. Now this is not the discriminative model we are still at the generative model basically, right. Is this part clear to everybody? Yes, we are still at the generative model we have not brought to the discriminative. Are we there or not people? I can't hear anything from anyone. Only two or three people are saying yes, we are clear. See because the problem is after I finish explaining sometimes people come and ask me a question from the first fact, right. So it will be a little difficult. So that's why just clarify if you are clear or not. Now you cannot have an image as 2 by 2. Can you feed this image of 2 by 2 into let's say for example into the discriminator. Let's say for example, I feed this is 2 by 2 as an example. Now this is a fake image right. Let's assume this is called as my fake image. What is the shape of the real image? Shape of the real image is 28 by 28. Everybody agrees with that. That was the original shape that we have passed in. So this is 28 by 28, which is real. Now if I provide this fake versus this real, isn't it easy for the model to discriminate? I mean come on. This is 7 by 7 and this is 28 by 28. The shape itself doesn't match. So automatically what will happen? Easy to discriminate? Does it make any sense to make it so easy for the discriminator in this case? Doesn't make sense, right. So what do you need to do is you need to actually make this 7 by 7 whatever you have over here. I have put it as 2 by 2 as an example. But let's say for example, we call it as 7 by 7, right. Let's call this as 7 by 7. Using the reshape function, we have a 7 by 7 value, which has some pixel 0.010, 2.5 and 3.5. What I need to do is I need to make it bigger. I need to get it to let's say 28 by 28 pixels. But I cannot directly convert this 7 by 7 into 28 by 28 pixels. So what I will do is I will increase it by one shape. I will increase the entire the entire matrix by a higher value. So this is where the function of up sampling actually comes into place. See remember the down sampling does the exact reverse. It converts your 20 by 28 by 28 into 14 by 14. Isn't that what you learned in your CNN layers in your CNN model? A 28 by 28 14 by 14 by 14 becomes 7 by 7 and a 7 by 7 becomes a 49 single dimensional vector, which is what we call as a flattened array and that flattened array is fed into the dense layer. So what you need to do is this 7 by 7 needs to be converted into a higher dimension. Because you are generating, you're not compressing, you're not trying to encode it right now. You're trying to recreate an image. So what will happen is this up sampling layer will take the 7 by 7 and generate as you can see here a 14 by 14 pixel values. It will generate a 14 by 14 pixel values. Now what will it actually do? It will recreate the entire pixels multiple number of times. Okay, so what will it do? It will just take the same values minus 0.01, 0, 2.5, 3.5, etc. It will create like this one. Then again, it will create over here also another set of pixel values. The same value that will repeat it will keep repeating it multiple number of times in order to make it a shape of with additional values, Russian. In a way, it's like a copy paste of the random values that it has actually generated. It will copy paste it across multiple places. So you'll have minus 0.01, 0, 2.5, 3.5. Basically, the copy of the same thing that it has generated here, it will just generate a copy over here. 0.01, 0.7 by 7. So you'll have again minus 0.5, 3.5, likewise minus 0.01, 0, 2.5 and 3.5. Till this part are we clear? Yes. Create a patch. It creates a patch of the same values over here across multiple things. It will create the patch over here. Yes, now how it actually converts it is. See, it has been reshaped, so it is 7 by 7 and you're reshaping it into two dimension only. So you're making sure you are actually reshaping it into a two dimensional. So it will, since it's a 2D, it will make sure that it takes an input as a 2D and it also converts the output as a 2D. So when it comes in, it's a two dimension and when it goes out also, it's a two dimension. So that's where you can see 7 by 7, comma 128 converts it into 14 by 14, comma 128 in this particular case. You can't create directly, it has to go step by step, if it does directly 28 by 28 pixels, that's a huge shoot up, right? So it will have exploding gradient problems, etc, which will end up happening. So it has to progressively go from 14, just like how you do your dimensionality reduction, right? You have to basically go step by step, 28 by 28, 14 by 14, 7 by 7, like that you have to reduce it. So it has doubled, no? Isn't it transient? 7 by 7 has become 14 by 14, hasn't it become double? The shape has become exactly, so here you have, let's say four, here it is ended up becoming 16, isn't it? So you have doubled it in a way. Okay, I'll wait for your question in the meanwhile in the interest of time, I'll just move on to the next part, just in the interest of time. Okay, so this 14 by 14, what we do after we upsampled it, we actually pass it through a convolutional 2D. Okay, so you can notice here, this is where the next step comes in, which is convolutional 2D. Now what the convolutional 2D will do is it will apply a, it will apply a filter here. So if you notice here we have the convolutional 2D, it will apply 128 filters on top of this, right? So it will apply 128 filters on top of this randomly generated image that you have over here, it will pass on to that basically, just give me a second, here it is. So if you notice very carefully here, can you see this, 14 by 14, 128. So the shape will not be changed, okay, it will not change the shape over here, what it will actually do is on the same image that you have, the upsampled image that you have, which is 14 by 14, it will now run a filter, okay, so it will run a filter of 128 filters, it will actually run, which are of the shape 14 by 14 here, right? Or sorry, I'll check the shape, what it has given over here, but let's assume it is a smaller shape, right? So let's say 5 by 5 is the filter shape. So what it will do is it will take the filter and run it across this entire image. Please pay attention to this part, this is a very important thing that we are talking about, so that's why I want you to understand here. Are we following till now, not following, can I get some feedback from you guys with me till now, not with me? Till the upsampling everybody has followed, have you understood till the upsampling, all it is doing is just taking a copy and making four copies of it, right? It is just making the same thing here again and again, but remember that doesn't make sense to us, right? Basically, so that's where we need to actually apply the convolutional 2D. Please have some not upsampling, exactly. His downsampling reduces it into half, right? I'm not understanding the question, run, in the interest of time, can I just quickly unmute you because I just want to move on quickly, but we'll take the question quickly and move on. Go ahead, run, yeah, good morning, you're up, sorry. What I understand you explained in downsampling, it will exactly reduce, of sampling, it will take just like sometimes maybe not exactly develop, but here it is happening, so maybe it is frequently occurred or it will be sometimes like this or sometimes it will be the size will be different. See, you're doing the exact opposite process of the, let me come back to this here very quickly. So, you're doing the exact process of a convolutional neural network in reverse, isn't it? So, when you're feeding 28 by 28, the first CNN layer, what does it actually do? It converts it into 14 by 14? From 14 by 14, it converts it down to 7 by 7, right? And so on, let's say goes down to, let's say even smaller, and then finally, what do you do? You take the entire thing and you stack it up into single dimension. So, let's say 7 by 7 will become a eventual of 49 single dimension, right? Now, you're doing the reverse here, you're taking the input as a 49, and you're converting it into 7 by 7, then 14 by 14, and then 28 by 28. It's a very fast network. Yeah, yeah. It makes sense? Yes, yes. I missed it a very fast. In your generate really. Okay. Yeah. Thank you. All right. Okay. Yes, absolutely. So, when you are actually doing that, that is exactly what is going to happen, right? You're creating the patch over here, which is, you're just copy-tacing this patch here. So easily, it will detect it, isn't it? It's a very simple process. It will find out that you're actually providing it with the fake images, isn't it? That is why your filter plays a very important role. Now, what the filter will do is, then the filter will actually go through this entire set of layers over here. See, remember, this is the filter, right? The filter is what I'm putting over here. The filter is going through the entire image over here. What will it actually do? It will multiply with the values that you have here, 0.01 minus 0.01 0.02 0.53. It will multiply. There will be a dot product. That's why you have parameters here, right? It will learn basically. Are we following or not? Yes. So, then what will happen is that this will actually generate in the interest of space. What it will generate is it will create a new image that will be created over here. So, after the convolutional 2D process, it will actually generate a new image, 128 images of the shape 14 by 14. But when you see the pixel values here, the pixel values will not be the same pixel values that you saw over here, right? Let me just draw it correctly once again. So, the 14 by 14 that you actually generate here, this 14 by 14 that you see here. The 14 by 14 will not have the same values as the 14 by 14 here. These 14 by 14 will have a very different set of values that will be actually generated in this particular case. So, when you see this here, when you see these 14 by 14 and the values in each of these cells, they will all change. Why will they all change? Very simple reason because the filter multiplies with the pixel values here, not pixel, but the random noise that you generated, they multiply that. And they recreate a new set of pixel values in this case. Is this part clear to everybody now? Yes. Are we able to follow? Everyone, any questions on this? Because if you understand the generator, you have understood everything else. This discriminator is very simple. Nothing very difficult as such. Now, again, I have a leaky relu here. So, I will pass it again through a leaky relu because some small pixel negative pixel values it will let it go. And the remaining part of it very large pixel negative pixel values it will pass it as zero in this particular case. So, that is where what you end up having is you pass it through a leaky relu. Now, look at it. One more upsampling. Can you notice this? One more upsampling. So, 14 by 14 becomes 28 by 28 pixels. Yes or no? Is this part clear? So, that is where if you notice here, this is the first upsampling block which we did earlier. Now, in the second upsampling block, we are making a bigger image of the same thing. But remember, in the upsampling, it will only create a copy of the same values, right? Same upsampling process, right? It will do the same process except that remember here, it was just generating a random set of values. That random values are converted into something more meaningful than by the convolutional 2D in this particular case. So, the convolutional 2D, the filter will multiply with those values that has come from the previous layer and create a new set of images in this case. Yes or no? Are we able to follow it now? Are we able to understand? Everybody, anybody not followed? Please let me know, just give me one quick minute, just a quick minute, sorry. Okay. So, let me explain it very quickly to all of you, just in the interest that, you know, it's 5 by 5, that's here, you can see, it's a 5 by 5 filter. So, let me just quickly explain to you all once again what we are actually doing. So, remember, we have basically a set of about, here we go. So, we have about 6 to 7 2, right? We have an individual 6 to 7 2 dimension, right? So, I made a confusion on a mistake earlier also. So, let's assume that you have 6 to 7 2, you pass it through a dense layer, where the, sorry, again, I'm making the same mistake, 128. So, you have a 128 input vector, you pass it through a dense layer and you convert it into 128 into whatever the number of values, it converts it into a single dimension of 6 to 7 2, 2, single dimensional vector of 6 to 7 2 and these values inside are completely randomly generated, right? So, all are random generated values, right? So, this is random generator. So, random numbers are generated here in this case and after that what you are doing is, you are converting it into a dimensional shape of 6 to 7 2, applying a reeky relu, which means some amount of negative values will be allowed for. So, you are looking at not a perfect relu, it will look like this, it will be like this, something like this, right? So, it will apply a leaky relu. So, which means some amount of negative values will be passed, the rest will be all converted into zeros in this particular case. After leaky relu, what you do is you reshape it into 7 by 7 of 128 pixels, right? Of 128 depth basically. So, 7 by 7 and 128 images you are actually generating, that is what you are actually doing. Now, this 7 by 7 has some pixel values, let us say 1, 2, 3 and 4, let us say some values are there. So, what I will do is I will convert them into, I will pass them into an upsampling, right? An upsampling what it will do is it will convert 7 by 7 into 14 by 14. So, which will end up converting it into something like this, right? So, think of it as 4 by 4, 1, 2, 3, 4, 2, 3, 4. So, you will end up having 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, 1, 2, 3, 4, okay? But that does not make sense because you are just copy-pasting the same patch again and again. So, the 7 by 7 becomes 14 by 14, which is very good, but you need to actually be able to generate purposeful values, meaning values, right? So, what do you do is you apply a convolutional 2D on top of this image with has a shape of 5 by 5, right? So, it is a filter that you are actually applying. So, the filter will be 5 by 5, 1, 28 that you are actually passing. And what do you do? You multiply the filter across this entire images that you have here, this, not image basically, this whatever representation inputs that you got, you multiply with that and you generate again a 14 by 14 image, but just that this time your values are not random values anymore, not 1, 2, 3, 4, 5, but rather you have some sort of a meaning attached to it in this case, right? So, now after the multiplication of all the values that you have here, I need to reconnect otherwise it will go off, yeah. So, what it will end up doing is some values will be there. So, it will not be 1, 2, 3, 4 anymore it will be some 0.01, 0.2, 0.3, 0.4, 2.5, 3.5, 4.5, something like that, right? So, you get the drift of what the values will be. Is this part clear to everybody? Yes. The same will go through again another upsampling layer, right? I am just calling it as US, so that you know it's an upsampling. So, this 14 by 14 will become now even larger, which is 28 by 28 and again it will be a copy of the values, right? So, I don't know how many it will be, but I am just creating some larger space. So, it will now become larger, which is 28 by 28. And finally, you have again a convolutional 2D, which will now generate a new set of values, actually, right? So, you can add as many blocks as you want to, there is no limits to it, the more you add blocks, the more better it will be able to recreate, but remember it will take a lot of time also to process. So, hence you have to choose appropriately in this particular case. Okay, are we clear to everyone? So, you can see here finally, we have the convolutional 2D, the only differences can you notice this very, very important. I hope everyone can see, we don't have a classification layer here, are you seeing this? No classification, we end with a convolutional 2D, which generates an image as an output. Can you see that everyone? Everybody, can you see this? No classification, there is no classification layer here at all. So, this is just a convolutional 2D, is everybody clear with the generator? So, you can see at every sampling convolutional 2D, there are parameters that are learning the weights, they are learning what exactly or how they have to update basically. Is everybody clear with the generator? Yes or no? Any questions? We are out of time, so I want you all to be a little quicker so that we can move through, at least we will finish this problem and then I will break for the day so that you guys can take rest. So, we will be clear in this case. Yes, we will get an output of 28 by 28 pixels, that is right Russian. Are we all clear? Yes, move on to the next part. So, you can actually generate right now some random images, you can actually see no single image, not 128 images, single image, one single image that we have generated after one. You can actually see generator dot predict, np dot random. So, we are creating four random images, 128 by 128 comma 1. So, 128 is basically the input shape, it will convert it into whatever shape it needs to, it will do it through the layers. You can plot that also, can you see this is noise? Can you see this noise? Can everybody see the noise? Random values, right? Does it make sense? Does it look like a bag? Does it look like a trouser or any accessory? Why it doesn't look like that because we are not trained the model yet, okay? Kind of like a painting, yeah. So, it has not been trained as yet, right? So, that's why it is it is just generating noise, it doesn't make any sense at this point in time. So, if you notice here, now this is where going back to the question, if I subtract the pixel values here with the pixel values here, will there be a difference in the pixel values? Most definitely, isn't it? There will be a huge difference actually if you ask me and that will clearly indicate that this is a fake image, isn't it? Now, if I pass this image to the discriminator, will it not be smart enough to discriminate and say this is a fake image? Answer is yes, absolutely, this is very simple for it to actually figure out that it's a fake image in this particular case. So, let's now build the discriminator. Have you all understood the generator now? Everybody's understood the generator? Any questions on the generator? Let's maybe take a quick two minutes, go through the generator layers and other 15 minutes and we'll end the session. So, don't worry, I'm just taking an additional 15 because tomorrow we'll only focus on diffusion and the other Gen AI concepts. So, just go through the codes once again and just make sure you're clear. With the concept, if any questions ask, then we'll move on to the discriminator. So, I'll come to that a little later maybe, yes, but yeah, I'll come to it later. That is right, yes, yes, absolutely Russian. Only then it can reshape it that way, otherwise it will become difficult for it to reshape in that part. So, always go one-fifth of your originals. So, if it's a 250 by 250, divided by 4, you'll find out. If it is 1000, then divided by 4, you will know exactly what you need to start off with. If it is 1500, then divided by 4, again, you'll be able to figure it out. So, find out what your real images are and divided by 4. That's the easiest way of your start point. Yes, or 25% of your, yeah, one-fourth, correct. Clear. Shall we move on to the discriminator now? Everybody, shall we move on to the discriminator? Any questions on the generator? I hope you're all clear. All right, very good. Okay, so the discriminator, I'm not going into it very detail. You know the reason why? Because it's a simple CNN model. Okay, it's a simple CNN model that you're seeing here. It's the exact reverse of what you have seen in the generator. So, if your generator had to recreate an image, the discriminator has to only downsample the image and classify it as simple as that, nothing great about it. It's that much only, right? So, if you look at it, we have the entire convolutional layers, we have the you know, convolutional 2D layers, leaky, and all of those layers, we have added. But primarily, if you look at it, what we are doing? Input shape is 28 by 28. Then after that, we are going on downsampling it, right? We are making it into a 64, then 128, 256, and then finally flattening it up into a single dimension. And then most important, can you see this layer here? What are we doing exactly here? We are classifying it. Can you see there is a sigmoid activation function? Did you see that sigmoid in the generator? The answer is no, you don't have a sigmoid in the generator. Only in the discriminator you have. Why? Because you want the probability that whether it's a fake image or a real image, that is what it needs to actually do. Now, it's not a multi-class, it's a binary classification, not a multi-class. So, if it was multi-class, then the dense layer should have been an output of 3 or 4 or whatever. Here it's only 2 class, 0 or 1. 0 is for real images and 1 is for fake images. So, it has to detect the fake image first. Is this clear to everybody? Discriminator? Yes, can you see? Go through the layers. You can see, 28 by 28 without padding, converts the convolutional 2D applies 24 by 24 without padding becomes 20 by 20. Without padding gets to 16 by 16, then 12 to 12. And then finally, it is converted into a single dimension of 36,364, 864, which it passes through a dense layer for classification. Classifying it as a fake or a real image. You can actually try it out as well. So, you can see, we are generating a random image, generator.predict, random values. We are generating 128 pixels, 4 images, we are generating. And what we will do is, we will pass it through the generator.predict. It has generated an image here. So, this is your image basically, by the way. Just look at it. It has randomly generated. Can you see these values here? These are randomly generated values. So, can we all see your image, randomly generated values? Everybody? Now, let's pass this to the model. Exactly. That is where the next part comes in. Let's pass this through the model basically. This randomly generated pixel values. Let's pass it through the model. Let's see what it can, what it comes back with. So, if you see discriminator.predict, when you pass the image as an input, what does it generate? Can you see the probabilities that it has generated? Exactly 50%, which means it doesn't know whether it's a fake or a real image. It's exactly at 50% right now. Why is it not able to predict that it's a fake image or a real image right now? The discriminator. Any guesses? Why is it not able to predict here? Exactly 50, right? It's not confident basically. Exactly. As Mukti Khan has said, it's not it's not trained yet. It's still it doesn't know what is a fake image, what is a real image. We have to start sending those batches in right? Only then will it learn. Isn't it? So, that is where the next part actually comes in right? We need to get to the next part of the model. Is this part clear to everybody? The generator and the discriminator layer? Is this clear to everyone? Yes or no? Any questions? Please ask. The loss after as of now there is no loss Russian because as of now we have not passed the real image at all right. Only when I pass the real image will it actually have losses coming into play. Okay. So, we'll move on to the next part now. We have to start training the model only then it will actually work. So, that is where what we are doing is we are bringing in the optimizer add-in and then we are bringing binary cross entropy. Why is it a binary cross entropy? Any guesses? Why are we using a binary cross entropy here? Because it is a two class problem right? It is only fake and real so you do not need to have multi-class kind of a situation. So, that is where we import the binary cross entropy here. And remember it is a functional model why? Because you have two sequential layers. One is the discriminator sequential layer and the other is the generator sequential layers. Both these models have to come together like how your encoder decoder had happened in the case of your earlier cases. Like that is how you have to bring together. So, that is why you have the model function here that you have created which is what we call as the functional API. Everyone remembers this. So, both the sequential layers will be brought together in the functional layers together right? So, you have two layers now. You have the generator layers and you have the discriminator layers marrying them both is the job of this functional API which is what this model functional API will do. So, first is we will create G opt. G opt is nothing but the G optimizer which is add-in and D opt is the discriminator optimizer which is add-in again both have the same learning rate. G loss is the loss for the generator and D loss is the loss for the discriminator. Both of them are binary cross entropy at this point in time. Is this part clear to everyone? Yes. Yes or no? Too tired everybody? Everyone feeling tired? I do not want to force it also. It is indeed too glasses. Two different losses because both losses are different right Rajesh because you have two different objective functions. One is trying to maximize the error the other is trying to minimize the error. So, you cannot have a same loss function for both of them because then they will not be adversarial. They will have the same objective function. So, for the generator, the objective function is going to be different for the discriminator the objective function is going to be different. So, that is why we need to have two different loss functions here. Correct. Real and fake images are the input to the discriminator. As of now we have not fed the input Russian that part comes in a little later. So, that part comes in only over here. As of now we have not yet put in the input. So, what is our capacity? Can we go through the next line of code or do you guys want to break as of now because anyways we have to cover this but will you guys be able to go through it or we will train with both both of them we will train with. But then tomorrow will be a longer session then because tomorrow I know you guys have asked for quite a lot of things. So, that is why it will be a little longer than in that case it will become 1130 odd in that case. So, will you guys be prepared for tomorrow for a longer session is that okay? Okay. So, tomorrow I am not doing a recap in the interest of time I will just quickly jump into the function. So, please make sure before you come back for the tomorrow session just finish off all these earlier parts of the code just read through them. We are not going through go through them because we want to just save our time and focus on other important things as well. So, that is why we will directly get into the model function right we will get to the compile functions sorry that is where we will get to. Okay. Is that all right? Something we can we all can do? I would have continued more but I know that you guys are tired as well so I do not want you to overburden over strain yourself as well. Okay. So, I will pause here just we will come back tomorrow we will first finish this GAN part and then we will get into the other generative AI concepts that you guys asked for we will do that and then with that we will wrap up a genie part and then move on to the next module basically. Okay. I hope that is okay with everybody. All right. Thanks guys. I will let you all take rest. Have a nice day relax.