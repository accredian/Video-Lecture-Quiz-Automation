 participants. Please confirm that you're able to see the screen and also the audio is clear if you can put a quick guess on the chat. Very good. I'm happy to see that we have already 11 people joining in. Good. All well, everyone. Anybody having good plans for the Sunday tomorrow? I believe there's no session tomorrow. So I'm sure many of you must have been planning a long sleep tomorrow. Won't have to get up early tomorrow. Good. Good to see all and yes, shall we all get started? Everybody. I hope the energy is good. I hope everyone is is mentally all set for the hands-on use case that we're going to look at, which is the segmentation use case. I'll talk about the use case in some time from now, but let's do a quick recap of what we did in the previous session. And then what are we going to do? I'll explain more in details as we go through it. It's a very interesting use case. And many of you did reach out also, how do I custom-train or create custom object detections, etc. So I'll talk about that. So this entire hands-on use case that we are going to look at is primarily focused at how you can leverage this entire code that we are about to go through in order to be able to do your own custom trainings, etc. Coming back to the concept of segmentation, unlike the entire concept of object detection. Remember, object detection is a bounding box across the object, but here there is a different objective that we are actually trying to solve where we are trying to bring together similar pixels. So if you remember, object detection creates a bounding box right across the objects. Now, of course, within the bounding box, other regions also come in. Like, for example, this green part over here, which is the pasture also comes into play. Sagnmentation, however, is a little different. Like the concept of segmentation you've learned in your sessions and machine learning where similar pixels are grouped or clustered together. In the same way segmentation over here is also about grouping similar pixels together and clustering them basically, right? Or for a lack of better word, I'll call it as clustering, but not necessarily clustering. Now, because of this, what happens is it is much more tighter, right? Sagnmentation compared to object detection, you can see here that when it is actually looking at the at the goats over here, it is or sheep over here. It is not actually considering the background, the green background at all. It's considering the green background as a different entity altogether. And that's where you have two paths to segmentation. One is semantic segmentation where it groups similar objects together. And in instant segmentation, it will group the similar objects together, but then also discriminate between the same group itself, right? Like different instances of the same object will also be discriminated in this case. Now, what is better semantic or instance depends, right? What you're trying to solve now, if you're very, very particular about the number of instances of a sheep, then of course, instant segmentation is more important. But if you're just looking at a boundary to be created for like understanding different objects, then semantic is more than enough. Either way, both will take about similar amount of resources to do the segmentation process as a whole. So we looked at a few examples of object detection, semantic segmentation and instance segmentation as well. And we talked about how, you know, how exactly does it actually look at the pixels, the similarities of the pixels in order to be able to delineate objects that are similar in nature? Then we talked about how we are actually going to do this also. So in this particular case, we are looking at YOLO. YOLO is going to be our go to model. As I said, YOLO is basically the state of the art algorithm now. At this point in time, YOLO is unbeatable. Of course, you have versions of YOLO V11, which is even better than YOLO V8. But then again, YOLO V8 is more than enough for our use case. We don't need that big of model also. YOLO V8, we used for object detection as you saw in the previous session, where we detected multiple objects of interest. And those objects of interest, as you would recall, could be, for example, detecting a car, a bus, or a truck, etc. Now, the point here is, let us assume that there is an object that you want to detect, which is not available in the YOLO object detection technique. Now, how will you do that? That is where what we are going to do is take an approach of using segmentation to do an object detection for us. So this is going to be a very interesting use case. You are going to custom-trained the model to be able to detect an object of interest that you wanted to detect in this case. So that is where this is going to be a use case that will focus on that. And how we do that, etc. We will talk about in the hands-on example. Now, YOLO is amazing. Great model does wonderfully well. We have already seen it in the object detection use case. It is a single-shot model. It doesn't have to do multiple iterations. Pretty large in terms of its size. And you have different models also from small to extra large depending on how much inference you can run, what kind of hardware you have. You can appropriately choose your right model in this particular case. So it gives you a lot of flexibility. Unlike any other object detection techniques where you don't have that level of flexibility that you get from here. So that's where we are looking at YOLO-V8 to be able to work on our dataset and do a segmentation as well in this case. So we are going to see again maybe a quick recap of what YOLO-V8 is. I will do a quick recap of the architecture when we get there maybe. And very quickly we will get started onto the segmentation aspect as well. Another important part which I think we spoke about briefly is the equivalent to this is something called as SAM. Facebook has something called as segment anything model. If you are time I'll probably share the code. It's not very difficult. It's available in hugging phase also. So you don't have to kind of do anything separately. As you would have already seen with YOLO-V8 when we did object detection the amount of coding is not that much. It's actually very simple. In fact it's just one line of code. The rest of the time actually goes in creating the helper functions and all of that. That's the maximum actually that it takes. But actual model per se is just about probably one two at the max will be three lines of code. The same with segment anything model as well that you don't have to write extensive amount of code to actually do any of it. You can do it very quickly. Now between segment anything model and between the other model which is let's say YOLO-V8 segmentation what would you prefer? My answer simple is go with YOLO-V8. Because you can see very clearly that in terms of performance the YOLO models are only benchmarked against YOLO. You can't even benchmark them against any other model. So their performance is so good or so nice that there is actually no comparison with any other model. And generally seen we observe that YOLO has performed outperformed any other model that we have come across. So that's why I would suggest if you have a confusion don't worry about it much just go ahead with segment anything model itself. So where is the application of it? It comes in autonomous driving. Today I'll talk about why exactly autonomous driving. We think object detection is what it uses but I'll come to a concept of what we call as FSD and how FSD uses segmentation to detect the road and then continue to move on that. So that's where autonomous driving or medical images like for example creating a 3D segment of let's say somebody's oral setup basically right to to understand how they're tooth and all of that is as well. Then we talked about how a lot of it is used in defense establishments typically in border areas in areas that are disputed to understand if there is any kind of growth or development that's happening that could affect let's say for example a particular country sovereignty or any of those things. So many satellite images are used for segmentation purposes. People try to see if there is some sort of unexpected behavior being seen or constructions being seen in a particular site and the moment they see that then of course they have people who start intervening and taking actions likewise security cameras trying to detect if you know there are people in a particular location in a particular place etc all of that that also you can use the object detection sorry the segmentation method. So those are few of the things we talked about in our last session and to review look at the hands-on use case as well related to it. What's the major difference between Sam and YoloV8 architecturally more or less the same dungeon not much of a difference but the YoloV8 being trained on multiple different data like be data related to image net it's also been trained on cocoa data set Pascal view see and so many other things it's generally considered to be much more efficient and much more accurate actually. Architecturally they are the same no difference as such segment anything model is from a pre-trained large computer vision model that meta actually built but they didn't they didn't actually spend too much or they didn't invest too much into it because of the fact that Yolo was anyway state of the art so they didn't want to proceed with that rather they started to focus their resources on more of the all parallel inside of it right like developing lama and those models so they move that side basically does Google Earth use segmentation absolutely yes Ranjini Google Earth does use that the use it for that purpose yep. Right so that was a quick recap I hope all are good we are all on the same page any questions doubts clarifications before we move on to quickly in terms of the model segmentation model yes please I'll share this notebook I forgot I'm sorry yeah give me a minute are we all good yes the batch is already in Republic de Mou I can sense that now already isn't it are it we all yeah we're looking forward to a longer sleep tomorrow morning looks like some of us are already asleep right okay I won't make fun of you I'm sorry I shouldn't do that I know that a lot of you have busy weekend weekdays so it's totally understandable but yeah object selection how can we use this object selection we didn't have object selection as such object detection is what is called as object selection in certain cases it's object detection only mostly we have umbrella we don't have anything as object selection per se maybe people call it as selection but generally it's object detection that we have okay good so very quickly I know some of you had mailed me asking questions on you low V8 so I thought we'll take a quick 10 minutes and I'll explain to you the architecture because many of you had questions on certain levels of the architecture etc right so I will give you a bit of a brief understanding of it I can give you more details if you want I can share with you the the paper also it's available online I'll share it with you in case you want to do more deep dive and research into it etc right so I'll explain to you what this architecture actually consists of there are three things that are there in any yolo basically or rather all yolo's until v5 version 5 all the way until version 11 11 is what got launched in 2024 so all the yolo's from v5 to v11 have what we call as an anchor free detection okay what do I mean by anchor free detection is that if you remember in yolo when we learned the concept it builds an anchor to understand an object being distributed across multiple grids right so which grid is responsible you basically take a combination of anchor plus grid in order to predict now they removed that entire concept of anchoring itself they said no we don't need anchors anymore we are going to actually bring a change in our entire layering process until yolo v5 the entire model was built on the concept of lanet we call it as lanet or google net Alex net whatever you'd like to call it that's that's a really large pre-trained model on the basis of which they actually built the yolo but yolo v5 onwards they changed it so the yolo v5 has three things over here okay so it has one we call as the backbone okay that's one block then you have a block called as neck okay where is the neck on here it is the neck is here somewhere bottle neck there is a neck on so this is a backbone that we have okay so the neck is there I'll show you that as well and then there is the head which is another important element basically right so backbone neck and head three things that it actually has so just like a human body basically right now what is the backbone actually have it is the most important layers that will help you do the feature extraction so if you look at it they've given a brief idea of what the backbone actually consists of so the backbone if you notice has convolutional layer it says k is equal to 3 s is equal to 2 and p is equal to 1 so what it actually means is k is basically the number of kernels three kernels s is equal to the stride which is you know it takes two strides and p is equal to 1 is if padding is applied or not if one means padding is applied zero means that no padding is applied basically so if you notice here it's a simple convolutional layer that takes an image of the shapes 640 by 640 into 3 and over a period of time you can see it actually gets it down to 160 comma 160 comma 128 so 128 filtered images is what it actually generates now after this backbone you will see within this backbone there is something called as a C2F okay now what is this concept of C2F generally right it's a it's a very specific kind of how do we say something very specific to the convolutional neural network just give me one second here it is C2F so you can see the architecture of C2F over here so it has basically a convolutional layer then it has something called as a split over here okay so this is exactly the C2F that you see over here this is what it actually got so C2F basically is I forgot the full form for it let me just check what's the full form if we get the full form of it full form I forget what it is exactly I don't remember it very well but typically what it has is as you can see here it has a k is equal to 1 s is equal to 1 and p is equal to 0 so same thing now one different thing that it actually does is it passes the input image through a split layer now what the split layer does is take the same image okay and passes it skips it basically right I am sure all of us remember this concept of skip connection it skips the connection over here and then the same image is actually passed through a set of bottle neck layers now what is the bottle neck I'll come to that a little later so here you have the bottle neck but it passes it through a set of bottle neck layers and then what it does interestingly at this point the splitted image that it takes so one image going in getting split into two parts the splitted image comes from here and it concatenates with the processed image in this particular case right so that is where it is using skip connection over here everyone remembers this concept of skip connection that we did in our you know when we did transformers that is exactly what it is applying over here basically so that is what the C2F layers do over here now it is notice if you look at it over here it has two C2F layers right it has two C2F layers and then after that again convolutional layers etc now within the C2F if you notice it has a very specific layer called as a bottle neck and in that bottle neck it has a different set of convolutional layers if you notice there is one which is called as true and one which is called as false so there are two types of bottle neck layers that it has and the first bottle neck layer is if it applies a skip connection within the bottle neck it puts it as shortcut is equal to true so you can see here it has a skip connection here so where it takes the image from the previous layer itself again it will apply what we call as a skip connection everyone remembers the concept of skip connection I hope we have forgotten this concept which we discussed in the in the transformer session do we remember it yes no what is it actually help you to do what is the advantage of using skip connection you anybody remembers what will it help you achieve if you actually use the skip connection yes it doesn't reduce overfitting it actually not really save time as well okay I think some of us have forgotten it skip connection allows you to you know you will not have basically the vanishing gradient problem basically right so remember when it is actually going through all of these layers there is a possibility that you will end up multiplying and taking all the kernels etc eventually you know the values will become too tiny or too small so you don't want that right you don't want values to become so small that you can't even apply any gradients on them basically so that's why what it does is it basically takes a skip connection to avoid so the same input that is being passed out from here is getting added to the input that is passing through the layers basically so one is the raw image is added to the processed image just to avoid any kind of vanishing gradient problem that we have guys recall this if not I would recommend you go back to our session you know where we have basically the discussion on skip connections and transformers so that was I think the second last session or the third last session in our NLP two session basically Ankita's question might sound funny but how come bottle neck has zero and one as the outcomes scenarios this one you're referring to here it doesn't have an outcome of one Ankita there's only two possibilities one is it's either true or it's a false basically so true basically it means that it applies skip connection false means it up it doesn't apply skip connection I'm just trying to see where the zero or one is you're referring to this layer here this entire part over here yeah true basically indicates that if you notice here true will show this skip connection part here if you notice here right it has a line here that is passing through whereas when you give it as false there is no skip connection that is getting applied in this case so it's not a it doesn't generate an output of zero or one that's not what it actually does it generates an image only over here in this case right is everybody clear with the bottle neck clear that you see here yep are we all clear yes or no guys people are very quiet today I think they are no more to learn you know it seems like okay now this convolution that you see over here right this convolutional layer that you're noticing over here this convolutional layer if you look at it it is actually just put as convolution but remember that convolution has actually been broken down so it has over here a convolutional 2D okay it has a convolutional 2D then it has a batch normalization 2D and then it applies an activation function called a silo silo is nothing but sigmoid linear unit so if you can read about it online also I'm not going to it much detail but again silo is much more better than relu because silo allows for non-linearity is a little more basically compared to your relu and in that case relu is a little more rigid so that's where we have silo over here so this is whenever you see the convolution over here this over here or this here always look at this box you will know exactly what the convolutional layer actually consists of now apart from this there is the SPPF layer right SPPF is I keep forgetting again the full form for SPPF layer but I think it's something called a spatial features spatial pyramid features something it is called as okay the entire full form I don't recall as of now on the top of my mind now in between if you notice it has a certain it is not showing up over here but it has a certain number of what we call as an SPPF layers which consists of its own convolutional layer max pool 2D layers and a few three max pool 2D layers and a concatenation and a convolutional layer so if you notice over here it applies skip connection after every max pool 2D layer in order to avoid again the vanishing gradient problem this is one of the reasons why these models are able to do really really fast they are able to predict or run really really quickly here in this case right spatial pyramid pooling something is what the full form is I don't recall the exact full form but primarily it's a it's a pooling layer that is applied in order to down sample the image basically that is what it actually does here now apart from that there's another interesting layer that it actually comes back with so if you notice this is where the entire convolutional layer c2f layers comes in I hope everyone can see this this is the SPPF layer this is where it actually comes in now look over here after it does this it actually passes it to another two layers over here which is called as an up sample layer can you see this everybody can see this now up sample is exactly the opposite of convolutional 2D so convolutional 2D layers or pooling layers is called as a down sampling layer which means they convert the image as you can notice original image that came in was if you notice here 640 by 640 into 3 it down sampled and you can see the final output that this SPPF layer actually generated is somewhere close to 20 by 20 into 512 can you notice this this is what it has down sampled it into but interestingly you will notice that what it is actually doing is from here on it is actually passing it to an up sample layer that's why if you if you look very carefully here it is here on but sorry my bad I went there but it's here so it is actually doing a 20 by 20 into 512 up sampling it into 40 by 40 it is increasing the shape of the image and likewise all the way until the c2f where it is actually increasing again by 80 by 80 into 512 basically I'll explain why that does it now the exact up sampling process I will explain to you a little later because in our next part after the break when we do a generative adversarial networks I'll explain that concept to you it'll be a little confusing right now to get it but it is called as in reverse of a convolutional 2D process to Rahul and Hemund's question on what up sample is or how up sampling actually works it's a reverse process it will actually not try to use the filter to generate the filtered images it will use the filter to generate images in this particular case right so that is what it does in this case so okay I'll come to that don't worry we we get there one by one just a minute for it right so after it does an up sampling if you notice very carefully the next part is where it actually starts to set into again a set of convolutional etc all those layers and finally into the detection layer so if you notice the up sampled output one goes to the detection layer and then there is another output that goes to a set of convolutional layers basically so if you notice there are three detections that it has actually set up over here detect one detect two and detect three now if you look at the detection every detection has two outputs that it actually generates over here one is the bounding box and the other is the class over here so if you notice this is what it is actually saying the bounding box and the class so the detect is a final layer that actually is used for the purpose of detecting the objects that are of interest and classification now why does it have three over here simple reason is because if you notice it has three different levels it takes so this is where it takes one the up sample damage itself 80 by 80 into 256 then it has the down sample a medium size if you notice 40 by 40 into 512 and finally it has a 20 by 20 into 512 as well so you see why it does up sampling is remember the point is sometimes it's quite possible that in the down sampling process there could be a feature loss that happens right it's quite possible because it might take its random feature extraction process right so it's quite natural that some feature might be lost now when you are concatenating if you notice here you have the down sample image and you are concatenating the up sample damage what you're actually doing is you're bringing back any feature that is lost in this process if it is lost it is trying to bring it back in this case so that is where it is looking at three different levels so even with the down sample damage let's assume that it actually does a poor detection here it has additionally two additional layers to compensate for that in this case so that is where it is able to ensure in a way kind of like a non-sample model right it is able to ensure that even though if the input is not enough for a detection very well at a down sample damage it has two additional layers that it can use for detecting in this particular case so that's where this entire concept of backbone head and neck these three layers help it to do a really good bounding box prediction and also the classification in this case now if you look at it over here it says some values 0.330.671 etc what it is actually telling you is depth depth is basically the number of layers with this not able to recall what the width actually stands for but depth is the number of layers that we call it out D and with this W is depth is the number of width is I think the scaling or something it applies anyways no problem and then there is the number of channels the maximum number of channels basically what you need to look out for is this is like a proportion in a nano model it uses only 33 percent of the depth whereas in excel model it uses 100 percent of the depth in the case of the width the nano model uses only 25 percent of the entire layers that you see here whereas in the excel it goes 1.25 times which means it has additional width that it adds basically right additional scales or channels that it actually adds over here likewise with the ratio also in this case the idea is it is telling you you know the proportion of the model right in terms of how big or how small it is compared to each other basically so when you look at it this is actually the entire model they have represented this is the excel model that you're looking at in this case it is not broken down if you double click on each of these layers you will further see that these layers break down into further smaller layers that's why if you notice here it is given like a pyramid structure in this case because they have further more layers that go inside of it but the ultimate objective throughout this entire layers now this is where you know expertise comes into play right people who have built it written it they have done so much amount of research that they know exactly why they have come up with the layer so if you ask me why exactly or who exactly decided these layers that is where research is right so we can't really or I can't really comment because that is something that I'm also not I'm not a person who's gone into research of all of these things so that's where we have to accept the fact that people who built this entire set of layers have done enough amount of you can say you know experimentation and have done enough number of let's say for example study on this entire process to be able to come up with a model like this right so this is available I'll share the paper also with you I don't have it right now handy I have it in my office laptop I'll share it with you guys so that you guys can also read through it in terms of how it actually works so the detector layer will change when it comes to object detection when it comes to segmentation it's a different detector layer so what they do is just like how in the transfer learning we change the final layer right the softmax layer or the dense layers how we change it in the same way they do that over here as well they change the detector layers depending on you know the the problem that they're trying to solve if it's a segmentation they have a different detection layer if it's an object detection a different detection layer if it's a classification a very different layer etc we'll say quick example of how it actually works so interestingly we always think that you know whenever we see autonomous vehicles we see that we think that it is object detection but in reality it's not object detection so if you look at fsd which is fully only self driving car right if you look at their look at their thing over here you'll notice very clearly it's actually trying to do a segmentation can you all see it it is trying to segment the road from the I don't know where is a good one maybe this is a good image to actually look at can you all see this it is trying to segment basically the road can you notice it's only that the UI is a little more slick when it comes to Tesla they have a better UI but if you notice over here it is trying to segment the entire locations right it's trying to break down there is that image one there's a good image that is there here you can see one more example right so it is actually trying to segment the road from the path or footpath or whatever it is it is trying to actually do that in reality it is when you look at an fsd it is not actually trying to do an object detection it's not trying to detect vehicles or cars or anything all it is trying to do is segment the region the boundaries basically it is trying to say this is the footpath this is a vehicle that is occupying this area or region so you don't notice a bounding box that it actually draws it doesn't do that so that's why in reality we don't actually look at when we talk about object detection it's useful but not in autonomous or fully self driven vehicles primarily it is segmentation that is applied and what algorithm Tesla uses nobody knows they've not open-source state of course they have kept it closed but either way I mean it could be something similar to a yo-lo itself which they would have customized it for their own purpose right they would have done something to it to ensure that it is able to detect the objects more than detection segment the regions basically so that's what we will see how the segmentation actually works in this case right so here's where the architecture is here the PDF is I'm sorry I was not I'll show it this is where it is actually so I had actually picked it up from here only this is where I'd actually picked it up so you can see each and every layer they actually talk about it in detail this is where I had picked up the entire layering as well so you can see the entire breakup of it like one by one by one they explain every single layer even more you know break it down even further so this is what I actually picked up we at Accredion also picked up this one from the from the paper now I have read the paper but the only problem is I've read it like one or two years back so even I'm like kind of I need to recall all the things that are there in this paper but if you're really interested you know you can read it up over here if you are the kind of person who wants to do research in all of those things I will yes it's a research paper it's the paper that documents how Yolo V8 works actually yeah so in India that's why we don't have SSD right it's impossible to bring in full self-driving unless and until there is a change in the entire driving people follow itself unless until that happens we just have to forget all these kind of things anyway I'm not going to comment on that it's not really the best of things but I'll set to use Yolo V8 for our use case I'll explain to you what we are going to do okay so what am I going to do actually is a quick use case that we are actually going to solve now we are not looking to draw segmentation here to just you know detect boundaries and all of that we are going to use segmentation to train custom train our let's say a class right so this is what I remember from an example people were doing in previously and I mentioned it also in our batch that one of the previous batch people actually used a Yolo V8 to detect you know if people in the shop floor in a factory are actually wearing a helmet or not so of course helmet is there but there are certain cases like for example walking on the pathway etc that you cannot actually use an object detection to do so they have a particular pathway where people have to walk any diversion from that is considered as a as a kind of you know not following the rules etc kind of thing right so that's what they had actually set up now in order to do this what he had done is he had done a segmentation now what we are actually going to do is we are going to have a set of images let's say we will have images that about let's say 10 images we will have right so one to let's say 10 images now I'm taking an example of a duck here although duck is a class in Yolo but still what we are doing is we are going to train the model to detect ducks now okay that's what we are going to do not detect but rather segment ducks right it has to be able to segment the ducks now every image has a image of duck but interestingly the duck is not put into a bounding box sorry one second so the images of the duck for example are not put into a bounding box in this particular case all we have is basically the duck over here over here wherever it is I don't know it is going to be in the entire image now this particular image is that we have we have something called as an annotations okay so we have labels or annotations where we are going to have the bounding boxes as well so we are going to say that for example the duck is over here in this particular whatever right so this is the coordinates at which the duck is located this is the location at which the duck is located this is the location at which the duck is located now how did I get those coordinates that I will explain to you a little later there is a certain number of software that will help you to detect this if you want to do it you can take a few example images and those images what you can actually do is you can annotate them manually okay and since it's you know we ate the the feature extraction layers are so good you don't need to actually give too many instances also a few instances is more than enough in this case so that is where what you need to do is get your images those images manually you annotate and I'll tell you what are those software to manually annotate those images and once you annotate them what you need to do is you will get the annotations in a txt file a text file basically so you'll have two sets of files one is the images itself it'll be whatever format jpeg png whatever it is that is one thing and the other thing is you'll have the annotations which will be you know text format that will be provided to you right and I'll talk about the software a little later what those are but once you have got your images and you've got your annotations you've got to store it in this particular format which means you have to create a folder called as datasets in that you have to create a subfolder called as my datasets and you have to create three different folders okay so there is going to be a train a validation and a test okay now in my case I'm just looking at duds okay but let's say tomorrow you want to classify between duds and alligators then you know likewise the same example you have basically annotated images of let's say duds and alligators also so let's assume that these are the images of duds and these are the images of alligators then again same 10 images and then you have the annotations wherever the alligator is let's say it's over here or here and then what you have is this is the this is basically the bounding boxes for duds and this is the bounding boxes for alligators and you'll have to store it like this so this particular images of the alligators let's say we'll go into one folder let's say class one and let's say this of the duds will go into the next folder which is class two somewhat similar to the object we did this remember image data directory in our first sessions this is exactly how you have to follow it so this is very very important you have to store the data set in this form only so and you have to have three different data sets which means if you have 10 images okay make sure you have three images for training okay about three images for testing and maybe four images for validation or either ways you make sure whichever it is right you want to put four images for training three images for testing and three images for validation but the point is you need to have all three training test and validation all three have to be there okay you have to put your data set in this form because it needs to evaluate the model how well it is doing also and it's a pre-trained model right so it has to accurately work as well so hence it's important that you put it in this format over here there is something called as a mydataset.yml file okay this aml file consists of the configurations what are the configurations it tells you exactly what is the train dataset location right the path of the train dataset the path of the validation dataset the path of the test dataset so all three you need to provide and then you need to say the number of classes now in this case it is three classes one two and three in our example it's going to be one class problem but whatever the number of classes you need to specify that over here as three and most importantly you need to provide the names of the classes also so you just don't give it as 0 1 2 3 you give it as class 1 class 2 and class 3 as well so that it knows exactly you know which classes they belong to also yes both of them in the same folder yes absolutely devotion I'll come to that I'll I'll show you how that that part actually is but is this clear to everyone this folder structure this is unique only to yolo by the way are you all clear yes any questions on this three people saying clear out of an entire batch of 38 people okay good so that is where I you know we are going to you are going to have to set it up in this way for the model to be able to extract and run basically maybe the way they set up Rahul yolo was set up probably had the datasets in this particular structure that's why they would have done it but it's not the same for others so if you do Sam for example segment anything model it'll be different so you can see here they've broken it down data images and within the data part of it to your question I think this was Rahul's question that came in earlier you can see here image and then you have the labels right so within the data so this is one class image and then you have the labels associated with it what is the labels the labels over here is the exact you know bounding I'll show you that it is exactly the the bound coordinates of where the object is located within that image I'll show you that for you guys now you have two objects also no problem that's absolutely fine as long as in your labels you clearly indicate the bounding box for the other object then you it's perfectly fine no problem at all so you just need to clearly indicate that there are two different objects and they have two different coordinates in this case then only give the coordinates for the object of interest for you so if you're only interested in detecting the duck but there's also alligator in your images then don't give the coordinates for alligator give the only ones for duck basically then it'll ignore the alligator in that case right is that clear to all everybody yes or no okay so very quickly what we'll do is we'll go ahead I'll show you the data very quickly so first thing we'll just go ahead and install ultralytics remember ultralytics is where all this yolo models are actually stored so we need that primarily to run the yolo models yolo segmentation model itself in this case and the other important thing is in order to run these models GPU is primarily required so you can't run it on CPU so make sure you have access to GPUs we here we are already using a T4 that should be fine let it go ahead install and then some of the standard libraries that we're importing for visualization of the image and all of those things basically yes I'll share the data in a moment just a few minutes actually and we'll download the data directly itself right so I'll show you guys that okay so once that is done you can see here this is where you just check very quickly the GPU is connected or not very very important you need GPU for this right you cannot run it on a CPU if you run it also it will not will not be very successful it will take a lot of time first of all and the second thing is mostly it will crash because you can see the kind of layers that are there over here right so I think that's clearly indicative of the fact that this is a really large amount of data we cannot really run a simple CPU and get it done now this is where the data is actually located we have it in gitlab.com a gradient this is our gitlab location this is where we have the data so we already have it created I'll show you what the data is just go ahead and download the data from this link right it takes a few minutes because it's a little large data said that we have so just go ahead and download it about two GBs what I think the file size is if I'm not mistaken and then after that followed by it if data is in your laptop then you know it'll be a little difficult to run number because you know if your laptop has GPUs then no problem but if you don't have GPUs it will take really long it might even crash the session in that case so that's why preferable that you run this on a cloud-based system that way it will be faster so just a quick unzip of the files and you will notice I'll show you very quickly this is what the most important part is so let me show you that takes a few seconds to unzip the entire file because it's about two GB of data that is compressed right so when you unzip it easily go to about three four GB at least so you see all of them are PNG files that it is unziping this is images not videos so it's PNG images PNG indicating that it's all images basically yeah it's done everyone the unziping of the file okay now look at my screen I'll just explain to you how the images are right so if you notice here go to data and can you see the data now it has two parts images and then it has labels right you can notice that over here so this is the image here the first image and you can see that we have two parts right training and validation so if you look at the training part here you will notice that this is the image that we have okay so this is an image of the duck okay this is the first image that I'm looking at we have a few images at least about 100 to 100 images we'll have we can see how many there are but this is the first image of the duck okay now pay attention to this part if you notice below that I have something called as labels can you see this so in that labels if I go to the train part of it you will notice that I have a notepad file here can you see it's a TXT file so if I open this text file what you will see is this particular duck that you see here right this particular duck it is actually segmented it'll be segmented so how will it segment is it is actually drawing a kind of not a bounding box remember it will draw something like this right it will create something like this now this entire thing is a not a bounding box it's a coordinate of where exactly the duck is located in order to get this you will notice that you will be able to see the coordinates that it has actually generated can you see this this entire file here now earlier we had bounding boxes so it was very simple isn't it bounding boxes had unique thing which is you had the x and you had the y from there you know you could actually create a box which had the width and the height of the box basically right so if you had the width and height automatically it will create a bounding box like this so that's where it was simple you had coordinates in that case which are only four values right you had only x y then width and height that's it there were only four values that you had earlier but now it is not four values because now it is a yes like a free form basically right you have a entire you can see a drawing that is bringing the entire position or location where the duck is located so in that case what will happen is you can't simply give four coordinates and finish it off in this case it is providing you the entire location of where the duck is in this image can you see that is this part clear to everyone just make sure you don't delete the coordinates but you can see that's why for a single duck it has so many values if you notice right that's why there are so many coordinates positions here likewise I'll show you one other example just so that we are all clear on this concept you will notice there is another image with multiple ducks actually not this one I think there's an image with multiple ducks I'll show you that yeah no this is a pigeon for example sorry I'll ignore this one so it doesn't give pigeon for example we don't want the pigeon to be detected now look at it over here these are toy ducks but nonetheless you can see here 600 17129 600 17129 you can literally go to 600 17129 I have to look at it it's not showing up over here let me take some other example you can see this is also another single duck only we're trying to see multiple duck it will give you a second row also this is also a single duck yeah this is also single duck but can you see just next to the one there is also another two over here can you all see that if there are two instances then what will happen is the second instance will be written over here I'm trying to see the two instance kind of an example two or three or whatever it is multi instance kind of a scenario not able to get but the point is that if there are more than two instances you will end up having another row over here which will actually give you the coordinates for that second instance or multiple instances whatever you have basically right so most of them are single instance that we are looking at but the point is you need to get these two things which is the image and then you get the coordinates as well right the coordinates of where exactly that object is located exactly now that comes to the second part right as Harish has asked how do I get this coordinates basically now this is where there are paid tools and then there are free tools also free tools have some restrictions paid tools have some kind of you know let's say for example free software etc so I'll show you what how you can actually get so these are the tools that you will actually get bounding box coordinates right bounding box coordinates generator this is the manual work that you need to actually do so you can do this here be box finder for example there is a coordinate system there is you can go through it is okay here also see that is something that we used can you see this here how it works so now this is free not completely free it is paid also so there is a certain number of it it will actually do for free I don't have the software right now but if you want to actually do it it's a very simple process it gives you the tools can you see how it is creating this segmentation it just look at the demo so it is you manually actually have to draw this by the way so which means you have to create this kind of a you know using the tool now you don't need to do it for multiple images maybe about 10 or 15 of them is more than enough you don't need to do it for too many so what it does gives is it gives you a set of tools here on the left hand side and it allows you to select the object that you actually want so you can do it for free also you know you have to of course sign up with Google and you get about a few images that you can actually do it for free then after that you have to pay for it now you'll have to do some amount of research online to see if there are many free ones but I haven't come across a lot of completely free annotation services right that some amount of payment you have to do now that depends right how much is it to be paid etc that you have to see I think they give you about 20 dollars or 30 dollars I don't know some some sort of yeah now they've increased it also so they've increased it to about 46 dollars in terms of the cost of doing it earlier I think last year it was about 20 dollars to do the annotations they've increased it 30 dollars I believe I don't remember exactly so it was about that much or was it this one I don't remember but we had actually got about 30 dollars was what they charged originally for doing annotations completely free I can't think of any I searched but I couldn't find any but if you have only 10 or 15 images you can do a free annotation they will do it for you when you finish annotating what they will do is they'll give you both they'll give you the option do you want the labels in a txt format or in a Yamel format or a json format or an XML format whatever format preferably take it as a take it as a notepad format because for the model it's better to have it in a txt format you'll know prefers the annotations or labels in a txt format rather than in a Yamel or an XML or json or any other format so once you have that you just need to make sure that you put your images in the train folder and within the train folder make sure you create a bifurcation one for images one for labels basically and the images going to the images folder and the you know labels go into the or the notepad files go into the labels in part of it basically it's nothing much there's no tutorial to it as such it's very simple you just have to use the tools as such Amruthu and you have to detect the boundaries that's all so like they're showing over here you don't have any code or anything in this case just have to identify the object of interest and you just have to draw or create a map on top of it to indicate that this is what it is once you're done with it you accept or you click on complete it will generate the notepad files with all the coordinates for you and you can use the coordinates for your you know purpose basically is this clear to all in terms of how you generate the annotations yes now this is one example I gave you please research there could be many more okay there could be free also there could be some which are charged also not everything is free it might be free to a certain extent after that they will of course charge you also so that's where you know you can't get you know completely free as well yes any any image you have on card anything you can use it or you can apply it but that's a research for you to actually find out which one is free which one is available etc this is again there but this also played I think label box this is another way you can see here they have actually created this yellow box and then once they do it you can actually get the output in a you can tag them one which is you can give the name as well and then you can extract it and get it as a notepad file as well this is free again for a certain amount of annotations like about a few images then after that you have to pay for so you know annotation is not free you have to pay for it completely free ones will not be very great they'll have their own problems issues etc so you have to choose an appropriate annotation method in this case is this clear to all yes of how you set up the folders structures everything duck we have already done so that's why the duck one is already created so you don't need to do it for the duck that's perfectly fine okay so you can see here so you do an LS it will actually show you the folders we have two folders as you can see LS.data images and labels there are two things that you have here this is one example of an image so image is a function from ipython when you give show image it will show you the png image this is of two duds that are there right duck one duck two now let's look at the coordinates of these two duds and you will notice you can open the text file and you can print the text file over here so if you see this is the link we are using the open function because remember text pxt file cannot be opened as pandas data frame or anything right you can't do a read dot csv so that's why we are just using the open function and printing the red file so you can see these are the coordinates of the two duds unlike a bounding box you don't have just four values right it has an entire value why because this is the entire region right you're looking at an entire region that is getting covered so that's why you have so many numbers here again I remind you make sure you give it as a txt file I don't think you should give any other file formats it'll not accept the other file formats in this case always ensure it's a txt no it's not a random file you can see that it is no it's a random file that's correct yeah this is this particular file we don't have this is the image for which it has given but yeah I mean it gives you the idea of the fact that it has coordinates in this case so just match the file names with the txt file as well yes not specifically for this raul but it is actually showing for a few of them right I mean it's not matching actually because if you notice this is the file name 605 b66 something but this is the the coordinate that it has got right so this is another one because there are so many files it's like we're not able to get all the files so we've just shown one example here of how it actually looks like so you have to do a manual one because this is a custom detection that you're doing right on car now the model doesn't know what a duck is unless and until we train the model to detect a duck it'll not know what a duck actually means basically right so you have to give a few examples for it to learn what it is and then in the future you don't need to train it to detect a duck automatically it'll detect the duck for you so this is the custom training we are bringing in that's right right yes Raul so this one is not the same this is a different image this is a different coordinate but we'll have to search through the entire labels list and that is going to be very difficult so that's why I just we've just randomly printed one coordinate to show you how it looks like and this is an image that shows you how the ducks are represented right is that clear to everyone yes are we all on the same page any questions till now you can do that Amrita I mean you can go through all the file etc you can go through it there are two minifiles let's not spend our time too much into that as long as we understood the concept that's about it right let's not go too much deep into it it's just going to waste time actually right let's focus on the important things are we all clear yes on the same page everybody okay let's do one quick thing a quick 10 minute break let's do that let's take a quick 10 and come back at 9 11 and then we'll continue with the the second part of it which is where the modeling comes in so that's why let's do quickly and come back okay I hope everyone is back yep we check so yes today's class is still 11 but not fully till 11 we'll probably stop at about 10 40 because I have another session at 11 so I'll just need to quickly have my breakfast and come back so yeah we'll wrap up around we're near around 10 40 is that okay yeah so that I can also quickly take a break and come back in that case okay so everybody back everyone's ready yes okay so we're going to create the YAML file and it's not very difficult as you can see here you can actually this over here this quote over here basically refers to us creating a file right right file is basically to create a file in this case so we're going to create a config.YAML file in this case right so so that is where the config.YAML will have first the path so you can see here this is the path of our data how did I get that I know I hope all of you know this you can just use the copy path and then you get the content slash data that's the path for the data and then within that the train data as you can see here is images.Train you can see this is the path images.Train is where the train dataset is and images.SlashVal is where the validation dataset is now the number of classes we are looking at is only one class okay so we are not building multiple classes we are just creating one class which is detecting the duck in this particular case so that's where names is equal to or NC number of classes is equal to one and names is duck so we are just detecting one object of duck now if you had two then put it as two and then comma whatever the second class is that you have that's about it so you can create that and it can automatically detect all of that so you can see the configuration.YAML file is created here right you can't really open the file again but you know you can notice that the YAML file is created and saved so that we can use this as an input to the model you will see that we will pass it on to the model in this case so now we have a training data ready we have the YAML file configuration file ready the data stored in the structure and format that YOLO requires it so that is taken care of so pretty much all the pre-processing required is taken care of in this case right so that's where more or less all the three requisites of the model is in place now comes the most important part which is to train the model right we need to train the model to be able to detect the ducks or at least it should be trained in order to be able to now go ahead in the future and detect ducks for us right it should be able to do that you know on our validation data set also so that is where simple like the last session how we had object detection model is equal to YOLO how did we get YOLO you can see it has already been imported here right so ultra-letics import YOLO we have imported it already now YOLO V8 nano segmentation now N over here is nano a small model why are we taking a small model because this model if you run the large one or the excel it takes a lot of time to run at least about 20 minutes or so it takes to run if you do the excel or large I'm not doing it because we are in the session right so that's why you know not of an interest right now for us in the session but if you really want to do it my suggestion to you would be try and do that later when you are let's say for example but you know when you have some more time when you have some extra GPUs also available in that case try it out but right now don't you know don't try to use the large one excel basically right don't try to use it because it will take really really slow so that's why not in the session but later after the session you can change it and replace it with the larger you know the larger model in this case followed by that see earlier we just did model dot predict if you recall our object detection example we directly did model dot predict because there were classes that were already there that we were using yes or no so we had that car by bus whatever it is truck etc. 2 3 5 and 7 we had already given those IDs so it had to just predict it for the it had in it wasn't trained to detect those it had already been trained to detect those particular objects of interest and hence no training was required there but here important thing is we are training the model we are teaching the model that hey we have new set of objects that we want you to segment or detect so hence you need to look out for these pieces of information in the configuration file so that's where model dot train data is equal to you need to specify this yaml file the yaml file is the configuration file and remember the yaml file has all the information in terms of where the path of the data is where the training images are where the validations are the model only accepts yaml file this is exactly the reason why I'm suggesting don't give JSON files XMLs or anything it has been it has been built to accept yaml so hence we are giving it yaml in this case right the next is we are running it for 10 epochs so the layers will get updated for the new you know the new images let's say the new objects here in this case is ducts so it will be updated to detect ducts in this particular case so the weights and biases the filters everything will go for an update and we are going to do this update within 10 epochs is 10 enough the answer is no it is not okay it is actually you require to give about 52 something odd epochs right 50 60 epochs minimum you need to actually provide in this case now why are we giving 10 again because if you see this you will run it on let's say for example really large 50 or 100 the free GPUs will actually get exhausted so what is the solution to that that is where in our next term when we build AI apps using google cloud platform I'll talk about you know how you can use the various various cloud instances that are provided with GPUs and you know how you manage the cost all of those things right so that is where the next part comes in we'll talk about that later but since we are using google colab here we are not going to be able to run 50 or 60 epochs primarily because of the space constraint right it will it will exhaust you can try that also if you want you can give it about 30 or 40 epochs after a few epochs you will see that the ram will shoot up and then after that it will stop running the other alternative if you really want to run large number of epochs then you can use the caggle kernel right caggle is allowing now a little more than more than just the 12 GB of GPUs that you have you have a little more that they are actually providing so feel free to use that as well so again I am outright saying that that's why the model is not going to be very accurate okay so it's going to have some problems but nonetheless the point is we learn how this works we can always scale it up later with the appropriate hardware that we need as long as we've understood the concept on how it is implemented basically so that's where epochs is equal to 10 image size is equal to 640 okay 640 is the size of image that goes in as the input we have not reshaped it automatically it will reshape it you don't need to worry about it so whatever is required it will take care of it by itself right and when we go ahead with that you will notice that first thing it will do is download the entire model you can see it has downloaded the model here and it has detected our it has detected our this thing as well GPU and these are the different layers of course it is passing through takes a few minutes you can see has done all of it validation is running it is showing this is basically called as the tensor board right so you can actually visualize the entire it's not opening up over here I don't know if it is doing for you guys but if you want you can actually notice the entire model the layers how it is going through it is doing all of it over here in this tensor board the tensor board is a visualization for the entire you know model through across different layers etc how it is going you can actually visualize it but it's not opening for me I think there is some installation that needs to be done which is not working for me so takes a few seconds a few minutes and it should be able to generate the outputs in this case yeah there is some installations that require to be done abg but those of you who want to know what exactly tensor board is you can actually see it will give you something like this it will give you a visualization like this so it will tell you exactly through which layers the images are going through so whatever we see here in the we have the verbose right the verbosity that is being captured here that verbosity is showed over here across each epoch it will show you some few images an example so it's a visualization in this case that is what you can actually see so it has some installations unfortunately I don't know why it is not installing earlier it used to work perfectly fine first 22 lines which one are you referring to well see is it model trained on 22 epochs no we are training it on 10 epochs 22 I'm starting to search where exactly is the 22 epochs 10 patients 100 you might see 22 seconds I can see it takes 22.1 mb's that it is actually taking the process of the process it takes 20 minutes to make it look like it is not done yet yeah there should be more than that I think it will have but as I said there are three layers only primarily if you look at it from a macro level there are three layers of engineering the head-neck and backbone they have given a brief about some of those layers I think you're referring to this one right this part here this 22 here basically yes that's right what you're referring to is the layers yes that's correct so a convolutional c2f we saw that c2f and then up sample we saw that as well and then there is the neck I saw the neck somewhere this is the head then there is the neck also in between somewhere so these are all the layers that it has actually used right so anyways we have that yeah it will come up in some time I'll I'll come to that the runs is the next part I'm actually coming to in some time sometime I'll come to it Russian because I'm just waiting for the model to finish running I'll come to it so you can see here this this is what we had actually you know we had you know we had made sure that in the previous example when we were running it you can see here it is actually showing you the bounding box laws the segmentation laws so how far it is from each other classification laws then it has instances and then it has map 50 I hope everyone can see that right there's map 50 now what is map actually maps stands for something called as a mean average precision okay it uses something but it's a mean average precision at 50 50 is IOU 50 IOU is intersection over union 50 now what does this actually mean first thing you know we generally look for a map 50 close to one okay the closer it is to one the better the model is now what does it actually consider when it calculates the map is it looks at precision okay and it looks at recall a definition of precision and recall we all know we've seen it earlier and then it uses a calculation of the average precision values at 50% IOU what is the 50% IOU 50% IOU is when you actually see that the area of overlap versus the area of union is greater than 50% so when there is a higher confidence that is what we call as an IOU of 15 so what it calculates is at 50% IOU where you know the overlap is or more or less the overlap and the union is you know a little pretty pretty much on top of each other kind of something like a good over here that is when we call it as a really good IOU right it's a it's considered to be a good intersection over union in that case now in that particular scenario what is the mean average precision the map value is what it is calculating so if you notice over here it is actually giving you the it is calculating the map values as well map 50 map 95 95 is at a 95% IOU which means there's just only about 5% difference between the ground truth and the predicted bounding boxes or not bounding boxes the coordinates basically right there's a difference of only 5% now of course the accuracy at this point is not going to be very great it's not going to be very accurate okay it's not it's going to be poor primarily because it's a nano model that we are looking at we are not looking at a very we are not looking at an excel model the map 50 as long as it is good it's fine right so it is as long as it can actually detect it well enough more than enough for us we don't have to worry about it now if you're looking for accuracy then one thing you need to do is if you need a higher map 50 or map 95 then instead of the nano model go with the excel model but the only reason we are using that is because it takes a lot of time to run so that's why we are going ahead with the nano now all of this information right while it is running it is getting captured into a folder called as runs can you all see that on your left hand side you'll be able to see it where if you look at your don't click anywhere because let it run but it'll actually have something called as a map over here so it'll actually have all this information that it is storing in parallel while it is running so it's a it's a log that it is creating by itself in this case closer to one hurry just like accuracy right you treat it like accuracy closer to one the better the model is so somewhere about 75 80 is like really really good map values and if you want to increase the map then increase the either choose a larger layer model like excel or something or increase the number of epochs right but either of them you do that you will get your map close to 0.800.75 which is really good actually so just about few epochs left two or three epochs and then we will be able to see how the model actually performs so all of these numbers you will be able to see it in our in our examples so you will I'll show it to you in a moment from now just give it a second it's almost done yeah 0.62 is fairly decent it's a good good number it can be increased further but then again remember we are running ten epochs if you actually run about 15 20 epochs it actually jumps up to about 0.700 point eight close to that it will actually jump up to see the amount of time it takes this is in spite of the fact that we are running it on GPU by the way okay so you can imagine how the model is actually doing 640 is the image of the image shape I'm with the 640 by 640 is the image shape that it transforms it into 100 person is the processing time so here right now it has completed 100% of that epoch is completed right now in the 9th epoch 59 out of 103 so it's like about 60% completed almost done actually it's over I think Russian has got all the outputs as well yes I'll show that very quickly to all of you so box loss is the bounding box loss the difference between the predicted and actual bounding box it's a regression function so it is mean squared error that it actually calculates run and segment loss is in this case it calculates the entropy the difference between the segment so let's say for example I have detected you can see here this is a segment loss like for example in my ground truth I have actually segmented this I have actually provided the coordinates for this but it has not detected it so which means it is a segment loss which means it's a loss of one so there are two segments here right this is one segment and this is another segment so the segment loss is about 50% here because 100% would have indicated both the segments are detected and here in this case it's a loss of 50% because one segment is not detected and the other segment is detected the other is the bounding box be box losses if you see very carefully here there are certain cases where you know it'll I'll come to it a little later you will see that it is not exactly segmenting there are some portions of the duct which will actually be missed out in terms of the segmentation that is called as a segment loss like the entire shape is not clearly covered there's some portion of the shape that gets missed out that is called as it's bounding box loss which I'll come to in a moment from now I'm just waiting for it to finish running so it has finished running done for everybody so you can see this entire log that it has actually got over here this entire computations etc whatever it is you can see that it has created a folder by itself called as runs okay so in this runs folder if you notice it has all the information that you need right so what it has done is it has predicted on the training data okay and it has also done it for your validation data also so it has done for your validation images as well in terms of prediction now you can do a model dot predict on an image and it can actually detect the objects of interest so what it actually has is the f1 curve you can see here ideal situation would have been that it should have touched perfectly one it has not touched a perfect one so that's why it's okay likewise you have the precision recall curve again close to about one not bad I think it getting to about 0.82 precision recall curve likewise you have what else you have you have the confusion matrix how many objects has it detected out of interest versus how many it has not detected like a duck for example it has predicted multiple times the duck as a background about 54 times it has predicted a duck as a background luckily it has not detected a background as a duck which is a good thing okay so that is oh actually it has so background it has a duck it has detected 40 times background is a background there's hardly any scenarios because we've not annotated backgrounds right so you can see more or less good number of times it detects a duck as a duck right in about any box it is able to do that so it's a good thing that it has actually been able to do right performance wise it's quite okay likewise you can see confusion matrix for the normalized okay normalized is after it turns a normalization etc so it has same thing almost not much of a difference so you can see here this is the percentage accuracy right 80% accuracy accuracy is what it has can you notice that right it's got about 80% accuracy which is okay not bad I mean we have only run 10 epochs basically right and you can see this this is the predictions that it has done it has taken a few images from the validation dataset and it has predicted them can you all see it this validation one dot batch underscore one dot predict can you see these are all predictions by the model by the way this is the models predictions is it accurate we'll have to evaluate we'll see a few images you can see batch two predictions these are the second batch of predictions now you notice here can you all see this image here in the value validation batch underscore two underscore bread dot jpeg just look at it here on your left hand side you should be able to see it just under the validation dataset can you all see that this is the prediction so it has created about three batches one two three is what it has created and then it has also the so you can see here if you notice very carefully this one is not detected accurately can you see there are three ducks here four ducks it has not predicted it accurately there are some other cases I'll show you that as well you'll see these ducks it is not predicting very accurately it's not great again it makes mistakes that's not a problem but as long as we know how to correct it it's not an issue we can increase the epochs or you can go with an excel for an anomodel this is quite good not bad actually it's quite fast so you can do this just see you can use the show image and we're going to take the validation batch two labels dot jpeg so I'm just going to run this particular example and you can see it has taken about how many one two three four one two three four so it has taken 16 images from our validation dataset if you notice here in our data you will see that this is the validation dataset this is the validation dataset and it has taken about 16 of those images randomly it has 16 images right for evaluation purpose so it has taken those 16 images and can you see this duck is not detected can we all notice this this is not detected it's a poor detection that it has actually done this is where it is called as a you know inaccuracy right the prediction is very poor in this case so there are two things bounding box as well as there is the classification now classification there are two ducks to be detected but it has detected only one which means it is only 50% accurate and then there is the bounding box loss basically right it is ideally not got this as well in this case it's not a duck I thought it was a duck it felt like a duck to me but then in that case it's good it's done a good job then in that case right if it's not okay then no problem but anyways look here this one is a duck I'm sure this one we can see I thought that looked like a duck but here you can see this is a good example if you want to look at it so here for example you know there are two instances of ducks it should have detected both of them but unfortunately it has detected only one instance so which means it's a 50% accurate and there is a loss because it has detected these coordinates but it has not detected these coordinates which is a poor prediction so that's why this is a failure actually there is another this is also can you see this not accurate right only one instance of a duck it has actually got over here right whereas the other instance it has missed two you can check another image also I have given one here validation batch two labels you can take one more example go to the runs you can take one more example I'll show it to you validation you can take batch one as well you can check that as well just take a new code here and we'll take batch one labels I'm just taking another example just to show you this as well so this piece that here you can actually see the prediction here also so image will show you here so it is detecting this so it is detected but this it has missed out over here so it's another example basically yeah it's detecting mostly only one it's not able to detect you know all the three in this case right or two in this particular case that's why it's a nano model right it's it's not very accurate but I want you to try it with the excel model I mean as a homework maybe I would encourage you to try it out with the excel and see how well it actually detects by the way did anyone use the excel model for the object detection example the previous one we did the traffic thing anybody used an excel nobody's done that guys should do it here you should practice it out and see okay very important question that roshan has actually bought it which is one with the shade in the water how it is differentiating that that's good right it shouldn't actually detect this as a duck right I don't know whether it's an error or whether it is something that it was smart enough to detect as a reflection but I will give the benefit of the doubt and say that it's possibly not able to detect the reflection that's why it has created that way it's not like it is that smart that you know it it found that is a reflection of something and considered it as a single instance maybe it's an error that went in the right direction right in another way it should have actually detected it in this case but if you ask me it would have detected but because you're using the nano model it basically the error turned out to be in favor of us basically the colors and pixel values both runjane so it is taking the entire texture of the images to be able to generate that that is what it is doing likewise yeah this is the batch two predictions so this also we saw just now this is the same thing this is where the precision recall curves are there which is showing you the precision confidence curve ideally it's at 0.90 not very great it's at about 80% on an average which is not very good also not very bad also you can of course there is hope for improvement as well likewise with the recall curve ideally it should be at 1% perfectly at 1 this should this should actually touch this part it's not perfectly sitting there which is also okay it's close to about 80% in terms of its accuracy at the confidence level then it shows you the loss right it shows you how the loss has reduced across the different epochs as well so it is actually showing you the precision the loss etc across different epochs across different scenarios etc so these are all the diagnostics that it provides after it runs we have not constructed them every time you run them automatically this particular information gets generated all this f1 curve PR curve the recall you know precision recall all of these things are generated confusion matrix etc all are generated by default the moment you run it every time for a new set of images it will actually generated likewise it has generated the confusion matrix as well this is the training confusion matrix right this is what it has actually generated likewise you can also check for the testing as well it is the same thing that I just showed you sometime back is just showing that in terms of the accuracy in this case now what we will do is we will take a prediction how now let's say for example I take a random image right and I pass that image into the model I would like the model to now predict the ducks in this case now let's say that I have an image that has not been that has not been exposed to the model I'm asking the model a completely independent image right so I have that image here I'll just show it to you guys so this is the image that we are going to actually pass inside the model I'm just sharing it on the link as well so this image that you see on the screen is not actually exposed to the model this will not be exposed it's an independent image so when it is not exposed to the model then ideally it should be able to detect it and of course there is also the reflection over here so let's see whether it is able to distinguish in the reflection or not right now how are we going to predict it in-ference it on a data set which it has not been exposed to how do you actually make it work that is where the next part comes in so the first thing we are going to do is our model that we have just run over here right this model that we have just done over here I'm going to take a backup of that model that we just ran now how are we going to do that you see here my model I'm going to give it a name called as my model your low and within the runs part of it if you notice here this runs you will see there is a dot pt file just look very carefully I'll show it to you here you notice can you see this best dot pt there are two things here best and last pt can you see that here in the left hand side right now this is a pre-trained model that has been fine tuned for my purpose and it gives you two different models one is it gives you best and the other is it gives you last now best is basically out of the 10 epochs it has chosen or it has frozen the weights at the epoch that has had the maximum accuracy that is what the best actually stands for so out of the 10 epochs it could have been the eight epoch where it has got the best accuracy it will freeze the weights and bias at the eight epoch right whatever the weights and bias kernels filters whatever it is it will store it into the best dot pt on the other hand the last dot pt what it will do is it will only store the 10th epoch so the final epochs if you run 10 epochs it will take the 10th epoch weights and bias and it will store it at the last dot pt now generally it's not necessary that the best and the last have to overlap okay it can be possible that your best is the last that can also happen but generally it can be quite different also so as a you know process what we generally tend to do is rather than go with the last we go with the best so last can be poor it cannot be very great sometimes it can be worse than the early epochs as well so that's why we will take the best pt so the best pt has all the weights and biases now tomorrow you don't need to rerun the model from yolo again so this best pt is your fine tuned model and this fine tuned model I can give it to anybody so let's say tomorrow I want this model to detect ducks and I'm happy with this model I can share this model to anybody and it can start detecting ducks for them so in the same way for example if you train it to detect let's say I was explaining about the example of how the person had actually detected helmets and safety right whether they are walking in that particular path after he finished running it once the model was satisfactory when he saw that it worked well then takes a backup of this the best dot pt you can take a simple download of it and then it remains as your own model it remains as your own custom yolo model which you can deploy it into the cloud etc all those things now how that deployment happens that we will see in the next term right I mean you can talk about the deployment in the next session but remember that now you have a copy of your model already stored it is ready for you to work with so fine tuned model which is a yolo customized for your own classes is ready for you to use so that is where what we are doing is we are taking my model is equal to yolo okay and we are storing it and calling it as my model the best model that we have actually got we are storing it as our custom model in this case now I'm not going to do my model dot train because this is a this is where we are actually going to do a inference on this particular data set so that's why my model dot predict source is equal to this is the link of where the images show is equal to true save is equal to true confidence is equal to 0.5 so anything greater than 0.5 it will tag it as a object that it has actually detected in this case yes preferably use the best dot pt for future usage that's the best thing to actually do so let's see how it actually runs on this image so you can see it has got these pixel values that it has generated past it through 168 comma 229 is the original shape of the image it will of course convert it into 260 by 260 and you can see the speed it takes about 1.74 seconds right to run it very fast it is able to do it really really quick in terms of the output and then you can go ahead show image it has predicted it over here can you see now I think somebody earlier asked you can see it has also tagged the reflection also here unfortunately can you see that ideally it shouldn't have I mean this is where the error is this lower part should have been ignored here right this this entire portion here it should have ignored it in this case but of course it has not done that you will notice that it has actually selected that part also can you see that it has colored it gray yes you can do that also do page either ways is perfectly fine now the reason is this is not accurate is because we are using the nano model right so that's why it is also very poor and so if you use the excel probably the reflections might have been ignored but here it has also painted or it has shaped the this as well so that's why you can see the results will be more accurate if you use a larger segmentation model and this smaller one of course will not be very very great as such if you have two objects it will if you have trained it for both the objects it will select both the objects I will do that if you have trained it only for one object then it will select only that one object so let's say for example in the background there is a some other bird and if you have trained it not to detect that bird it will not detect it or if you trained it not to detect fish it will not detect fish but if you have trained it then it will detect it on the in that case that's how it will work right is that clear to everyone on how the yellow V8 segmentation works and how it can be used for custom object detection any questions any doubts any clarifications are we all clear only one person is clear the rest are all not clear is it I'm going to assume that way yes excel model you can will require a higher amount of GPU runjen in case so just hold on for the next term I'll show you guys about GPU instances as well if you want to run excel but if you alternatively want to do it right now itself you can also use taggy kernel actually now I think yesterday only they said they they are giving more GPUs actually so you can use that as well give them enough number of images through page where you know you have inverse images also when you say inverse I think you're referring to the images that are a reflection right that's what you're referring to okay so just don't give the coordinates for this just tag this part that's all just give this part and again it is making a mistake here because as I said it's a small model if you give the larger model excel it'll not show the reflection because the reflection patterns will not match it's the primary reason this error is happening is because of the smaller model but when you provide the annotations just make sure that the annotations does not include this reflections over here that should work out perfectly fine so as I was saying you know you can use the taggy notebook kernel as well it can here you can see there are some kernels that are available for you you can actually get about a little more GPUs if you want to in this case runs a little more heavy in case you want to but I think you have a restriction in terms of only a few GPUs in a day that you can actually use so that's where you can try it out and see if inverse coordinates can help can't be included in the training but why would you want to give the inverse coordinates I mean I'm not understanding the reason why you want to give the inverse coordinates like the why would you want to give let's say for example the reflection and its coordinates but if you give it then it will detect it right if you provide the reflection coordinates then it will start to detect that also rather when you don't give it it's a pattern that you're saying it's not it's not of interest to the model in that case so that's why it's better not to give the inverse coordinates so if you give the inverse coordinates it'll think that there can be ducks that have a shape like this also because at the end of the day the you low model doesn't know what is a duck or anything at the end of the day it just recognizes the shape so that's where it's better not to provide them because otherwise it'll start to think that this is a part of the duck or there can be ducks who look like this with two face basically that's what it'll end up thinking so that's it yes same thing for videos also it'll follow the same method basically yes videos also you can pass it if you want to you can do it that way 0.82 is the probability 82% probability is what it gives and see yes so it's about fairly decent we've given 0.5 as the threshold so anything greater than 0.5 it will take it up it will show it so you can specify the confidence here if you want only anything greater than 0.75 to be detected you can get 0.75 and only those objects will be detected in that case right are we all clear yes any other questions please ask before we move on to the next topic very quickly all good all clear okay so if you are clear then just one thing maybe please go ahead and try it with the excel model also try it out with the GPU it should work fine if it is giving an error then we can check it out and see I didn't get your question Amrita can you please put your question again I'm sorry I missed it is it about the video you asked video instead of image we can use the same method is that the question I don't know I can't see any other question so first thing same same process Amrita only thing is the videos you have to read it using open cv2 and convert all the videos into frames and store the frames here that's about it just make sure you store the frames into your same process nothing changes as said just that every frame that you have just make sure that the frames are annotated and every frame put it into the data images here put the train all the frames here within itself and also the coordinates that you get from the frame so those coordinates also put it over here once you're done with that you can run train the model and then after that it will do the same sort of prediction model. predict as well you can do it and then it will predict on individual frames you can you know concatenate all the frames using again the video writer function that we used in the previous session the frames will be put together or sequence together and then you have a segmentation applied onto the videos as well it's the same process only thing is while training it make sure you provide it as a frame instead of it yes annotate headphones and use the cbat or any of those those libraries that we just saw any of those tools that will also help you to kind of detect all those things so it can whether it's headphones or anything else any other object of interest you can specify it there. Okay so very quickly we'll move on to the next concept which is on the genai part of it so images also remember have generative AI right so just before moving one quick go at any questions that are pending just to quickly check if anybody has anything then we'll quickly move on to the genai part as well right because that's another important concept that we need to cover as well. We're all good I'll take the silence as no questions can we create annotation without Tio I didn't get what is that Russian without tools no not possible you can do it open cv allows you to annotate like you know this part open cv you can do it this way but it is very very difficult actually it has something here using without those tools you can try doing it but you'll have to write a lot of code and you know extracting it is like a lot of process actually so that's why you can do it if you want to it has a custom segmentation method to do it and get it but very very difficult I would say the easier method is to go ahead with those tools like cv etc they are much more easier they'll do a much better job actually. Well not really Russian in fact at least we'll finish the theoretical concept Russian maybe the hands-on we'll see but I'll have to because we're already a little behind schedule and I think a lot of participants have been asking when will the sessions get over so I've been constantly asked by the accords team to ensure that we you know finish those things importantly. Yes we'll start off with the genai part of it very quickly so now we are getting into the genai part which is primarily on the generative models on computer vision so don't confuse them with generative models that are there in NLP these are not transformer based models these are very different models that we're looking at the only difference between those models and this is the output over there is text here the output is going to be computer vision or rather it's going to be images that are going to be generated as an output. Now what we are going to actually see is a quick introduction to generative AI models then we'll talk about why do we need genai models in the first place and then primarily you know find out the difference between a discriminative and generative. We've already discussed this concept of latent space and latent variable the hidden variable concept basically right and then followed by that you know I'll explain that same concept with reference to here right in variational auto encoders. The most important genai model in computer vision is called as a GAN or what we call as a generative adversarial network basically right and we'll talk about it it's one of the earliest inventions in the world of computer vision but of course we have now moved on to really really advanced models as well so from the GANs we have now moved to something called as diffusion models and I'll explain to you what diffusions are also right diffusion is nothing but it takes the noise and it tries to convert it into a latent space or a latent variable and using the latent space and latent variable itself generates a new image or a new output basically that is what a diffusion model is. The most famous images that you see when it comes to genai those are all mostly done by diffusion models and what is interesting about diffusion models is that you don't pass an image as an input rather you actually provide a text a prompt as an input it takes the prompt and what it does is it passes it through a set of transformer models and converts them into attention scores and uses the attention scores to actually generate an image right that is where the entire concept is so these are few concepts that we are actually going to cover hands on use case I'll show you both GAN as well as a diffusion model I'll show you both of them as an example the diffusion model of course we're not going to train it or anything we're just going to take a pre-trained model called as dally which is provided by open AI we are going to use it directly and we are going to see how it actually works right Harish don't give me a heart attack please we completed the entire GNI concept in NLP I think the entire NLP 2 was GNI right I kind of got a little bit of a heart attack when I saw that message there didn't we complete that we did transform us everyone yes or no yeah Harish you forgot I think okay might have to go back to the videos you might be able to review it yes we did that text generation yes we did the transformers etc all of that we actually did right so those are all part of it okay so what do you see I mean what are these images that you see in front of you how many of them are actually real in this particular case right no not only theory we covered practical also we did a bird exact right transient we use bird for classification we did one example with that then we did it with also hugging face come on guys you can't put just theory we did practicals also a lot of you have very short term memories I must say you know we do a lot of examples then you tell that we don't do examples at all and then I'm asked by the academic steam that why are you not covering examples of I'm like I've done that we've done this but people forget it but anyways okay so coming back to this concepts here the images that you see here none of them are actually real okay they are all fake images that are actually being generated in this particular case right and that is where what we have now is a generative model that are capable of generating fake images right there a these are no real human beings in the first place I mean a person like this doesn't even exist in the world right they don't they are not there I mean this is just some fake image that has been actually generated but interestingly generative AI models are able to generate images which looks so real which looks so realistic or so similar to real human beings that sometimes we are not able to distinguish them right and that is where genie is when it comes to images like text text is being generated so realistically isn't it when we look at chat gpt today or when we look at Gemini for example we are actually looking at text that looks so true or looks so perfect that sometimes we are you know thinking whether that in fact when gpt was first launched many people who are actually saying that that this is not generated by a computer or a model or machine learning model in fact somebody is based out of Philippines who's every time you ask a question is actually copy-pasting and generating outputs no so people actually felt that is how the models were so this this is what generative AI is right the capability to generate really really good images and yes as pointed outright there are some finer cues like as he mentioned which will tell you that this is not a real image like he already detected one the mismatching hearings then also there is no hearing on this person's ear you will notice that there's some sort of disconnect between the the year between years of the people it'll be two variational kind of shapes it'll be very disconnected eyes or some some unique things which you will have to be very very careful when you actually look at it you'll be able to actually see how they look like basically right i'll give you a bit of background to how generative modeling actually came into play this is very very important because you will be able to understand how genai came into play and how when it comes to computer visions what was the idea behind actually creating these genai models basically right so back in 2011 2010 2011 we had multiple discriminative models right 2011 2012 actually we had like lot of CNN models that were built which were primary reason were to actually distinguish between let's say two class kind of problems or multi-class kind of problems that people were developing now the problem is for the success of those models a lot of training data is required so let's take a simple example right you want to do a cat and dog image classification model you need to have enough images or instances of images of cats and dogs to be able to classify now luckily you know when it comes to let's say dogs and cats we have enough amount of images if we go to the worldwide web we will find you know just put cats you will get millions of images take dogs you will get millions of images and you can write a function or a code to download all these images to write into a location train a model etc and you'll have a perfect cat or a dog classification model but the point was there are certain rare cases or rare scenarios where you can't actually let's say for example get enough amount of images right let's say I'll take an example right let's say for example you want to do a tumor classification you want to classify let's say images of a person with a tumor versus no tumor or a rare disease versus non-rare disease right those kind of classifications if you want to do you can't just simply go to google and search for like images of these scenarios and it provides you all the images and you train so what happened is there are certain cases where they actually found that there was a bit of an imbalance which means they didn't actually have enough amount of images or they didn't have enough amount of instances to be able to train the model now just like ML scenario what happens is in ML the moment you see an imbalanced structured data what do you do you actually are you're actually a easiest thing that we all do is basically use mode isn't it everyone remembers synthetic minority oversampling technique so in small what happens is it will actually generate you know a random set of features right or rather it'll generate a synthetic set of data set this is not real by the way so if you see titanic data set use feed in 800 rows of data it will generate an additional 800 rows of data very similar to the original training data set that you have provided right that is exactly what it will follow as a process but as such there is no relationship they're they're just synthetic they are just newly created data they are not the original data so they wanted a solution something very similar in the case of image or computer vision problems as well so what they did was they proposed a way by creating a synthetic images right so the intent originally was not to create fake images that was not the intent it was to create synthetic images which look similar to the original images but the intent was that we use that for creating you know new images that can be used for training better discriminative models to augment the shortage of data so that is where generative modeling's original objective was not deep fake they didn't want to create fake images they wanted to actually create what we call a synthetic images for training purposes and what these models were supposed to do is look for patterns in the data but create brand new data instances like for example in the sport it creates new instances it creates new rows but none of those rows are similar or they're similar but are not the same as the original rows on which they were provided with so that is a big difference that comes in so this is what we call as an unsupervised learning technique where what we will do is we will actually expose the generative AI model to real images or real data and then what we will do is we'll push in something called as noise within the generative AI model so what the model will do is it'll combine the real data provided and the noise together to create a synthetic image now it is going to be accurate we don't know like somebody pointed it's going to have some sort of shortcomings which are 100% sure but the point is that we want it to have some sort of variation so that it can actually learn well so what the generative AI models are actually doing is when you provide an input data they take samples from that input data and they actually try to get what we call as a density estimation and sample generation which means in simple terms they're just looking at the pixel distribution basically right so they are trying to understand let's say for example you provide an input of let's say you know a celebrity image it is trying to understand how the pixel values are actually distributed it is trying to create a learning of those values and then what it actually does is in the generated model it will actually try to take those same distribution and replicate that distribution but it will not try to create I mean when it tries to create that distribution it will not do a control see control leads not going to be a copy paste of it what it is going to do is it is using the probability distribution which means there is going to be some amount of variation that is actually going to come in and that variation is what actually creates a generated sample which looks very very different from the original sample that is what is the beauty of generative I modeling and you'll see how it actually does you know a little more detailed into the math part of it basically yes noise can be 100% absolutely yes you know how 100% noise actually looks like anybody knows how it looks like the 100% noise not white I'm sure many of you would have seen this this is noise by the way in your 90s TV right yeah this is called as noise random pixel values yes if you notice there could be anything 255 0 whatever it is any numbers can be there so this is called as noise nowadays we don't have noise instead what the moment the screen comes up automatically it goes to a blue screen basically right it converts it into something like this something like this it will make it into blue screen basically so this is 100% noise this is a 100% noisy video basically right yeah it uses the original image for reference or that is where we'll get into the math a little more in detail I'll explain how it actually was so synthetic image generation is Ankita similar images to the original image right very very similar in nature but they are not the same it's like creating a new image out of a set of original real images like for example I'll give real images of let's say a Donald Trump for example right I'll give let's say hundreds of images of Donald Trump then what it actually does is also provide it some other images let's say additional images of let's say some other person let's say Vladimir Putin for example right I'll give 100 images of that as well it's an unsupervised learning so the intent is not to classify between let's say Donald Trump's images versus Vladimir Putin's images what it will actually do is the final layer where you actually do a classification and dense layer where it is 0 or 1 basically the shape of it is 1 that dense layer what you will do is you will change it into an image shape and what it will convert is use that same images to create a new image so it will be a mashup between both these presidents to create a new image so that's called as a synthetic image which is not real I mean a combination of Donald Trump and Vladimir Putin doesn't exist in the real world right it is a creation by the model in this case if you want to generate a report that is a different problem all together Amruthas so that when you want to generate a report you will have to use transformer models because report is remember text right here the output is an image it's not going to generate a text so in that case you will have to look at transformer based models NLP models basically that's what you need to look at yes multimodal basically you have to use multimodal models which can take inputs as image and generate text or take text as an input and generate images either way that's what we call as multimodal right is that clear to everyone till now with me yes people are tired or people are still there okay good so why did the need for generative AI come in the primary need or the primary requirement to actually bring in genie I was primarily because of the fact that you know there are certain scenarios for which you don't have enough number of instances I'm talking about about 10-15 years back basically right so not I'm not talking about today today we have enough amount of images there is no problem but back in the day when deep learning was about just about taking baby steps you didn't have that many number of images so that is where you wanted to discover or detect some sort of not outlier on an anomaly but rather what we are saying is we wanted to detect unique cases or unique scenarios right so for example most of your training data would be something like this right like you'll have a sunny day you'll have a highway you'll have a straight road this is what normally your model will be exposed to because 95% of your data is that but there will be about 5% cases where you'll have scenarios like an accident for example or harsh weather or pedestrian walking etc now when you have only 5% it's an imbalance data set right 95% of your data normal data right standard data and only 5% of outlier scenarios basically now imagine trying to train a autonomous vehicle in this case what will end up happening the autonomous vehicle will never be able to look out for scenarios which are let's say outliers right or scenarios like an HK or a pedestrian or a harsh weather for example it will not be able to detect and that autonomous vehicle if it is not been trained with enough instances of pedestrians it will not detect pedestrians and automatically it will run over the pedestrian so that is where the original need for Genai was to actually create what we call a synthetic data for training better models or to be able to train better discriminative models in this case right so that's the idea behind creating generative AI in this case exactly right it is kind of like how smooth is working in ML that is what the idea behind it was in terms of creating it so that is where the various domains like data augmentation data denoising I'll talk about that now even recommendation systems are using genai models like computer vision models NLP and you have a huge set of what we call as multi models what we mean by multi models is the capability to actually generate images or text or taken inputs as text or image and are able to actually generate an output as well that is where they've actually started getting into so while the intent was to originally augment data with lesser number of frequency or a class with lesser number of you know items you wanted to augment that with more information that was the original intent but now what you're looking at is that people have found out that this can actually generate content also for us right we don't need to worry about generating content it can automatically do that the interest actually just moved you know by multiple bounds basically right it became like crazy basically now first thing we need to remember is we've already discussed this the primary difference between a discriminative and a generative model I think we all know that we've seen it in our NLP sessions also the Hamm and spam problem was a clear example of a discriminative model whereas you know the generative model was more around generating let's say for example we looked at a scenario of generating text generating a sequence of text or creating a chatbot or asking a question all of those become a generative problem in the computer vision world also much the same what we've seen till now is a discriminative models that we have actually come across right for example we have done a cat and dog classification we know really well how to use a pre-trained model to classify between a cat and a dog for example but here comes the interesting use case that now I'm not just looking to actually discriminate between a cat and a dog using a CNN but rather what I'm interested is to create a model that can generate images of cats and dogs that is what the idea is so the output shape completely changes here the output layer will completely change over here which is you don't look at simply creating a CNN layers that will have the final layer of 0 or 1 rather the output layer is not a 0 or 1 rather the output layer could be a 220 by 220 comma 3 or 650 comma 650 comma 3 or 108 0 comma 108 0 comma 3 whatever it is right an image is the output in this particular case so how do you structure your layers in order to be able to generate an image as an output right is this something we are all clear difference between discriminative and generative yes there are multiple retail use cases as well Russian one of the most common use cases for retail is generating planograms right so many generative models actually automatically generate planograms so earlier there used to be people who used to create manually this thing called as a planogram which actually tells you how to stack up different products right on the shelf of a retail showroom so nowadays you have actually planograms AI based planograms that you actually has right so they are all based on these models only you have here right these are all softwares that are actually built on computer vision based in AI models that can actually generate planogram compliance or create planograms so it will also stack up the products in such a way that you know it will be able to exactly detect or tell you what objects you should place together like you know coffee and let's say sugar together or something like that right or you put red bulls together and Pepsi together or something it will be able to stack it up for maximum sales it can do that now input is images it takes images as an input you can give videos also but images is more than enough it can it can work perfectly fine these are all images of planograms where you can actually use them this is one of the common use cases for retail which is I think the best use case basically and it automates the entire process in this case yeah probably the some amount of it AI requires human in the loop also remember humans are not AI are not very smart also they they change the entire positioning so weirdly that the customer also might get confused basically right this entire planogram thing is nowadays developed by Nielsen originally it was done by Nielsen only so now Nielsen does it for mostly all the big chains that you come across right like smart bazaar and reliance retail and you know nil gris in the south or more and all of these places they are the people who generally generate it for them and they now use AI to do it earlier there used to be a person manually sitting on CAD yeah not AC anymore they call it only Nielsen so earlier people used to sit on CAD right 3D CAD and they used to do all this manually create all of these drawings and everything there used to be at least every every partnership Nielsen used to have like five or ten they call it as planogram generators people who used to create planograms or they call it as planars basically now they have mostly replaced a lot of these manual planograms with AI based planograms so they have these people who just validate and check for compliance and then they go ahead and approve it basically in ML human dependencies more cleaning data EDF feature extraction ML models and so on but not little amount of dependencies slash code for us to do in deep learning yeah absolutely it's going to reduce even further right Rahul going forward now we have agent frameworks that are coming into play that are pretty much going to automate everything that we do in terms of finance for example planning taxation all of those things are also going to get automated you have agents that are capable of creating an app itself for you you don't even have to sit and do all of that so the way in which the world is moving towards its complete automation anything that required a person to sit in front of a laptop or a PC to actually build anything is now no longer required you don't need anybody as I was giving an example I know Nielsen used to have like five or six people sitting on CAD and generating these planograms on a monthly basis right they used to do that and five people at least for every account right like for example more there used to be five people there used to be five people for Alliance etc but now they don't I think they have only about two or three people and that also those two three people are only for compliance which means they will validate and verify the the model generated output or planograms are actually right or wrong that's all so same is happening in the software world as well you don't need people to learn how to build APIs or create APIs or write front end you know interfaces etc that's not required anymore you have AI models that can do all that for you co-pilot is an example of it so that's the intent eventually yes after we develop the model we deliver it as an application to you so that's the intent behind it right are we all clear yes with me till now yes or no people are already tired and sleepy or have this left off already are you guys tired not tired okay good another few minutes and I'll pause because I don't want to overload as well and then the second thing is I've also got to prepare for the next session so that's why I'll not push too much also but I'll just take this one important concept and then I will actually pause the session so that you guys can take a rest as well now remember remember this concept we talked about latent space in our NLP sessions as well this is what we call as a hidden variable and I believe everyone remembers the concept we discussed it when we were doing tan hitch everyone remembers that what typically neural network models are actually doing is they're multiplying it with a set of weights and are encoding it right so you have an activation function let's say typically a tan hitch activation function where the range of values between minus 1 to 1 is basically a representation that the model understands okay you and I cannot interpret it let's say for example when I multiply with a weight add it with a bias and pass it to a tan hitch and it gives an output of let's say 0.85 or 0.32 what does that actually mean it's not a probability okay unlike a sigmoid I can't say that it is 30% probability that this is the output or it's a 50% probability that it's the output encoding is where the beauty of neural networks comes in it is the ability to transform and input into a condensed form or a shape that only it understands which you and I will not understand but it knows exactly how to use that encoded value to generate an output a very simple example for you to understand right if you remember our LSTM sessions right if you go back to that you will remember that there were gates isn't it there were four gates there used to be the hidden gate the sorry the output gate the input gate the forget gate and so many other layers used to be there those gates were there now what those gates actually did in reality was they were encoding the information they were actually transforming the output from the previous layer storing it in a way where it knows what it is important and what it needs to discard off right in the same way the transformers encoded information in the form of attention we call them as attention but attention at the end of the day is nothing but being able to find out which words are important with respect to another set of words right that is what is called as an encoding so encoding is a mathematical representation right where an input is converted into a lower dimensional form the most beautiful example of latent space is your PCA right that is the most important example your embedding is another example of latent so if you think about your journey in neural networks in the last whatever two or three months that we have been together you will notice that it's all about taking a set of inputs transforming them into a lower dimensional representation and then using that lower dimensional representation to recreate it okay I'll explain this entire concept in a very very simple way for you to understand the most beautiful thing is think of yourself as an artist now what do you actually do so let's assume that when you are looking at let's say a beautiful scene outside your window let's say that scene you are actually capturing it that is called as your latent space which means you are not you're not doing a control see control we are not taking a photograph of it it's not a photographic memory none of us have that kind of capability right we don't have photographic memory what we actually do is when we look at something we are actually capturing the features so if you look at let's say a mountain outside as a painting or an artist what you're seeing is the shape of the mountain the trees the color of the trees the shape of the trees etc and you are storing it this is what you're storing it as in your brain now how you're storing it I don't know right none of us can explain it basically but you're storing it in a lower dimension and what you're doing is when that scene goes off from in front of your eyes what you're able to do is you're able to recreate that entire scene that you saw now some of us can do a very good recreation that is why we might be very good artist and people like me when we are asked to create a drawing I'm very pathetic at it it doesn't resemble anything close to the origin like for example if I had to recreate a dog my dog would probably look like a mashed up dog right that's how bad it is now that is where the skill or the art of compressing information comes in really good artist know exactly what features to look out for their capability to look out for what is unique what is important what is it that differentiates etc all that is what their capabilities are right another good example is when I'm teaching something to you you are actually storing information in your latent space this is also another example I quoted to you right now what is your latent space many of us have our own way of learning things right like many we call it as mnemonics right where we might have some sort of rhymes that we would keep in our brain to remember that's a representation that you learn some of us store it in our mother tongue like I might teach it to you in in let's say English but you would rather store it in Canada or let's say in Hindi in your mind so because you're able to better understand concepts in your mother tongue that's your own latent representation some of us like to create a mental map like anything that has been taught you try to create a kind of mapping in your brain like some sort of a graph or a chart that you try to build that is again something like that right so the way we are able to represent information in our brain clearly and then be able to retrieve it is what the advantage of our brain now we want the model to do the exact same thing which is you give an input image it has to store it in its latent space which means it has to store it within its layers and when it is required it should be able to regenerate it this is exactly what we learned in the case of embeddings I'm sure everyone remembers we have let's say 10,000 tokens we learn that 10,000 tokens cosine similarity and can be represented in a 10,000 dimensional shape but rather what it does it represents it into a lower dimensional shape right it brings it down into I'm sure everyone remembers this part right where we had embedding at let's say 100 dimensions or 50 dimensions or 60 dimensions etc we represented it into a lower dimension and that is the advantage of embedding in that case that is also called as a latent space so this is a very very important concept that we all have to remember is where the art of neural networks comes in the ability to convert it into a lower dimensional compressed information there is another very commonly used method for example we call it as exploratory factor analysis if you are free you can read about it later like for example finding out the technical ability of a person now if you want to evaluate the technical ability the easiest way to evaluate a person's technical ability now let's say for example what is a rules technical ability I can't give a score and say a rules technical ability is 80 or 90 or 60 no that is not possible but rather I can observe certain features of a rule and then compute the let's say technical ability and how do I do that for example how many lines of code as you return can it be measured the answer is yes whether I have written 100 lines of code 300 lines of code etc that is one way caggle scores can it be measured the answer is yes so how many hackathons have I completed let's say right that is an example can I use this hacker rank yes I mean that again represents how what is my ranking in comparison to other people etc so using a certain set of observed variables when I say observed it basically means variables that can measure I can actually condense them into something called as technical ability so I can create a score which basically combines 1 2 3 and gives me an index value which tells me let's say for example you know a rules technical ability is 90 which means it's a combination of all these four features that tells you how good I am a close closer score to one indicates I'm very very good at technical cap you know capabilities but a closer score to 0 indicates that it is I'm very poor so rather than having four features explain that element I have condensed it into one single element but measured across four different features basically that is what is called as a latent variable this is a very very important concept and I want you all to remember this concept we will talk about this a little more in detail in the next session as well but I'll quickly pause here I just have two important things that I want you all to do before you come back for the next session basically right which is I want you all to try this particular example with an excel model tell me how you found it if you can create a new example also we've done duck over here take a new class try and detect that class as well and get back to me and tell me how it has worked out so I want you to do this two things very quickly we'll finish the next session will be our last session just one session we'll have about two hours we'll be done with it very quickly I'll explain to you the theory behind the not new class just the last session there's just one session left where I'll explain to you the concept of generative adversarial networks and we'll quickly do the hands-on use case as well okay is that something we can do when I say new class new category right a new label basically that's what I mean tomorrow I think is a holiday a Harish tomorrow is a Republic Day holiday that we have I think the email was sent out I believe I'm not sure but I think we have a holiday that has been sent out to everybody so I think you would have got this email from everyone okay that's why actually I'm intent was to finish the session tomorrow but unfortunately you know because of Republic Day not unfortunately it's a good way also yeah I thought we will complete today but no problem just a few things is left we will do it tomorrow in the next session just about an hour's worth of session okay anyways I'll pause over here and I will let you all get back because you know I'll also have to quickly have my breakfast get ready and then move on to the next session as well okay all of that we will discuss in the next session because we're about time I'll come back to you guys on the next module whatever it is and we have only one class left in computer vision we are done almost in CV2 and then we will wrap up on CV2 right thanks everyone thank you for your time see you all next week take care bye bye