 And there is no recurrent information. So it will immediately generate the output as and in this particular case, right. So it generates the output as and now what it does is it has something called as HD minus one recurrent information that carries the memory of the previous input in itself. Okay, I think the again and again, it's going on flickering. I hope everyone can see now my screen. Okay, keep getting disconnected. I hope this time it doesn't disconnect me. It's kind of a little weird. Anyways, so as I'm seeing it takes as you can see here, it takes the first input, which is Jack here. And once it takes Jack as an input, what it'll do is it'll generate the output as and okay, how it generates is very simple. It has a weight that it is assigned to this. So Wt I'm going to call it as it'll take this input and what it will do is multiply with the remember this is two dimensional right so Jack will be represented as this way so it'll be a it'll be a two dimensional input that goes in so some values will be there like how we saw it dim one dim two so I'll put it as dim one and dim two. So this is the two dimensions that will go in and of course you have the weight into I'll put it as a Strix so Wt they will get multiplied and this entire operation will go through a sigmoid activation function right so it'll be a sigmoid or softmax mostly it'll be a softmax because it'll generate multiple outputs as a text. So when you do all of this the output that you will get is basically a Y which is what is the token and that it has generated here in this case. So it is embedded right Carthicain so when you embed it it will be converted into a two dimension. So this is your X team Jack here right so can you see two dimensions and then the weight that is what is getting multiplied here. No we can't use tanH here because it is generating a predicted value right it's it has to predict the next token so no tanH here tanH will use but that will use it in the next one actually that is in another problem not here that's an LSTM's is that clear to everyone yes are we all on the same page everybody on how Y is generated. A little quickly we'll move on because somehow the internet connectivity has made it a miserable form. Okay then likewise what you have is for now don't worry about the ht minus one the next token that we are actually going to pass is we are going to pass as an input what are we going to pass the token and so and again remember it'll be multiplying with the Wt P plus one I'm going to call it as and let us assume that and is represented in two dimensions again so it'll again have like this right and is going same same function basically right why hat is equal to sigma and two dimensions I'm going to put it like this in the interest of space D1 D2 now dimension you can specify when you embed it you can give it as 10 dimensions also for purposes of understanding I put it as two dimensions into Wt which is the weight associated with and and that generates the output Jill in this particular case right so it will generate the output Jill here. Now it just doesn't multiply with the inputs and the weights it also multiplies with an additional element called as ht this is where the additional element ht comes in so can you see input xt which is this is xt d1 D2 is nothing but it's xt plus one this is xt so you can see xt plus one into Wt weight into ht right ht is what is coming in from the previous part over here now let's say I'm saying that it should look back into three tokens to predict the next token so three is a hyper parameter which is it has to look back it is what we call as a sequence length so I'm saying that three sequence length will generate the fourth sequence length so likewise it will keep on going so here we don't have three but we have one so what it will do is it will generate an additional ht vector which will store three now off the three the first one is jack the second is nothing there right so we don't have any tokens before that so it will be put as zero in this particular case and one value will be stored here for jack to indicate how important jack is while predicting the token and in this case is this part clear mathematically yes everybody able to follow now how Jill is generated with me till now are we all following any questions please ask okay I hope everybody is clear right not always it depends money so we can keep it as three we can keep it as four so if you keep it as three then it will take jack and Jill as the memory to generate the token and if you keep four then it will need jack and Jill went as the input to generate the fifth token so it's a basically a sequence length a look back like how you generated three here right you can generate an also you can go yt minus four five so it's like a hyper parameter to the model you can specify how much you wanted to look back basically I have kept three as an assumption you can keep four five whatever you want basically so Jill is just taking over here the token and as the input which is coming in from the previous state and it is multiplying with the HD which is what we have added see here we don't have HD here in this part over here if you notice there is no HD over here whereas over here there is the new element which is HD getting added what is HD holding it's a vector that consists of a value to indicate the importance of jack in this particular case that is what it is using this is part clear to all yes can we move on everyone that is correct joy dim one and dim two are generated from embedding not the word to embedding process basically that is what it generates okay now next part what we will have is easily we know by now I'm sure everybody would have guessed it by now it will pass Jill as an input in this case right so it will take Jill as an input and remember the same function that you see below right that is what is going to happen Jill will generate the output what will it generate Jack and Jill when is what it is going to actually generate as an output in this case okay now you have the HD here also HD minus one here the HD minus one will consist of what all will it consist of what are the three things it will have it will have Jack and it will have and in this case right so it will have Jack here some value to indicate the importance of Jack and in this case now and is not very important is it isn't it I mean it's just a it's a stop word at the end of the day right even though if I know Jack I can always generate Jill so automatically the weightage for and will reduce and the weightage for Jack will increase and then we don't have the third token so for now it will be zero in this case is this part clear to everyone yes same function like you see here it will follow correct Wt plus one that is correct I'm not putting it as one by one every one of them because otherwise it will become to you know it will be too difficult to write here so that's why just assume that it will be T plus one XT plus one basically it takes the full vector it is the full vector money it will store the entire vector in this case that is absolutely right now again this will unroll okay so which means it will again go ahead and now remember it will generate the next token Jack and Jill went up basically right so up it comes the next token that it generates and remember it goes in as an input it will take Jack and so it will take a vent over here as the input right so it goes in as vent and most importantly you can notice that the vector will update HT vector HT minus one vector will update to hold now Jack and Jill right so it will take Jack here it will take and here and it will take Jill Jill here and generate the next token along with vent it will take the next token and generate up over here in this case is this clear to everyone yes HT minus two is this part clear or put it as HT minus one because it is carrying the information from the previous time step that's all right so it will be easier are we all clear everybody yes now just tell me one thing the process I hope everyone is clear so now explain to me in the next time step what will go in as the input what will be the next input up very good everybody agrees up will be the input how will the recurrent vector look like this is the interesting part I want you to tell me how the recurrent vector will look like so right now it is Jack and Jill what will be the recurrent vector what will get replaced very good right so Jack will be replaced and you will be only left with and Jill went basically right so it will only store and Jill went as the vector basically and up will basically become the input again are you following it so how it drops and adds one vector can you see that how it is dropping one token and adding now of course this is because we have kept it as three right first and first out yes that's the logic it is applying so it is taking over here three now if you give four then no problem then it would have kept it as Jack and Jill went also in the same HT minus one now how do you choose the look back you will answer the question how do you choose the look back any guesses should it be long or should it be short like here we have taken three should it be three four five six what should it be optimum is a good answer yes it's a very diplomatic answer but what do you guys think should it be short or should it be long mediums such it as some say long okay those who said long can you please tell me why long let's see let's see the reasoning because if you've reasoned it correctly you understood the algorithm so those who said long while long those who said short why short good answers coming I really like it I think everybody has had an extra dose of coffee today before the session that's why the answers are very very good today so people who are saying short are saying short because it will avoid unnecessary tokens okay good such it as more the length more the dimensions okay now think from a human perspective right okay just think from a human perspective if I give you a longer article will you not be able to process the information better let's say for example if I give you more information will you not be able to generate better output from a human perspective simply right the more I give you information the better it is which means basically more context I provide to you the better you will be able to generate an output so let's say for example if I just tell you like let's say what is H2O okay let's let's look at a simple scenario I ask you what is H2O now you don't know what am I referring to H2 as the chemical composition you want me to give the answer or do you want me to tell you what water is basically right that's kind of a confusion you fall into but if I tell you what is H2O explain to me the chemical calculation or the chemical composition of water in detail then now I'm providing you more context is in it I'm providing you more information so this is where the concept of context comes into play so the more context context is nothing but it is this recurrent information which you see here this is called as context or memory in some terms we call it as memory context whatever mathematically we call it as memory but in human language we call it as context basically right so the more context I provide the longer context I provide the better you will be able to do so if I just say Jack and tell you to tell me the next set of tokens will you be able to predict it answer is probably no you don't know it right you will be thinking okay what does Jack mean Jack and Rose and Titanic are you referring to that or are you referring to Jack Nicholson or are you referring to some Jack as a person the answer is not clear you don't know anything but yeah it could be even Jack's power of rights so you don't know anything but if I give you Jack and Jill aren't you able to generate the entire poem are you able to do it or not so the more I provide you the more context I provide you or the more look back I provide you or the more inputs or HD minus one I provide you the better you will be able to generate isn't it right are you able to follow now so instead of three if I give four it will be even better instead of four if I give five even better basically but you cannot give the entire poem as a context also that doesn't make sense but the point is the more you give look back the better it is so those of you who answered you know longer context better it is or longer HD minus one better it is or longer look back the better it is you are absolutely right but there is also a problem what will happen if I give long go really long or have a very long look back what will be the problem more memory yes and the HD will become better bigger isn't it this HD will become bigger right as of now it is only storing three so what will end up happening processing will become really really slow right it will not go as fast as you expected to be basically will become really soon in this case give me one moment okay right right right right right right right so the processing will become more as the sequence length increases right so that is exactly the problem but that's not something we need to worry about as of now right see it's like for example if I give you the first token of a Harry Potter book I'm sure you'll not understand right first book of Harry Potter you're not going to understand or let's assume for example without reading the first Harry Potter book is start reading Harry Potter second book basically you're not going to understand anything about the second book unless and until you read the first book right you'll lose out in that particular case so that is where the point is you need to have a look back that looks back or context length we call it as context length sequence length etc the higher it is the better it is the more you know it's able to generate a better sequence at out of this basically good question let me read out what joy has actually put can't it be dynamically adjusted based on the requirement start short increase length has required and then reduce if possible that's a very very good point and that is where there is something where we are going to break down the memory later joy we are going to have something called as a long term memory and short term memory as of now everything is in a way a short term memory but what you're saying is basically just remember Jack and Jill right throughout the entire point why do you need to remember all these intermediate tokens like and and went and all of that that is where the long term memory will come into play so that we will bring in later there is something called as a cell state that will come later that will be integrated where in short term memory I will store what is important like immediate importance and long term memory I will keep the context like Jack and Jill etc so that is where the entire concept comes in yes very good point I really like that question actually are we all clear everybody now are we able to follow how the recurrent neural network for works everyone able to understand yeah in the interest of time so I let the recording I'll put the recording we'll have to move on so I'll probably you know do it a little later so that we can quickly move on to the hands on use case as well because we have one example that we want to do but I hope everybody is clear no questions as of now all on the same page with respect to this concept here only three people replied back the rest I'm guessing are still there in the session okay hopefully you've not switched off right very good so see that is why if you see the modern l lm so I don't know how many of you use modern lm so you will see there is something called as context length in lm have you guys heard of this right so memory of an l m basically right so the higher the context lend the better the model actually performs so some of the context lens in like let's say for example GPD four old let's say for example right you see the context length is 128K context window It says, I think it has, you can actually ask what is the context length of GPT 4. So you can see it takes about 128,000 tokens. So if your question is let's say an entire book, it can look, so let's say for example, your question is 150,000 tokens. It will look at the last 128,000 tokens to actually keep in its memory to generate the next tokens. So see the magnitude of these models, like GPT 4, this is the one that is actually powering the chat GPT has about 8192 tokens. So that is what the maximum length or the context length or the sequence length comes into play. The higher the sequence length, the more better the model can perform. But then again, remember, higher the sequence length, more the processing power as well. Then how is opening I able to do all this so well? Answer is very simple because they have so many, you know, GPUs running, right? So you can actually see how much they incur open AI, right? So you can see how much they incur in terms of GPU basically, right? So see here, it costs them about $3 per hour to actually run for chat GPT basically, right? So that's how much expensive it is for one person or one unique example that they actually take up. So that's why they have hardware costs are the maximum cost that of open AI, if you actually see, you'll see that's the biggest cost that they have actually incurred, right? So you can see $7 billion is what they have generally been, you know, doing. Now why are they spending so much money? Answer is very simple because they are getting us all addicted to using open AI, chat GPT etc. Every model is becoming higher in terms of its context length, the more sequence it looks back, the better it is. So if you see context length of llama, right? Lama is a meta model, right? So meta is Facebook. So you can see it can go up to 131 K tokens. That's the sequence length it can go to. So you can do a comparison of context lens of LLM. If you do it will actually show you the different context lens of different different models, it can actually show you. So this is GPT 3.5 turbo, which is about 4000 tokens. This is an old model. 8 K tokens up to 100 K tokens, etc. So that is where you can compare and see. But is this concept clear to everybody on how the context length is basically the memory that is getting generated? Yes, if you give half, then yes, llama can complete the rest and it can predict it. There's interesting thing here, right? As of now text money, not images, multi models consider images, but for ease purpose, we'll consider it as a text for now. It doesn't consider images. It doesn't matter, Praveen, as long as your context length, even if it is less, see, though it's the maximum it can take, right? It can take about 128,000. But if your question is simple, it can be fit in, let's say 40 tokens, then it doesn't require additional. It'll just pad it up. There's a concept called padding, which it'll just put zeros for the remaining 128,000 and it'll process it. It's not a problem. As long as it has enough information, that's okay. So I'll show you something. Actually, this morning I was playing around with Gemini. We were in another session, we were actually playing and we were doing a code explanation for our previous session. And what exactly happened is I had actually shown them this entire process where we had actually pasted the code. So here you see, I'll show it to you. It's very interesting. So I have shared an entire piece of code, okay, this look over here. See, I have pasted an entire piece of code. Now, I think the context length is a little shorter. It was Gemini 1.5 that was being used at that time. So I had given a little more than what its context length was. So it had taken this entire input and what it did was it explained every line of code. Okay. And when it was explaining, it partially only explained some part of the code it didn't explain. So it goes on till some point and it actually stops here, which is not the entire portion of the code. That is when I have given you missed out on the rest of the lines of code, what it actually did was it went ahead and read back again the input and then you know it's starting to explain basically right. So that is where context length is very important. So it has somehow missed out the important context length and it has you know made an error basically. So that's why you know it had to be reprompted to actually do in this case. There is no ideal context length and there is nothing like that. It's called as a prompting basically right. So again, we are getting into the LLM world, which we will talk about in our NLP to session. But the point is that your context has to be it doesn't have to be too long. It doesn't have to be too short. It has to be appropriate. That's why you have this entire thing. Where you are providing context in the right form in the right way so that the model is able to learn and explain better or it's able to find out when you are giving a question like this for example you have missed out the rest of the lines of code. It is actually looking back on this text and finding out what is the important elements here like missed out rest lines code. Okay, so missed out rest lines code then use this to generate the next sequence of code basically. So that's how it is doing in this case. You can use I mean there is you can use pararie phrase yes you can do that but not exactly sure about that part for. Yes, it's asking for apologies because it has been trained on chat right so it has been trained on being polite and everything but if you use the base Germany model it will not apologize in all it will just say you know this is the example it will not do this. This is just additional training it has been provided. It uses the same optimization technique it will use the SDD add in the same methods right no change in the optimization percent it will not change any of those things. Are we all clear everybody now are we able to understand the concept of RNN and it's working. Yes, anybody has any questions. Okay, good. You guys want a break or shall we continue. Let's see what people say break please okay, I think everybody is tired now. Yes, we'll do after the break okay, so let's take a quick 10 minute break shall we be back at 1211 everyone. And then we'll continue thank you. You You You You You You You You You You You You You You You You You You You You You You You You I hope everyone is back a quick check from all participants a quick yes on the chat if you can. Very good. Nice to see that everyone is in. Okay. Good. So very quickly one last portion that is actually left about problems of recurrent neural network right I think. I think some of it, I think you guys have already answered it yourself. I'm guessing you by now have probably observed what is the issues with it. I'm just going to paste the link to the notebook here so that you can read it. Recreate neural networks exhibit a form of short term memory. Why is it a short term memory? Because you can only give three or four or five as the length. You can't go for a very long length. Okay. Unlike the LLMs that I just show you to 128,000 and 7,000 and 8,000 as the context length or the number of tokens that is not possible in RNN. You can't give very long token as the length in this particular case. In all probability, it will flail basically, right? It can't handle it. And since they only have the last output value as the input, they are limited to how much state they can hold at any given time. So in this example that we were just seeing they can they restricted only to about three or four or five at the max, which is not enough basically, right? So it is not able to actually perform really well in terms of the inability to handle a higher context length in this particular case. So that again prevents it from going on to a really large sequence length basically, right? So for example, what time is it? So it actually breaks it like this, right? Like what time is it? It breaks it and passes one input at a time. What then generates the next output and then time is it like this? It keeps going on, right? So it is generating one. Process what happens is you notice look at the color change. What is the color change basically? If you notice what has a very high importance when it is generating time, what has again a very high importance? Look at that color dot blue indicating that the importance of what is reducing over a period of time. So eventually what will happen is that what would be a very important point, but eventually it will get lost out in the memory and then indicates that the importance of that token eventually dies out, which means that you know it'll forget it. The memory will not be able to hold it. So this is a very good diagram to actually visualize how the entire process actually works, right? I'll just change this. Let me know if you guys can access now. So this is one of the biggest problems where just look this, this is a perfect diagram, which is actually showing you how the importance of the token, what reduces by the time it gets to the token it, you can see it is almost diminished. It's become zero. Like the importance has gone, but it's a very important token likewise look at time time is an important token. What time is supposed to be the important element here, but look at how time diminishes the importance or the value of time is almost diminished to zero basically. So that is where sometimes it has what we call as a short term memory. It can't remember very old information, right? It can't handle like for example, you can't explain it to predict a book, predict the next sequence in a book. No, it can't do all those kind of problems. It is going to have trouble with that. So that is where it fails in scenarios like this, right? Right, which is to retain the information for a really long period of time. You can't give a very high context length also. You can't give it a very high memory also. So you can't just say my sequence length will go or my context length will go or my memory will go until 100 or 200 or 300 impossible. If you give that high, it will not able to process it at all. It'll all probability fail because as you all know, it completely has to process, multiply, generate, etc. Fully becomes problematic basically. So that is where to the question, I think joy asked and I think a few other people mentioned as well. Comes the concept of LSTM like how do I utilize the memory in a better way? I'm not getting into it as of now. The reason for that is because I thought we will first do RNN as an example. And then I'll explain to you LSTM, right? Because otherwise it will confuse us all. And but again, it's similar to an RNN only except that it has a memory which is done in a way that it can handle much better as well. Exactly well, vanishing gradient and exploding gradient as well. Why? Because if you give too long a sequence, this is vanishing, right? This gets vanishing. Can you see it's almost vanishing to zero? So if the length becomes longer and longer and longer, this will become, it might not become zero, but it will be a really small value close to zero. It will be like 0.000000000 something, right? Which is not enough, which is a really, really tiny value basically. So all these problems contribute towards the failure of an RNN. Again, I don't want to call it failure immediately because I know we have not yet done the hands-on use case. But it's important we understand that there are models that can have longer memory. They can handle longer memories. And how do they handle that? That is something we'll talk about probably in the next session. But for now, shall we do a quick example of RNN? Yes, shall we try out something? Everybody looks like the energy is low. Only three people feel like doing it. The rest are all feeling like, okay, I might as well take some rest. Okay, all right, let me just quickly take up one example. Give me one second. Okay. So once I do this, I think the memory part of it will also become very clear to all of us, right? So let's go ahead and do that. Let's open a new notebook very quickly. Okay. I'm just sharing the link to my notebook as well so that you all can use it. And I'll also share with you what is the approach going to be, okay? Everybody has the notebook with them? Ready? Okay, please don't try it on my notebook. Just take a copy of it or use it to copy anything you want from this notebook. Okay, step number one, what I'm going to do is load the libraries and the data, okay, standard process. And once we've loaded the libraries and the data, then what we're going to do is tokenize the text. So I'll explain to you how the tokenization works. Now, remember in our previous example, we did all the pre-processing where we did stopper removal, etc. Everyone remembers that stopper removal, limitation and stemming and all of that. Do you think we can apply that here? Is it possible to apply those techniques here? How many things? No, not possible, right? You can't apply those kind of pre-processing because what will happen then it'll become Jack Jill went up hill. Jack fell down broke ground like that. It'll come now that that doesn't really sound like a good nursery rhyme, right? You don't want that kind of a nursery rhyme. It looks like, you know, the model is having some hiccups that it is not able to generate the next sequence. So hence the point is let's not try to, you know, let us not try to actually remove those stoppers and all of that's all all the pre processing we did for discriminative AI. We are not going to apply it for this particular use case here. So then what we will do is after we tokenize it, we'll convert the tokens to a to a sequence in this case, right? So a sequence means we have to tell the model, right, that it's a sequence like Jack and Jill is a sequence. We need to tell that to it. So how do we do that? I'll explain to you that sequencing thought will also come into our place. And we need to specify the sequence length. What is the sequence length? Right now we gave it as three, right? In the example that I was showing you using the whiteboard, we get the sequence length as three. I can give sequence length as four sequence length as five, etc. So how much does it have to look back into? So I'll call it as look back or sequence length or context or memory. So don't get confused when I use these words sequence length context look back memory, etc. These are all indicating what is the past? How much should it look back into? For it to be able to generate the next sequence the longer the better, the shorter the faster. So remember that logic as well. Then what we will do is create an input and output sequence. Okay, that's the next thing that we're going to do. And followed by the six sentences, we're going to define a sequential model. So seventh is define RNN layers. Number of layers. Number of cells in each layer. We are going to provide that two things. And followed by that eight step is going to be. By the way, this is RNN layers and densely as also we need both actually. RNN layers and densely as both we need. Then we are going to compile the model like how we did earlier. Compilation remember first thing is we need to specify the optimizer. Then the loss function. And after the loss function what else do we need the metric metric of evaluation. Right here it is going to be accuracy. Ninth one is going to be. Fit the model. Where we are going to provide the input and output sequence. And two is going to be number of e-boss. And I think that should be about enough. And in terms of the step number 10, it's going to be evaluate the model. And I'm going to create a inference pipeline. So I'm not going to look at all this. I'm sure many of us have already seen that we use confusion matrix classification report, etc. Right. I'm not going to use any of that. I'm going to use work. Recall as an inference pipeline. So what I mentioned. I hope everyone remembers from the last session, what an inference pipeline is. Where we just given input and it should be able to pass it automatically through the model. Go through all of these steps and generate the output basically. Right. That's an inference pipeline. That is exactly what I wanted to do. Is that clear to all? Yes. Following. Any questions on the steps? If you have any. So these are the 10 steps we are going to follow in this case. Okay. Not too complex, right? It's quite simple only. Nothing great as such. Thank you. Have you all done? Yes. Not possible as such. No, after we do the entire processing only then we can write the inference pipeline. We can't write the inference pipeline first itself. So let's start with the loading the libraries. Let's do that. Go with the libraries. So what are we going to do? I'm just going to copy paste. I have have the code here. So this copy paste and do that. So let's go ahead and import all the libraries. Please. So this is like one of my famous favorite examples of taking Jack and Jill right and the reason why I take Jack and Jill most of the times is because. That's how children learn right the rhyming their memory etc. So I have had in one of my past sessions I think one of the students was learning. And I think his child walked in and I think probably you would have kept me on the speaker or something of his laptop. So I was going on speaking about Jack and Jill. So I think the kid was surprised if the parent was actually in kindergarten class or was he actually learning something related to AI right. That's how it actually turned out to be. So usually it's a bad example to take. But then again the point is like you know it's the easiest example to take because going on forward later you will see more complex data that comes in right. And so I hope fun in your kid and ask you right. What is it you're doing exactly are you now going to learn all these school reins or are you like. He actually shared it to me over message he was like. You know my son kept on asking me why exactly are you learning Jack and Jill. So it's like it was very funny actually. Tensor board is not an evaluation matrix session. I'll probably show an example of a tensor board tensor board is like you can visualize the model. Do you guys maybe I'll write when corporate it in this example good point let's do tensor board in this example as well. I'll show it to you later when we fit the model. Everyone's done have you all imported the libraries. I don't know why it's not working today but anyways. I know you will all hate me for this but okay we'll take some other or we'll take Jack and Jill itself right. Jack and Jill lyrics and people will hate me for this but then what can I do will take first a simple example I will give you some more complex example later. So for RNN I cannot use very complex also so please bear with me with this. You know extremely sorry about that but I hope you guys understand this is the simplest I could come back with. Yeah so let's let's just take the simple example the complex ones we anyways have you know lot of complex examples also so don't worry about it right. I'm going to put text okay so just going to call it as sentence and I'm going to paste the entire Jack and Jill as a whole. I'm just going to do it because I'm playing I think I don't know why. I'm going to remove all of this and. This is my entire sentence by the way right I'm just putting it a sentence if you want I'll paste it in the map notebook as well or in the chat for those of you who want it you can take it like this just execute it it's just a small text that we are creating in the next example when we do I'll show a book right I'll take a proper book and I'll explain to you. So Jack and Jill went up the hill to fetch a pillow water Jack fell down and broke his phone and Jill came tumbling after and whom did Trotters fastest equal keeper to be old. Everyone's done. I come to embedding a little later the reason why I'm not using embedding for now as when it's show the sequence embedding I'll use it in the next example when we do with the last team first I wanted to show sequence like put sequence output sequence how I get that lens etc. I'm going to do it this way first. Let's convert all of it into lower case right I don't want I don't want anything like big as that's in terms of it. Yeah you can put it in the Q&A I'll paste it there if you can put something in the chat in the Q&A if you put a question I can paste it there. So you have to put a question there then it will come up this. So let's convert first everything into lower case because I don't want you know Jack to be treated differently or Jill or you know capital letters to be treated differently I just want them to be treated as an individual same thing right. So that's why let's go ahead and convert everything to a lower case in this case. Once you're done with these steps just let me know put a quick guess on the chat so that it's clear. Because capital letters it will consider it as two different tokens right I don't want it to think like that I wanted to treat it as the same token even if it is capital or non basically that's why. It'll be treated as two different tokens altogether so I think we discussed this earlier like Mumbai like this and Mumbai like this are two different tokens it's considered as two different things right so you can't why do you want to treat them as two different tokens rather just treat them as a single token which is lower case Mumbai that's it that's why we're going to treat them as lower case basically. To avoid the vector size becoming really big we don't want that to happen right so that's why. Done everyone next step is to tokenize now earlier we had used tokenization functions that were available within. You know which were available within the NLTK library etc now we're not going to use the NLTK library the first step of tokenizing which is Keras tokenizer what we're going to do is we are going to attach a token number we're going to attach an integer to every token here so like Jack will be a signed integer and will be assigned an integer or random integer it doesn't matter. But it'll just be an integer because you know we don't want these words to actually go through the model we want the actual numbers right numbers as are the most important part here so that is why this step number two is to tokenize the text now how do I tokenize it is Keras actually has. Something called as Keras tokenizer if you actually look at it here so we are going to use the tokenizer function now what the tokenizer will actually do as a process is it will take the input sequence that you provide and every unique token it will attach an integer to it like Jack will be given one and will be given to Jill will be given three when will be given for etc now what will happen is once you convert it into integers it is. It is easier for the model to process right it will not vectorize it but it's easier for it to process I'll show you how it works first thing we will do is we will actually initialize the tokenizer class and how do we do that i'm just going to call it as token is equal to tokenizer just initialize the class basically right so we're just going to initialize it as tokenizer is equal to tokenizer just give it like this basically right then what I will do is tokenizer dot fit underscore. On underscore text and I'll pass my sentence as an input in this case it'll assign the same number one only so here if it is Jack and Jill if it has assigned one here again Jack over here will be assigned one only the same i'll show that to you guys i'll show you that example very quickly so this uses tokenizer fit on text and the sentence goes in as the input into this case and then followed by that followed by that the next step is convert them into sequences so sequences is basically tokenizer what it will do is unfortunately it will end up creating a list like this so it'll call it as one two three like this it'll end up creating right it'll like this it'll do some numbers it'll do which doesn't make sense so hence what i'm going to do is i'm going to convert them into sequences also so that is where sequences is equal to tokenizer dot text will be in a way that I'm going to do it. So, sequences and the sentences go in as the input right it'll do it this way so just pass the sentence as an input so now you can actually see the sequences. Once you're done just let me know it'll be just random integers it doesn't have to be one two three percent it'll randomly assign some integer values it's like label encoder so in label encoder how for each category we provide integer value that is how it is actually providing a value. It's just randomly assigning it that's all yes that's why it'll look jumbled but I'll show it to you is everyone done with this step it's unique one integer for every token it will provide it will be unique for a token but has everyone done this if you done just put a yes it will be a if you done just put a yes on the chat will come to that will come to that will get to one by one just give it a moment will i'm going to explain every single process not going to miss any step right so you see here Jack and Jill went up the hill we've got it here as a process you can see here it has converted it into this sequence integers over here so that entire poem that you can see it's a entire poem that we had in words have been considered as integers here now you want to know what is the mapping you can check that also very quickly right so if you see here tokenizer dot if you do you can very quickly check just a minute tokenizer index underscore index underscore word word underscore index word underscore index look at it here check this out can you see Jack is given the integer two and is given the integer one Jill is given the integer three can you all see that now so that is where what it has done is it has got the mapping we don't need to worry about it the mapping is all stored in this tokenizer class variable but based on this it has created a dictionary as you can see every token is now assigned a unique integer basically right so even though Jack might appear three times it doesn't matter so see you can see Jack is given two you can see the other next time it comes again it is represented as two likewise Jill for example is represented three two times right so you can see here this is how it has converted all the tokens first and all the tokens into an integer and it has the mapping so don't need to worry about it even though it has changed it it's not a problem for us it will be taken care of by itself is that clear to all yes preferably this tokenizer library is better basically yes we have now the sequence mapped against integers not words integers we have basically is this clear to all the tokenization function everybody are we all clear calling yes everybody on the same page now okay so tokenization is done it has been converted into integers next is we have converted it into a sequence also now the only thing is we need to specify the sequence length okay so we need to specify how much the sequence length should be right so let's say for example I'm just taking an example so if I take the sequence length as five Jack and Jill went up okay so take this as an input generate the next token the okay and then what we will do is and Jill went up the to predict the next token hill and then what we will do is take Jill went up the hill to predict the next token to like this right so the same process we are going to you know follow as a process where we're going to take the first five sequences to generate the next so the sequence length is the amount it has to look back to generate the next sequence now we have quite a lot of data not very small so I'm thinking we'll take the sequence length as five which means you look at five sequences as input and generate the so here in this case we have let's say five take this as the five generate the sixth word then drop one take this as the five and generate the sixth token likewise take this as the input and generate the next token like this keep on dropping one token adding one token and keep generating a sequence of token like a window basically right it'll keep on more basically in this case yes but one token at a time will keep generating right will not generate every token it can generate all the tokens in one shot because it is if you recall it is going to actually process it one token at a time only right so if you remember it takes one token and Jill bent so it'll only do one at a time it can generate all of it just that we are going to ensure that we follow it as a sequence in this particular case is this clear to all yes the process So, integer is just a dictionary value, Succita. It's just a value that we are providing to tell that this is the token. It's a mapping like label encoder basically, right? How you do it in label encoder. Mail is encoded as one and female as two, let's say for example. In the same way, this is also a label encoding except that you have a multi-class problem. So, you are converting it into a one, two, three, four like that, you're converting it. You're making it into integers. So, simply put multi-class problem categorized into whatever the total number of words or tokens you have. So, if you look at it, we have a total of how many tokens roughly about 360 or taking only one second. So, if you look at it, it will give you 76 tokens. We have a total of 76 tokens. So, it is actually assigning for every single token. It is giving a value like a label encoder. That's what it is actually doing. Sequence number attached against. There is no sequence number. I didn't get you which one are you referring to? This one. You're referring to this word underscore index. This particular dictionary. Okay, this is the mapping. This mapping tells you. So, here you have basically two, right? What is this two stands for? You can actually go here and check two basically stands for Jack. Likewise, if you look at it here, it gives one. So, one you can go back and check what one is one is and. And likewise, Jill is three. So, you can come back here and see. Jack engine. That is what it is actually assigning it. So, it's this the dictionary. Also, what we call as the vocabulary, right? We call it dictionary or vocabulary. Whatever you want to call it as. This is the vocabulary. All the unique tokens that are there in our. Problem statement basically. That's what we have here. Is that clear to all? Yes. Are we all following this process? Wait, we have not yet got to breathe that we will get to later as of now the modeling part. We haven't yet got to it at all. We are just assigning integers to every token. That's about it. Random. There is no logic to it. It's like label encoder. Haven't you guys have used label encoder? Haven't you? Escaleer and dot label encoder. You guys used it. It's the same function basically. So, any categorical variable. It applies some sort of a class, right? Like you can see here. Tokio, Paris, whatever it gives basically 2 to 1. Now, how does it give 2 or why does it give 1? That's a random process. That is what it is assigning over here. Now, I'll ask you the question. But you tell me if you can do it by yourself. Okay. So, we'll take the sequence length as 5. Okay. So, what I want you to do is consider this as the X. So, Jack and Jill went up. Consider this as the X. This will be the Y. Okay. So, entire this portion to this portion. Let me just one second. Yeah. So, this will be my X. Jack and Jill went up. Okay. This will be my X. And this will be my Y in this case. Okay. Jack and Jill went up and then predict the next token. The in the next scenario, what will happen is I will have. And Jill went up the I will take this as my X. Okay. And the token hill will become my Y in this particular case. So, this will be my Y that I will treat it as likewise. What I will do is in the next line, I will actually convert. I will take. Jill went up the hill as the input to generate this as my X. And then I'll convert this as my Y in this particular case. So, two will become my Y here. So, I need to keep doing this. How will how do you think I'll be able to do it any guesses. Any idea how we can do that. Can anybody think of a particular method to do it. A for loop. Yes, that is right. You have to run a for loop. That is true. What else will you do. Okay, I'll explain to you the code. Don't worry. I will write it out to you here. So, what I'm going to do is I'm going to create two things over here. Right. So, I'm going to first call it as max underscore length. So, now my max underscore length, I'm going to give it as five. Okay, so we have agreed on five. That is completely, you know, a random thing. If you want to give six also, you give, but don't give too high. Okay, so otherwise it will become difficult. So, I'm giving the maximum length or this is the look back or the. Previous set of tokens that it has to go through. So, that is where I'm giving it as five in this particular case. You can give as many as you want, but don't give too high as I said. I'm creating two list empty list. So, I'm going to give it as x and y. So, list x and list y. So, two things I have created empty as of now. Then what I do is I'll do a for loop for I in the range of maximum length. What is a maximum length maximum length is the entire sequence length that I have. Right, which is here five. So, I start from five until length of sequences zero. So, length of sequences zero is last part of the sequence till this entire length. End of the sequence basically. It will loop so five to the end where x is basically nothing but sequences the entire sequence that I have here sequences. The entire sequences starting at zero at position until I minus max length I minus max length is max length is five. So, I minus four zero to four basically right zero is starting to the it point basically so it is nothing but the iterator in this case. And then why dot append is the fifth token or the sixth token basically right so why dot append will basically take it as token number fifth in this case. So, until this is the for a fifth zero to four which is basically your token starting from the beginning till four and then followed by it the last token which is excluded becomes your why basically. So, let's execute this I'll show it to you will be able to see how this looks like. Just go ahead and do it. And I'll show you how this has generated the sequences for our model. Exactly we'll go ahead and print x and y in fact I'll show it to you guys right now itself. Go ahead and see this. Once you're done just let me know. Everyone's done. So, no guys. Now look at it here the first input sequence look at the first one just pay attention to this two one three seven four this is the first input sequence let's go back. Two one three seven four can you see this is the first sequence two one three seven four which is nothing but Jack and Jill. Jack and Jill went up to one three seven four so two one three seven and four the token number the is basically eight so go to why you will see that it would have generated y as well just look at it. This is my why can you see here. I hope everyone can see see the why token that predicted token is why in this particular case right eight here likewise look at it it has dropped. Two can you see it has dropped to now in the next row it is one three seven four and it has added eight can you see it. It has added eight and the predicted token is nine that it has actually predicted in this case so you can see why is nine here. In the next row you can see automatically nine gets added and you know you have seven four eight nine and then five so you see I've converted it into a matrix format haven't I. We have made it into a matrix isn't it the text has been converted into a matrix but not vectorized okay it has not vectorized yet it is just converted into a matrix format at this point in time. Isn't it clear now are we able to follow it see how the input and output sequences have been created. Any questions on this. No library as such money that's why we are doing it through for loop because the sequence lens are not not fixed right it can be five it can be 10 it can be 15. So that's right but you can create a function if you want you can create a class object and create a function and you know reuse it if you want to avoid writing this code again and again absolutely fine you can do that. But are we all clear everybody able to follow yes. Okay very good so what comes next. So we have done with the steps of loading the libraries that is done tokenizing the text done converting it into a sequence done specifying the sequence length which is five which is done creating an input sequence and output sequence that is also done isn't it problem solved isn't it everything is over now. It's done it with simple programming that is true but then again the model now someone has to learn this input sequence and output sequences isn't it someone some model has to figure out that. The sequence follows this token and likewise this sequence follows this token so no matter where I begin it doesn't matter right even if I give tomorrow let's say. I give this as the input to the model then it should be able to even generate from here feature payload of water jack fell down and broke his crown etc and keep on generating the text as an output so that's another important part right so this entire sequence has to be learned by the model in this particular case so that is where the entire cons. So now we have to bring in the model because now we have our sequences we have our input data we have our output data also both are available to us so since all is available the only work left for all of us to do is train a model that learns on both x and y the input and the output sequence if it has learned really well then no problem I can just give any sequence I can just say for example. Something like this right I can say fell down and broke like let's say I give this as an input it should be able to say his crown and Jill came tumbling after so whatever I give from there on it should be able to predict the next sequence isn't that what it is supposed to do yes or no. Do you agree with that or not. So what I'm going to do here is I'm using a model called as RNN right so simple RNN is what we are going to actually use so it's called as a simple RNN here as if you actually go to it this is where it is available right so you can see this is the model that you're going to use and we have already imported it if you notice I've done it much earlier I've gotten the simple RNN itself here so we've got the simple RNN it is got all the activation functions everything that we've got. It's got all the functions everything that it requires etc everything is taken care of so that is where you know we'll go through each and every one of these elements about you know what the simple RNN is all about but fundamentally the next part is just like how earlier we set up the dense layers right everyone recalls that from our session right where we set up. So we're going to start with the dense layers and created multiple dense layers that is exactly how we are going to do here as well which is first start with calling the model itself so we're going to call it as a sequential model let's start that very quickly just put here. Model so we'll start with model is equal to sequential everyone remembers this part from our previous session model is equal to sequential which is a sequential layers which means every layer is going to generate an output that is going on to the next layer and that is exactly what is happening if you notice here. This is what is happening it is sequence it's a sequence of outputs that is being generated from one to another to another that keeps moving the information from one stamp to another stamp so that's where it is important that we clearly look at that aspect right where it is a sequence of layers basically. Done everyone so step number one is module dot add can I directly give the sequential layers is it can I give the RNN layers is it possible to give directly the inputs that I have here can I actually directly pass the inputs into the model. Model can I just give two one three seven four as a input to the model why why not what will end up happening. What will it end up doing it will treat it as a number it will end up treating it as a continuous value it will become a regression problem isn't it won't it become a regression problem if you look at this these are all numbers right these are all values look at the Y variable these are all values so but it is discrete as well when I say discrete though these are numbers these are encoded these are just values that we have provided to understand like 8 9 10 11 whatever it is these are a mapping that we have they are not actually numbers they are not values these are not sales numbers or these are not the temperatures of a city or anything like that these are just an integers that are referring to a particular token. So this step number one is to vectorize this particular text so what you have here 8 9 5 10 11 12 whatever numbers you have they need to be vectorized so that is where I know we are about time the next session when we come back what we will do is we will understand the vectorization we are going to use so we are not going to use all that you know count vectorizer etc we're not going to apply that I'm going to tell you how you're going to vectorize this text I'll explain that to you. So in the next session the moment we come back first thing we'll vectorize the text run the model and then inference the model to see how well it works if it works good very well if not also we'll quickly jump into the next technique which is LSTM's and once we have an understanding of LSTM's we'll not do a hands on use case because we have an LSTM use case coming up in our NLP to session as well so we'll quickly understand the theory behind NLP one LSTM a basic intro to LSTM and then next. We close our NLP one session does that make sense everybody does it help? Yep all good then. Okay but before I end the session I really want to we have only two more sessions will be done in two more sessions but I really want to appreciate the batch for you know such good learnings I feel you know the pace is good with you guys you guys are doing really well and also the questions some of the questions are really really good. I like it and very intuitively some of those questions are coming up right so I really want you to keep the spirit up that makes me feel also good about the session right it makes me feel like okay you guys are all on the same pace but in case you're not able to understand also please drop me an email through the week sometimes it's possible in the session you don't understand something I might be a little delayed in applying sometimes because of you know travel and all of that but as much as possible I'll try to reply as quickly as I can but I'm not sure if I can do that. Make sure you go through the concepts through the break that you have in this week. All right with that I'll wrap up and I'll let you all take a break relax enjoy your weekend have a wonderful week ahead as well a productive week and we'll see each other the next Saturday thank you everyone take care bye bye see you. you you you